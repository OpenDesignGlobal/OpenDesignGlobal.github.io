<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.9" />
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html,
      body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme')
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia('(prefers-color-scheme: dark)').matches
      if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
        document.documentElement.classList.toggle('dark', true)
      }
    </script>
    <meta property="og:url" content="https://opendesign.world/posts/XR/paper/XR.html"><meta property="og:site_name" content="OpenDesign"><meta property="og:title" content="XR"><meta property="og:description" content="XR 2024-04-25 ResVR: Joint Rescaling and Viewport Rendering of Omnidirectional Images Authors: Weiqi Li, Shijie Zhao, Bin Chen, Xinhua Cheng, Junlin Li, Li Zhang, Jian Zhang Lin..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"XR","image":[""],"dateModified":null,"author":[]}</script><script src="/translate.js"></script><script src="/model-viewer.min.js" type="module"></script><meta name="google-site-verification" content="1"><meta name="description" content="OpenDesign - Open Source Makes Design More Transparent. Enhance transparency, collaboration, and innovation in design through our open-source community."><meta name="keywords" content="OpenDesign, open source, design, collaboration, innovation, transparency"><meta name="author" content="OpenDesign Community"><link rel="icon" href="/favicon.ico"><title>XR | OpenDesign</title>
    <link rel="preload" href="/assets/style-Dz7YGj4i.css" as="style"><link rel="stylesheet" href="/assets/style-Dz7YGj4i.css">
    <link rel="modulepreload" href="/assets/app-BHieihQC.js"><link rel="modulepreload" href="/assets/XR.html-BTT_5J0_.js">
    <link rel="prefetch" href="/assets/index.html-NRWTlqDK.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-gWnHuKod.js" as="script"><link rel="prefetch" href="/assets/index.html-CMCnOeoa.js" as="script"><link rel="prefetch" href="/assets/contact.html-CqFPXh--.js" as="script"><link rel="prefetch" href="/assets/contribute.html-BFxunDGb.js" as="script"><link rel="prefetch" href="/assets/mission.html-BpQijo_J.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-C0nIS-ZL.js" as="script"><link rel="prefetch" href="/assets/index.html-DV40K-nG.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-5WNkQHH1.js" as="script"><link rel="prefetch" href="/assets/index.html-DsOLjwMC.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-OEKY4fNp.js" as="script"><link rel="prefetch" href="/assets/index.html-D6SLkxn5.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-CTLQEMNK.js" as="script"><link rel="prefetch" href="/assets/index.html-8L5jEYDT.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-BlL-UHuL.js" as="script"><link rel="prefetch" href="/assets/index.html-BL_lWJqR.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-DldxBWcT.js" as="script"><link rel="prefetch" href="/assets/index.html-BkEITCMf.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-DylMK7R0.js" as="script"><link rel="prefetch" href="/assets/index.html-B8KjNhFS.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-DFU_34gV.js" as="script"><link rel="prefetch" href="/assets/index.html-B-ltSokT.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-BwvxlA-E.js" as="script"><link rel="prefetch" href="/assets/index.html-TjaW36cS.js" as="script"><link rel="prefetch" href="/assets/Resource.html-cKjFR58C.js" as="script"><link rel="prefetch" href="/assets/LLM.html-CROKeg-W.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BH-QAtP3.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-C9yRmFpr.js" as="script"><link rel="prefetch" href="/assets/Stable Diffusion.html-qQEKRMyq.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-Dd1_onmw.js" as="script"><link rel="prefetch" href="/assets/Resource.html-D1Y9bEHO.js" as="script"><link rel="prefetch" href="/assets/HMI.html-D8vWIASk.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-Dtf1DQM3.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BMb2BmZS.js" as="script"><link rel="prefetch" href="/assets/HCI.html-COH8_LzK.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-Bh6mrQDI.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-ar4dH4a5.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-Cs5NSxRQ.js" as="script"><link rel="prefetch" href="/assets/Resource.html-BjwoO9Mj.js" as="script"><link rel="prefetch" href="/assets/Resource.html-BWnwIjod.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-CajRREq3.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-CHQtE9Rr.js" as="script"><link rel="prefetch" href="/assets/Resource.html-BDYXJau7.js" as="script"><link rel="prefetch" href="/assets/Robot.html-lVb-6o3h.js" as="script"><link rel="prefetch" href="/assets/Robot Vector.html-C8t2jD7z.js" as="script"><link rel="prefetch" href="/assets/Resource.html-BXb4czMK.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-CkJoXYn7.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-CI3BSJ5h.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-Blg2-Qj8.js" as="script"><link rel="prefetch" href="/assets/404.html-B0N4on9T.js" as="script"><link rel="prefetch" href="/assets/index.html-CX4UruRJ.js" as="script"><link rel="prefetch" href="/assets/index.html-CE-x7rXR.js" as="script"><link rel="prefetch" href="/assets/index.html-_Upf1qxR.js" as="script"><link rel="prefetch" href="/assets/index.html-UuOi8n4k.js" as="script"><link rel="prefetch" href="/assets/index.html-cuioN4E6.js" as="script"><link rel="prefetch" href="/assets/index.html-BU8qXK0C.js" as="script"><link rel="prefetch" href="/assets/index.html-CjueV7ZD.js" as="script"><link rel="prefetch" href="/assets/index.html-DzRKcQMP.js" as="script"><link rel="prefetch" href="/assets/index.html-DZXGm0GG.js" as="script"><link rel="prefetch" href="/assets/index.html-CyPIvh6-.js" as="script"><link rel="prefetch" href="/assets/index.html-CwoC2CzM.js" as="script"><link rel="prefetch" href="/assets/index.html-CqIYfX-r.js" as="script"><link rel="prefetch" href="/assets/index.html-CcgmtGJS.js" as="script"><link rel="prefetch" href="/assets/index.html-CHyQ5zhe.js" as="script"><link rel="prefetch" href="/assets/index.html-D1eUR0Jx.js" as="script"><link rel="prefetch" href="/assets/index.html-CtAa_bYv.js" as="script"><link rel="prefetch" href="/assets/index.html-HwWHeGX1.js" as="script"><link rel="prefetch" href="/assets/index.html-YnYPIFeD.js" as="script"><link rel="prefetch" href="/assets/index.html-BdiLBDqG.js" as="script"><link rel="prefetch" href="/assets/index.html-DHCL9MoX.js" as="script"><link rel="prefetch" href="/assets/index.html-DAB1cBSf.js" as="script"><link rel="prefetch" href="/assets/index.html-CR2ikgki.js" as="script"><link rel="prefetch" href="/assets/index.html-BDg5XzdW.js" as="script"><link rel="prefetch" href="/assets/index.html-Cq5Acrt5.js" as="script"><link rel="prefetch" href="/assets/index.html-DaplB37d.js" as="script"><link rel="prefetch" href="/assets/index.html-CG57wrfy.js" as="script"><link rel="prefetch" href="/assets/index.html-DchwL6Vh.js" as="script"><link rel="prefetch" href="/assets/index.html-BG6cNR8C.js" as="script"><link rel="prefetch" href="/assets/index.html-BWIhuuwO.js" as="script"><link rel="prefetch" href="/assets/index.html-D8lbgYJK.js" as="script"><link rel="prefetch" href="/assets/index.html-B1egvi4P.js" as="script"><link rel="prefetch" href="/assets/index.html-DNd4Yo5Y.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="logo" src="/logo.png" alt="OpenDesign"><span class="site-name can-hide" aria-hidden="true">OpenDesign</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide" aria-label="site navigation"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/knowledge/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/knowledge/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/knowledge/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/knowledge/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/knowledge/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/knowledge/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/knowledge/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/knowledge/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/solutions/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/solutions/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/solutions/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/solutions/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/solutions/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/solutions/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/solutions/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/solutions/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Paper"><span class="title">Paper</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Paper"><span class="title">Paper</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/paper/LLM.html" aria-label="LLM"><!--[--><!--[--><!--]--> LLM <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/paper/Stable%20Diffusion.html" aria-label="Stable Diffusion"><!--[--><!--[--><!--]--> Stable Diffusion <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/paper/HCI.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/paper/HMI.html" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/paper/Robot.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link route-link-active" href="/posts/XR/paper/XR.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="About"><span class="title">About</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="About"><span class="title">About</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/mission.html" aria-label="Mission"><!--[--><!--[--><!--]--> Mission <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contribute.html" aria-label="How to Contribute"><!--[--><!--[--><!--]--> How to Contribute <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contact.html" aria-label="Contact Us"><!--[--><!--[--><!--]--> Contact Us <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><!--[--><h4 class="navbar-dropdown-subtitle"><span>Contributors</span></h4><ul class="navbar-dropdown-subitem-wrapper"><!--[--><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/AGI/Contributors.html" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Generative-Art/Contributors.html" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HCI/Contributors.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HMI/Contributors.html" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Human-Factor/Contributors.html" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Intelligent-Device/Contributors.html" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Interactive-Installations/Contributors.html" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Robot/Contributors.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Spatial-Interaction/Contributors.html" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/XR/Contributors.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="navbar-item"><a class="external-link" href="https://github.com/OpenDesignGlobal/OpenDesign" rel="noopener noreferrer" target="_blank" aria-label="Github"><!--[--><!--[--><!--]--> Github <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--[--><select id="translateSelectLanguage"><option value="Not Translate" selected="selected">Not Translate</option><option value="Default">Auto Translate</option><option value="english">English</option><option value="chinese_simplified">简体中文</option><option value="chinese_traditional">繁體中文</option><option value="russian">Русский</option><option value="japanese">しろうと</option><option value="korean">한국어</option><option value="deutsch">Deutsch</option><option value="spanish">Español</option><option value="italian">italiano</option><option value="french">Français</option><option value="dutch">nederlands</option><option value="norwegian">Norge</option><option value="filipino">Pilipino</option><option value="lao">ກະຣຸນາ</option><option value="romanian">Română</option><option value="nepali">नेपालीName</option><option value="haitian_creole">Kreyòl ayisyen</option><option value="czech">český</option><option value="swedish">Svenska</option><option value="russian">Русский язык</option><option value="malagasy">Malagasy</option><option value="burmese">ဗာရမ်</option><option value="thai">คนไทย</option><option value="persian">Persian</option><option value="kurdish">Kurdî</option><option value="turkish">Türkçe</option><option value="hindi">हिन्दी</option><option value="bulgarian">български</option><option value="malay">Malay</option><option value="swahili">Kiswahili</option><option value="oriya">ଓଡିଆ</option><option value="irish">Íris</option><option value="gujarati">ગુજરાતી</option><option value="slovak">Slovenská</option><option value="hebrew">היברית</option><option value="hungarian">magyar</option><option value="marathi">मराठीName</option><option value="tamil">தாமில்</option><option value="estonian">eesti keel</option><option value="malayalam">മലമാലം</option><option value="inuktitut">ᐃᓄᒃᑎᑐᑦ</option><option value="arabic">بالعربية</option><option value="slovene">slovenščina</option><option value="bengali">বেঙ্গালী</option><option value="urdu">اوردو</option><option value="azerbaijani">azerbaijani</option><option value="portuguese">português</option><option value="samoan">lifiava</option><option value="afrikaans">afrikaans</option><option value="greek">ελληνικά</option><option value="danish">dansk</option><option value="amharic">amharic</option><option value="albanian">albanian</option><option value="lithuanian">Lietuva</option><option value="vietnamese">Tiếng Việt</option><option value="maltese">Malti</option><option value="finnish">suomi</option><option value="catalan">català</option><option value="croatian">hrvatski</option><option value="bosnian">bosnian</option><option value="polish">Polski</option><option value="latvian">latviešu</option><option value="maori">Maori</option></select><!--]--><!--]--><button class="toggle-color-mode-button" title="toggle color mode"><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items" aria-label="site navigation"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/knowledge/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/knowledge/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/knowledge/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/knowledge/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/knowledge/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/knowledge/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/knowledge/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/knowledge/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/solutions/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/solutions/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/solutions/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/solutions/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/solutions/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/solutions/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/solutions/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/solutions/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Paper"><span class="title">Paper</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Paper"><span class="title">Paper</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/paper/LLM.html" aria-label="LLM"><!--[--><!--[--><!--]--> LLM <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/paper/Stable%20Diffusion.html" aria-label="Stable Diffusion"><!--[--><!--[--><!--]--> Stable Diffusion <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/paper/HCI.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/paper/HMI.html" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/paper/Robot.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link route-link-active" href="/posts/XR/paper/XR.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="About"><span class="title">About</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="About"><span class="title">About</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/mission.html" aria-label="Mission"><!--[--><!--[--><!--]--> Mission <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contribute.html" aria-label="How to Contribute"><!--[--><!--[--><!--]--> How to Contribute <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contact.html" aria-label="Contact Us"><!--[--><!--[--><!--]--> Contact Us <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><!--[--><h4 class="navbar-dropdown-subtitle"><span>Contributors</span></h4><ul class="navbar-dropdown-subitem-wrapper"><!--[--><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/AGI/Contributors.html" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Generative-Art/Contributors.html" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HCI/Contributors.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HMI/Contributors.html" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Human-Factor/Contributors.html" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Intelligent-Device/Contributors.html" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Interactive-Installations/Contributors.html" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Robot/Contributors.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Spatial-Interaction/Contributors.html" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/XR/Contributors.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="navbar-item"><a class="external-link" href="https://github.com/OpenDesignGlobal/OpenDesign" rel="noopener noreferrer" target="_blank" aria-label="Github"><!--[--><!--[--><!--]--> Github <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading">XR <!----></p><ul style="" class="sidebar-item-children"><!--[--><li><a class="route-link sidebar-item" href="#_2024-04-25" aria-label="2024-04-25"><!--[--><!--[--><!--]--> 2024-04-25 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-24" aria-label="2024-04-24"><!--[--><!--[--><!--]--> 2024-04-24 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-23" aria-label="2024-04-23"><!--[--><!--[--><!--]--> 2024-04-23 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-22" aria-label="2024-04-22"><!--[--><!--[--><!--]--> 2024-04-22 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-18" aria-label="2024-04-18"><!--[--><!--[--><!--]--> 2024-04-18 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-17" aria-label="2024-04-17"><!--[--><!--[--><!--]--> 2024-04-17 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-16" aria-label="2024-04-16"><!--[--><!--[--><!--]--> 2024-04-16 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-15" aria-label="2024-04-15"><!--[--><!--[--><!--]--> 2024-04-15 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-14" aria-label="2024-04-14"><!--[--><!--[--><!--]--> 2024-04-14 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-12" aria-label="2024-04-12"><!--[--><!--[--><!--]--> 2024-04-12 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-11" aria-label="2024-04-11"><!--[--><!--[--><!--]--> 2024-04-11 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-10" aria-label="2024-04-10"><!--[--><!--[--><!--]--> 2024-04-10 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-09" aria-label="2024-04-09"><!--[--><!--[--><!--]--> 2024-04-09 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-08" aria-label="2024-04-08"><!--[--><!--[--><!--]--> 2024-04-08 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-07" aria-label="2024-04-07"><!--[--><!--[--><!--]--> 2024-04-07 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-06" aria-label="2024-04-06"><!--[--><!--[--><!--]--> 2024-04-06 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-05" aria-label="2024-04-05"><!--[--><!--[--><!--]--> 2024-04-05 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-04" aria-label="2024-04-04"><!--[--><!--[--><!--]--> 2024-04-04 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-03" aria-label="2024-04-03"><!--[--><!--[--><!--]--> 2024-04-03 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-02" aria-label="2024-04-02"><!--[--><!--[--><!--]--> 2024-04-02 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-01" aria-label="2024-04-01"><!--[--><!--[--><!--]--> 2024-04-01 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-03-30" aria-label="2024-03-30"><!--[--><!--[--><!--]--> 2024-03-30 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-03-28" aria-label="2024-03-28"><!--[--><!--[--><!--]--> 2024-03-28 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-03-27" aria-label="2024-03-27"><!--[--><!--[--><!--]--> 2024-03-27 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-03-25" aria-label="2024-03-25"><!--[--><!--[--><!--]--> 2024-03-25 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-03-24" aria-label="2024-03-24"><!--[--><!--[--><!--]--> 2024-03-24 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-03-23" aria-label="2024-03-23"><!--[--><!--[--><!--]--> 2024-03-23 <!--[--><!--]--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><!--]--><div><h1 id="xr" tabindex="-1"><a class="header-anchor" href="#xr"><span>XR</span></a></h1><h2 id="_2024-04-25" tabindex="-1"><a class="header-anchor" href="#_2024-04-25"><span>2024-04-25</span></a></h2><h4 id="resvr-joint-rescaling-and-viewport-rendering-of-omnidirectional-images" tabindex="-1"><a class="header-anchor" href="#resvr-joint-rescaling-and-viewport-rendering-of-omnidirectional-images"><span>ResVR: Joint Rescaling and Viewport Rendering of Omnidirectional Images</span></a></h4><p><strong>Authors</strong>: Weiqi Li, Shijie Zhao, Bin Chen, Xinhua Cheng, Junlin Li, Li Zhang, Jian Zhang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.16825v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.16825v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: With the advent of virtual reality technology, omnidirectional image (ODI) rescaling techniques are increasingly embraced for reducing transmitted and stored file sizes while preserving high image quality. Despite this progress, current ODI rescaling methods predominantly focus on enhancing the quality of images in equirectangular projection (ERP) format, which overlooks the fact that the content viewed on head mounted displays (HMDs) is actually a rendered viewport instead of an ERP image. In this work, we emphasize that focusing solely on ERP quality results in inferior viewport visual experiences for users. Thus, we propose ResVR, which is the first comprehensive framework for the joint Rescaling and Viewport Rendering of ODIs. ResVR allows obtaining LR ERP images for transmission while rendering high-quality viewports for users to watch on HMDs. In our ResVR, a novel discrete pixel sampling strategy is developed to tackle the complex mapping between the viewport and ERP, enabling end-to-end training of ResVR pipeline. Furthermore, a spherical pixel shape representation technique is innovatively derived from spherical differentiation to significantly improve the visual quality of rendered viewports. Extensive experiments demonstrate that our ResVR outperforms existing methods in viewport rendering tasks across different fields of view, resolutions, and view directions while keeping a low transmission overhead.</p><h4 id="comparing-continuous-and-retrospective-emotion-ratings-in-remote-vr-study" tabindex="-1"><a class="header-anchor" href="#comparing-continuous-and-retrospective-emotion-ratings-in-remote-vr-study"><span>Comparing Continuous and Retrospective Emotion Ratings in Remote VR Study</span></a></h4><p><strong>Authors</strong>: Maximilian Warsinke, Tanja Kojić, Maurizio Vergari, Robert Spang, Jan-Niklas Voigt-Antons, Sebastian Möller</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.16487v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.16487v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This study investigates the feasibility of remote virtual reality (VR) studies conducted at home using VR headsets and video conferencing by deploying an experiment on emotion ratings. 20 participants used head-mounted displays to immerse themselves in 360{\deg} videos selected to evoke emotional responses. The research compares continuous ratings using a graphical interface to retrospective questionnaires on a digitized Likert Scale for measuring arousal and valence, both based on the self-assessment manikin (SAM). It was hypothesized that the two different rating methods would lead to significantly different values for both valence and arousal. The goal was to investigate whether continuous ratings during the experience would better reflect users&#39; emotions compared to the post-questionnaire by mitigating biases such as the peak-end rule. The results show significant differences with moderate to strong effect sizes for valence and no significant differences for arousal with low to moderate effect sizes. This indicates the need for further investigation of the methods used to assess emotion ratings in VR studies. Overall, this study is an example of a remotely conducted VR experiment, offering insights into methods for emotion elicitation in VR by varying the timing and interface of the rating.</p><h4 id="the-impact-of-social-environment-and-interaction-focus-on-user-experience-and-social-acceptability-of-an-augmented-reality-game" tabindex="-1"><a class="header-anchor" href="#the-impact-of-social-environment-and-interaction-focus-on-user-experience-and-social-acceptability-of-an-augmented-reality-game"><span>The Impact of Social Environment and Interaction Focus on User Experience and Social Acceptability of an Augmented Reality Game</span></a></h4><p><strong>Authors</strong>: Lorenzo Cocchia, Maurizio Vergari, Tanja Kojic, Francesco Vona, Sebastian Moller, Franca Garzotto, Jan-Niklas Voigt-Antons</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.16479v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.16479v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: One of the most promising technologies inside the Extended Reality (XR) spectrum is Augmented Reality. This technology is already in people&#39;s pockets regarding Mobile Augmented Reality with their smartphones. The scientific community still needs answers about how humans could and should interact in environments where perceived stimuli are different from fully physical or digital circumstances. Moreover, it is still being determined if people accept these new technologies in different social environments and interaction settings or if some obstacles could exist. This paper explores the impact of the Social Environment and the Focus of social interaction on users while playing a location-based augmented reality game, measuring it with user experience and social acceptance indicators. An empirical study in a within-subject fashion was performed in different social environments and under different settings of social interaction focus with N = 28 participants compiling self-reported questionnaires after playing a Scavenger Hunt in Augmented Reality. The measures from two different Social Environments (Crowded vs. Uncrowded) resulted in statistically relevant mean differences with indicators from the Social Acceptability dimension. Moreover, the analyses show statistically relevant differences between the variances from different degrees of Social Interaction Focus with Overall Social Presence, Perceived Psychological Engagement, Perceived Attentional Engagement, and Perceived Emotional Contagion. The results suggest that a location-based AR game played in different social environments and settings can influence the user experience&#39;s social dimension. Therefore, they should be carefully considered while designing immersive technological experiences in public spaces involving social interactions between players.</p><h4 id="impact-of-spatial-auditory-navigation-on-user-experience-during-augmented-outdoor-navigation-tasks" tabindex="-1"><a class="header-anchor" href="#impact-of-spatial-auditory-navigation-on-user-experience-during-augmented-outdoor-navigation-tasks"><span>Impact of spatial auditory navigation on user experience during augmented outdoor navigation tasks</span></a></h4><p><strong>Authors</strong>: Jan-Niklas Voigt-Antons, Zhirou Sun, Maurizio Vergari, Navid Ashrafi, Francesco Vona, Tanja Kojic</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.16473v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.16473v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The auditory sense of humans is important when it comes to navigation. The importance is especially high in cases when an object of interest is visually partly or fully covered. Interactions with users of technology are mainly focused on the visual domain of navigation tasks. This paper presents the results of a literature review and user study exploring the impact of spatial auditory navigation on user experience during an augmented outdoor navigation task. For the user test, participants used an augmented reality app guiding them to different locations with different digital augmentation. We conclude that the utilization of the auditory sense is yet still underrepresented in augmented reality applications. In the future, more usage scenarios for audio-augmented reality such as navigation will enhance user experience and interaction quality.</p><h2 id="_2024-04-24" tabindex="-1"><a class="header-anchor" href="#_2024-04-24"><span>2024-04-24</span></a></h2><h4 id="shared-boundary-interfaces-can-one-fit-all-a-controlled-study-on-virtual-reality-vs-touch-screen-interfaces-on-persons-with-neurodevelopmental-disorders" tabindex="-1"><a class="header-anchor" href="#shared-boundary-interfaces-can-one-fit-all-a-controlled-study-on-virtual-reality-vs-touch-screen-interfaces-on-persons-with-neurodevelopmental-disorders"><span>Shared Boundary Interfaces: can one fit all? A controlled study on virtual reality vs touch-screen interfaces on persons with Neurodevelopmental Disorders</span></a></h4><p><strong>Authors</strong>: Francesco Vona, Eleonora Beccaluva, Marco Mores, Franca Garzotto</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.15970v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.15970v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Technology presents a significant educational opportunity, particularly in enhancing emotional engagement and expanding learning and educational prospects for individuals with Neurodevelopmental Disorders (NDD). Virtual reality emerges as a promising tool for addressing such disorders, complemented by numerous touchscreen applications that have shown efficacy in fostering education and learning abilities. VR and touchscreen technologies represent diverse interface modalities. This study primarily investigates which interface, VR or touchscreen, more effectively facilitates food education for individuals with NDD. We compared learning outcomes via pre- and post-exposure questionnaires. To this end, we developed GEA, a dual-interface, user-friendly web application for Food Education, adaptable for either immersive use in a head-mounted display (HMD) or non-immersive use on a tablet. A controlled study was conducted to determine which interface better promotes learning. Over three sessions, the experimental group engaged with all GEA games in VR (condition A), while the control group interacted with the same games on a tablet (condition B). Results indicated a significant increase in post-questionnaire scores across subjects, averaging a 46% improvement. This enhancement was notably consistent between groups, with VR and Tablet groups showing 42% and 41% improvements, respectively.</p><h4 id="training-attention-skills-in-individuals-with-neurodevelopmental-disorders-using-virtual-reality-and-eye-tracking-technology" tabindex="-1"><a class="header-anchor" href="#training-attention-skills-in-individuals-with-neurodevelopmental-disorders-using-virtual-reality-and-eye-tracking-technology"><span>Training Attention Skills in Individuals with Neurodevelopmental Disorders using Virtual Reality and Eye-tracking technology</span></a></h4><p><strong>Authors</strong>: Alberto Patti, Francesco Vona, Anna Barberio, Marco Domenico Buttiglione, Ivan Crusco, Marco Mores, Franca Garzotto</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.15960v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.15960v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Neurodevelopmental disorders (NDD), encompassing conditions like Intellectual Disability, Attention Deficit Hyperactivity Disorder, and Autism Spectrum Disorder, present challenges across various cognitive capacities. Attention deficits are often common in individuals with NDD due to the sensory system dysfunction that characterizes these disorders. Consequently, limited attention capability can affect the overall quality of life and the ability to transfer knowledge from one circumstance to another. The literature has increasingly recognized the potential benefits of virtual reality (VR) in supporting NDD learning and rehabilitation due to its interactive and engaging nature, which is critical for consistent practice. In previous studies, we explored the usage of a VR application called Wildcard to enhance attention skills in persons with NDD. The application has been redesigned in this study, exploiting eye-tracking technology to enable novel and more fine-grade interactions. A four-week experiment with 38 NDD participants was conducted to evaluate its usability and effectiveness in improving Visual Attention Skills. Results show the usability and effectiveness of Wildcard in enhancing attention skills, advocating for continued exploration of VR and eye-tracking technology&#39;s potential in NDD interventions.</p><h4 id="blisscam-boosting-eye-tracking-efficiency-with-learned-in-sensor-sparse-sampling" tabindex="-1"><a class="header-anchor" href="#blisscam-boosting-eye-tracking-efficiency-with-learned-in-sensor-sparse-sampling"><span>BlissCam: Boosting Eye Tracking Efficiency with Learned In-Sensor Sparse Sampling</span></a></h4><p><strong>Authors</strong>: Yu Feng, Tianrui Ma, Yuhao Zhu, Xuan Zhang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.15733v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.15733v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Eye tracking is becoming an increasingly important task domain in emerging computing platforms such as Augmented/Virtual Reality (AR/VR). Today&#39;s eye tracking system suffers from long end-to-end tracking latency and can easily eat up half of the power budget of a mobile VR device. Most existing optimization efforts exclusively focus on the computation pipeline by optimizing the algorithm and/or designing dedicated accelerators while largely ignoring the front-end of any eye tracking pipeline: the image sensor. This paper makes a case for co-designing the imaging system with the computing system. In particular, we propose the notion of &quot;in-sensor sparse sampling&quot;, whereby the pixels are drastically downsampled (by 20x) within the sensor. Such in-sensor sampling enhances the overall tracking efficiency by significantly reducing 1) the power consumption of the sensor readout chain and sensor-host communication interfaces, two major power contributors, and 2) the work done on the host, which receives and operates on far fewer pixels. With careful reuse of existing pixel circuitry, our proposed BLISSCAM requires little hardware augmentation to support the in-sensor operations. Our synthesis results show up to 8.2x energy reduction and 1.4x latency reduction over existing eye tracking pipelines.</p><h2 id="_2024-04-23" tabindex="-1"><a class="header-anchor" href="#_2024-04-23"><span>2024-04-23</span></a></h2><h4 id="augmented-voices-an-augmented-reality-experience-highlighting-the-social-injustices-of-gender-based-violence-in-the-muslim-south-asian-diaspora" tabindex="-1"><a class="header-anchor" href="#augmented-voices-an-augmented-reality-experience-highlighting-the-social-injustices-of-gender-based-violence-in-the-muslim-south-asian-diaspora"><span>Augmented Voices: An Augmented Reality Experience Highlighting the Social Injustices of Gender-Based Violence in the Muslim South-Asian Diaspora</span></a></h4><p><strong>Authors</strong>: Hamida Khatri</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.15239v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.15239v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This paper delves into the distressing prevalence of gender-based violence (GBV) and its deep-seated psychological ramifications, particularly among Muslim South Asian women living in diasporic communities. Despite the gravity of GBV, these women often face formidable barriers in voicing their experiences and accessing support. &quot;Augmented Voices&quot; emerges as a technological beacon, harnessing the potential of augmented reality (AR) to bridge the digital and physical realms through mobile devices, enhancing the visibility of these often-silenced voices. With its technological motivation firmly anchored in the convergence of AR and real-world interactions, &quot;Augmented Voices&quot; offers a digital platform where storytelling acts as a catalyst, bringing to the fore the experiences shared by these women. By superimposing their narratives onto physical locations via Geographic Information System (GIS) Mapping, the application &quot;augments their voices&quot; in the diaspora, providing a conduit for expression and solidarity. This project, currently at its developmental stage, aspires to elevate the stories of GBV victims to a level where their struggles are not just heard but felt, forging a powerful connection between the user and the narrative. It is designed to transcend the limitations of conventional storytelling, creating an &quot;augmented&quot; reality where voices that are often muted by societal constraints can resonate powerfully. The project underscores the urgent imperative to confront GBV, catalyzing societal transformation and fostering robust support networks for those in the margins. It is a pioneering example of how technology can become a formidable ally in the fight for social justice and the empowerment of the oppressed. Additionally, this paper delves into the AR workflow illustrating its relevance and contribution to the broader theme of site-specific AR for social justice.</p><h4 id="virtual-takeovers-in-the-metaverse-interrogating-power-in-our-past-and-future-s-with-multi-layered-narratives" tabindex="-1"><a class="header-anchor" href="#virtual-takeovers-in-the-metaverse-interrogating-power-in-our-past-and-future-s-with-multi-layered-narratives"><span>Virtual Takeovers in the Metaverse: Interrogating Power in Our Past and Future(s) with Multi-Layered Narratives</span></a></h4><p><strong>Authors</strong>: Heather Snyder Quinn, Jessa Dickinson</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.15108v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.15108v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Mariah is an augmented reality (AR) mobile application that exposes power structures (e.g., capitalism, patriarchy, white supremacy) through storytelling and celebrates acts of resistance against them. People can use Mariah to &quot;legally trespass&quot; the metaverse as a form of protest. Mariah provides historical context to the user&#39;s physical surroundings by superimposing images and playing stories about people who have experienced, and resisted, injustice. We share two implementations of Mariah that raise questions about free speech and property rights in the metaverse: (1) a protest against museums accepting &quot;dirty money&quot; from the opioid epidemic; and (2) a commemoration of sites where people have resisted power structures. Mariah is a case study for how experimenting with a technology in non-sanctioned ways (i.e., &quot;hacking&quot;) can expose ways that it might interact with, and potentially amplify, existing power structures.</p><h4 id="dynamicity-aware-social-bot-detection-with-dynamic-graph-transformers" tabindex="-1"><a class="header-anchor" href="#dynamicity-aware-social-bot-detection-with-dynamic-graph-transformers"><span>Dynamicity-aware Social Bot Detection with Dynamic Graph Transformers</span></a></h4><p><strong>Authors</strong>: Buyun He, Yingguang Yang, Qi Wu, Hao Liu, Renyu Yang, Hao Peng, Xiang Wang, Yong Liao, Pengyuan Zhou</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.15070v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.15070v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Detecting social bots has evolved into a pivotal yet intricate task, aimed at combating the dissemination of misinformation and preserving the authenticity of online interactions. While earlier graph-based approaches, which leverage topological structure of social networks, yielded notable outcomes, they overlooked the inherent dynamicity of social networks -- In reality, they largely depicted the social network as a static graph and solely relied on its most recent state. Due to the absence of dynamicity modeling, such approaches are vulnerable to evasion, particularly when advanced social bots interact with other users to camouflage identities and escape detection. To tackle these challenges, we propose BotDGT, a novel framework that not only considers the topological structure, but also effectively incorporates dynamic nature of social network. Specifically, we characterize a social network as a dynamic graph. A structural module is employed to acquire topological information from each historical snapshot. Additionally, a temporal module is proposed to integrate historical context and model the evolving behavior patterns exhibited by social bots and legitimate users. Experimental results demonstrate the superiority of BotDGT against the leading methods that neglected the dynamic nature of social networks in terms of accuracy, recall, and F1-score.</p><h4 id="quantitative-evaluation-of-driver-s-situation-awareness-in-virtual-driving-through-eye-tracking-analysis" tabindex="-1"><a class="header-anchor" href="#quantitative-evaluation-of-driver-s-situation-awareness-in-virtual-driving-through-eye-tracking-analysis"><span>Quantitative Evaluation of driver&#39;s situation awareness in virtual driving through Eye tracking analysis</span></a></h4><p><strong>Authors</strong>: Yunxiang Jiang, Qing Xu, Kai Zhen, Yu Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.14817v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.14817v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In driving tasks, the driver&#39;s situation awareness of the surrounding scenario is crucial for safety driving. However, current methods of measuring situation awareness mostly rely on subjective questionnaires, which interrupt tasks and lack non-intrusive quantification. To address this issue, our study utilizes objective gaze motion data to provide an interference-free quantification method for situation awareness. Three quantitative scores are proposed to represent three different levels of awareness: perception, comprehension, and projection, and an overall score of situation awareness is also proposed based on above three scores. To validate our findings, we conducted experiments where subjects performed driving tasks in a virtual reality simulated environment. All the four proposed situation awareness scores have clearly shown a significant correlation with driving performance. The proposed not only illuminates a new path for understanding and evaluating the situation awareness but also offers a satisfying proxy for driving performance.</p><h2 id="_2024-04-22" tabindex="-1"><a class="header-anchor" href="#_2024-04-22"><span>2024-04-22</span></a></h2><h4 id="penn-slavery-project-s-augmented-reality-tour-augmenting-a-campus-to-reveal-a-hidden-history" tabindex="-1"><a class="header-anchor" href="#penn-slavery-project-s-augmented-reality-tour-augmenting-a-campus-to-reveal-a-hidden-history"><span>Penn &amp; Slavery Project&#39;s Augmented Reality Tour: Augmenting a Campus to Reveal a Hidden History</span></a></h4><p><strong>Authors</strong>: VanJessica Gladney, Breanna Moore, Kathleen Brown</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.14379v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.14379v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In 2006 and 2016, the University of Pennsylvania denied any ties to slavery. In 2017, a group of undergraduate researchers, led by Professor Kathleen Brown, investigated this claim. Initial research, focused on 18th century faculty and trustees who owned slaves, revealed deep connections between the university&#39;s history and the institution of slavery. These findings, and discussions amongst the researchers shaped the Penn and Slavery Project&#39;s goal of redefining complicity beyond ownership. Breanna Moore&#39;s contributions in PSP&#39;s second semester expanded the project&#39;s focus to include generational wealth gaps. In 2018, VanJessica Gladney served as the PSP&#39;s Public History Fellow and spread the project outreach in the greater Philadelphia area. That year, the PSP team began to design an augmented reality app as a Digital Interruption and an attempt to display the truth about Penn&#39;s history on its campus. Unfortunately, PSP faced delays due to COVID 19. Despite setbacks, the project persisted, engaging with activists and the wider community to confront historical injustices and modern inequalities.</p><h4 id="immersive-rover-control-and-obstacle-detection-based-on-extended-reality-and-artificial-intelligence" tabindex="-1"><a class="header-anchor" href="#immersive-rover-control-and-obstacle-detection-based-on-extended-reality-and-artificial-intelligence"><span>Immersive Rover Control and Obstacle Detection based on Extended Reality and Artificial Intelligence</span></a></h4><p><strong>Authors</strong>: Sofía Coloma, Alexandre Frantz, Dave van der Meer, Ernest Skrzypczyk, Andrej Orsula, Miguel Olivares-Mendez</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.14095v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.14095v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Lunar exploration has become a key focus, driving scientific and technological advances. Ongoing missions are deploying rovers to the surface of the Moon, targeting the far side and south pole. However, these terrains pose challenges, emphasizing the need for precise obstacles and resource detection to avoid mission risks. This work proposes a novel system that integrates eXtended Reality (XR) and Artificial Intelligence (AI) to teleoperate lunar rovers. It is capable of autonomously detecting rocks and recreating an immersive 3D virtual environment of the location of the robot. This system has been validated in a lunar laboratory to observe its advantages over traditional 2D-based teleoperation approaches</p><h2 id="_2024-04-18" tabindex="-1"><a class="header-anchor" href="#_2024-04-18"><span>2024-04-18</span></a></h2><h4 id="holographic-parallax-improves-3d-perceptual-realism" tabindex="-1"><a class="header-anchor" href="#holographic-parallax-improves-3d-perceptual-realism"><span>Holographic Parallax Improves 3D Perceptual Realism</span></a></h4><p><strong>Authors</strong>: Dongyeon Kim, Seung-Woo Nam, Suyeon Choi, Jong-Mo Seo, Gordon Wetzstein, Yoonchan Jeong</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.11810v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.11810v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Holographic near-eye displays are a promising technology to solve long-standing challenges in virtual and augmented reality display systems. Over the last few years, many different computer-generated holography (CGH) algorithms have been proposed that are supervised by different types of target content, such as 2.5D RGB-depth maps, 3D focal stacks, and 4D light fields. It is unclear, however, what the perceptual implications are of the choice of algorithm and target content type. In this work, we build a perceptual testbed of a full-color, high-quality holographic near-eye display. Under natural viewing conditions, we examine the effects of various CGH supervision formats and conduct user studies to assess their perceptual impacts on 3D realism. Our results indicate that CGH algorithms designed for specific viewpoints exhibit noticeable deficiencies in achieving 3D realism. In contrast, holograms incorporating parallax cues consistently outperform other formats across different viewing conditions, including the center of the eyebox. This finding is particularly interesting and suggests that the inclusion of parallax cues in CGH rendering plays a crucial role in enhancing the overall quality of the holographic experience. This work represents an initial stride towards delivering a perceptually realistic 3D experience with holographic near-eye displays.</p><h2 id="_2024-04-17" tabindex="-1"><a class="header-anchor" href="#_2024-04-17"><span>2024-04-17</span></a></h2><h4 id="establishing-a-baseline-for-gaze-driven-authentication-performance-in-vr-a-breadth-first-investigation-on-a-very-large-dataset" tabindex="-1"><a class="header-anchor" href="#establishing-a-baseline-for-gaze-driven-authentication-performance-in-vr-a-breadth-first-investigation-on-a-very-large-dataset"><span>Establishing a Baseline for Gaze-driven Authentication Performance in VR: A Breadth-First Investigation on a Very Large Dataset</span></a></h4><p><strong>Authors</strong>: Dillon Lohr, Michael J. Proulx, Oleg Komogortsev</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.11798v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.11798v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This paper performs the crucial work of establishing a baseline for gaze-driven authentication performance to begin answering fundamental research questions using a very large dataset of gaze recordings from 9202 people with a level of eye tracking (ET) signal quality equivalent to modern consumer-facing virtual reality (VR) platforms. The size of the employed dataset is at least an order-of-magnitude larger than any other dataset from previous related work. Binocular estimates of the optical and visual axes of the eyes and a minimum duration for enrollment and verification are required for our model to achieve a false rejection rate (FRR) of below 3% at a false acceptance rate (FAR) of 1 in 50,000. In terms of identification accuracy which decreases with gallery size, we estimate that our model would fall below chance-level accuracy for gallery sizes of 148,000 or more. Our major findings indicate that gaze authentication can be as accurate as required by the FIDO standard when driven by a state-of-the-art machine learning architecture and a sufficiently large training dataset.</p><h4 id="deep-pattern-network-for-click-through-rate-prediction" tabindex="-1"><a class="header-anchor" href="#deep-pattern-network-for-click-through-rate-prediction"><span>Deep Pattern Network for Click-Through Rate Prediction</span></a></h4><p><strong>Authors</strong>: Hengyu Zhang, Junwei Pan, Dapeng Liu, Jie Jiang, Xiu Li</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.11456v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.11456v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Click-through rate (CTR) prediction tasks play a pivotal role in real-world applications, particularly in recommendation systems and online advertising. A significant research branch in this domain focuses on user behavior modeling. Current research predominantly centers on modeling co-occurrence relationships between the target item and items previously interacted with by users in their historical data. However, this focus neglects the intricate modeling of user behavior patterns. In reality, the abundance of user interaction records encompasses diverse behavior patterns, indicative of a spectrum of habitual paradigms. These patterns harbor substantial potential to significantly enhance CTR prediction performance. To harness the informational potential within user behavior patterns, we extend Target Attention (TA) to Target Pattern Attention (TPA) to model pattern-level dependencies. Furthermore, three critical challenges demand attention: the inclusion of unrelated items within behavior patterns, data sparsity in behavior patterns, and computational complexity arising from numerous patterns. To address these challenges, we introduce the Deep Pattern Network (DPN), designed to comprehensively leverage information from user behavior patterns. DPN efficiently retrieves target-related user behavior patterns using a target-aware attention mechanism. Additionally, it contributes to refining user behavior patterns through a pre-training paradigm based on self-supervised learning while promoting dependency learning within sparse patterns. Our comprehensive experiments, conducted across three public datasets, substantiate the superior performance and broad compatibility of DPN.</p><h4 id="ar-for-sexual-violence-maintaining-ethical-balance-while-enhancing-empathy" tabindex="-1"><a class="header-anchor" href="#ar-for-sexual-violence-maintaining-ethical-balance-while-enhancing-empathy"><span>AR for Sexual Violence: Maintaining Ethical Balance While Enhancing Empathy</span></a></h4><p><strong>Authors</strong>: Chunwei Lin</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.11305v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.11305v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This study showcases an augmented reality (AR) experience designed to promote gender justice and increase awareness of sexual violence in Taiwan. By leveraging AR, this project overcomes the limitations of offline exhibitions on social issues by motivating the public to participate and enhancing their willingness to delve into the topic. The discussion explores how direct exposure to sexual violence can induce negative emotions and secondary trauma among users. It also suggests strategies for using AR to alleviate such issues, particularly by avoiding simulations of actual incidents.</p><h4 id="novel-view-synthesis-for-cinematic-anatomy-on-mobile-and-immersive-displays" tabindex="-1"><a class="header-anchor" href="#novel-view-synthesis-for-cinematic-anatomy-on-mobile-and-immersive-displays"><span>Novel View Synthesis for Cinematic Anatomy on Mobile and Immersive Displays</span></a></h4><p><strong>Authors</strong>: Simon Niedermayr, Christoph Neuhauser, Kaloian Petkov, Klaus Engel, Rüdiger Westermann</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.11285v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.11285v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Interactive photorealistic visualization of 3D anatomy (i.e., Cinematic Anatomy) is used in medical education to explain the structure of the human body. It is currently restricted to frontal teaching scenarios, where the demonstrator needs a powerful GPU and high-speed access to a large storage device where the dataset is hosted. We demonstrate the use of novel view synthesis via compressed 3D Gaussian splatting to overcome this restriction and to enable students to perform cinematic anatomy on lightweight mobile devices and in virtual reality environments. We present an automatic approach for finding a set of images that captures all potentially seen structures in the data. By mixing closeup views with images from a distance, the splat representation can recover structures up to the voxel resolution. The use of Mip-Splatting enables smooth transitions when the focal length is increased. Even for GB datasets, the final renderable representation can usually be compressed to less than 70 MB, enabling interactive rendering on low-end devices using rasterization.</p><h2 id="_2024-04-16" tabindex="-1"><a class="header-anchor" href="#_2024-04-16"><span>2024-04-16</span></a></h2><h4 id="a-plausibility-study-of-using-augmented-reality-in-the-ventriculoperitoneal-shunt-operations" tabindex="-1"><a class="header-anchor" href="#a-plausibility-study-of-using-augmented-reality-in-the-ventriculoperitoneal-shunt-operations"><span>A Plausibility Study of Using Augmented Reality in the Ventriculoperitoneal Shunt Operations</span></a></h4><p><strong>Authors</strong>: Tandin Dorji, Pakinee Aimmanee, Vich Yindeedej</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.10713v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.10713v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The field of augmented reality (AR) has undergone substantial growth, finding diverse applications in the medical industry. This paper delves into various techniques employed in medical surgeries, scrutinizing factors such as cost, implementation, and accessibility. The focus of this exploration is on AR-based solutions, with a particular emphasis on addressing challenges and proposing an innovative solution for ventriculoperitoneal shunt (VP) operations. The proposed solution introduces a novel flow in the pre-surgery phase, aiming to substantially reduce setup time and operation duration by creating 3D models of the skull and ventricles. Experiments are conducted where the models are visualized on a 3D- printed skull through an AR device, specifically the Microsoft HoloLens 2. The paper then conducts an in-depth analysis of this proposed solution, discussing its feasibility, advantages, limitations,and future implications.</p><h4 id="the-application-of-augmented-reality-ar-in-remote-work-and-education" tabindex="-1"><a class="header-anchor" href="#the-application-of-augmented-reality-ar-in-remote-work-and-education"><span>The application of Augmented Reality (AR) in Remote Work and Education</span></a></h4><p><strong>Authors</strong>: Keqin Li, Peng Xirui, Jintong Song, Bo Hong, Jin Wang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.10579v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.10579v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: With the rapid advancement of technology, Augmented Reality (AR) technology, known for its ability to deeply integrate virtual information with the real world, is gradually transforming traditional work modes and teaching methods. Particularly in the realms of remote work and online education, AR technology demonstrates a broad spectrum of application prospects. This paper delves into the application potential and actual effects of AR technology in remote work and education. Through a systematic literature review, this study outlines the key features, advantages, and challenges of AR technology. Based on theoretical analysis, it discusses the scientific basis and technical support that AR technology provides for enhancing remote work efficiency and promoting innovation in educational teaching models. Additionally, by designing an empirical research plan and analyzing experimental data, this article reveals the specific performance and influencing factors of AR technology in practical applications. Finally, based on the results of the experiments, this research summarizes the application value of AR technology in remote work and education, looks forward to its future development trends, and proposes forward-looking research directions and strategic suggestions, offering empirical foundation and theoretical guidance for further promoting the in-depth application of AR technology in related fields.</p><h4 id="teaching-chinese-sign-language-with-feedback-in-mixed-reality" tabindex="-1"><a class="header-anchor" href="#teaching-chinese-sign-language-with-feedback-in-mixed-reality"><span>Teaching Chinese Sign Language with Feedback in Mixed Reality</span></a></h4><p><strong>Authors</strong>: Hongli Wen, Yang Xu, Lin Li, Xudong Ru</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.10490v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.10490v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Traditional sign language teaching methods face challenges such as limited feedback and diverse learning scenarios. Although 2D resources lack real-time feedback, classroom teaching is constrained by a scarcity of teacher. Methods based on VR and AR have relatively primitive interaction feedback mechanisms. This study proposes an innovative teaching model that uses real-time monocular vision and mixed reality technology. First, we introduce an improved hand-posture reconstruction method to achieve sign language semantic retention and real-time feedback. Second, a ternary system evaluation algorithm is proposed for a comprehensive assessment, maintaining good consistency with experts in sign language. Furthermore, we use mixed reality technology to construct a scenario-based 3D sign language classroom and explore the user experience of scenario teaching. Overall, this paper presents a novel teaching method that provides an immersive learning experience, advanced posture reconstruction, and precise feedback, achieving positive feedback on user experience and learning effectiveness.</p><h4 id="referring-flexible-image-restoration" tabindex="-1"><a class="header-anchor" href="#referring-flexible-image-restoration"><span>Referring Flexible Image Restoration</span></a></h4><p><strong>Authors</strong>: Runwei Guan, Rongsheng Hu, Zhuhao Zhou, Tianlang Xue, Ka Lok Man, Jeremy Smith, Eng Gee Lim, Weiping Ding, Yutao Yue</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.10342v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.10342v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In reality, images often exhibit multiple degradations, such as rain and fog at night (triple degradations). However, in many cases, individuals may not want to remove all degradations, for instance, a blurry lens revealing a beautiful snowy landscape (double degradations). In such scenarios, people may only desire to deblur. These situations and requirements shed light on a new challenge in image restoration, where a model must perceive and remove specific degradation types specified by human commands in images with multiple degradations. We term this task Referring Flexible Image Restoration (RFIR). To address this, we first construct a large-scale synthetic dataset called RFIR, comprising 153,423 samples with the degraded image, text prompt for specific degradation removal and restored image. RFIR consists of five basic degradation types: blur, rain, haze, low light and snow while six main sub-categories are included for varying degrees of degradation removal. To tackle the challenge, we propose a novel transformer-based multi-task model named TransRFIR, which simultaneously perceives degradation types in the degraded image and removes specific degradation upon text prompt. TransRFIR is based on two devised attention modules, Multi-Head Agent Self-Attention (MHASA) and Multi-Head Agent Cross Attention (MHACA), where MHASA and MHACA introduce the agent token and reach the linear complexity, achieving lower computation cost than vanilla self-attention and cross-attention and obtaining competitive performances. Our TransRFIR achieves state-of-the-art performances compared with other counterparts and is proven as an effective architecture for image restoration. We release our project at https://github.com/GuanRunwei/FIR-CP.</p><h2 id="_2024-04-15" tabindex="-1"><a class="header-anchor" href="#_2024-04-15"><span>2024-04-15</span></a></h2><h4 id="shaping-realities-enhancing-3d-generative-ai-with-fabrication-constraints" tabindex="-1"><a class="header-anchor" href="#shaping-realities-enhancing-3d-generative-ai-with-fabrication-constraints"><span>Shaping Realities: Enhancing 3D Generative AI with Fabrication Constraints</span></a></h4><p><strong>Authors</strong>: Faraz Faruqi, Yingtao Tian, Vrushank Phadnis, Varun Jampani, Stefanie Mueller</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.10142v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.10142v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Generative AI tools are becoming more prevalent in 3D modeling, enabling users to manipulate or create new models with text or images as inputs. This makes it easier for users to rapidly customize and iterate on their 3D designs and explore new creative ideas. These methods focus on the aesthetic quality of the 3D models, refining them to look similar to the prompts provided by the user. However, when creating 3D models intended for fabrication, designers need to trade-off the aesthetic qualities of a 3D model with their intended physical properties. To be functional post-fabrication, 3D models have to satisfy structural constraints informed by physical principles. Currently, such requirements are not enforced by generative AI tools. This leads to the development of aesthetically appealing, but potentially non-functional 3D geometry, that would be hard to fabricate and use in the real world. This workshop paper highlights the limitations of generative AI tools in translating digital creations into the physical world and proposes new augmentations to generative AI tools for creating physically viable 3D models. We advocate for the development of tools that manipulate or generate 3D models by considering not only the aesthetic appearance but also using physical properties as constraints. This exploration seeks to bridge the gap between digital creativity and real-world applicability, extending the creative potential of generative AI into the tangible domain.</p><h4 id="edgerelight360-text-conditioned-360-degree-hdr-image-generation-for-real-time-on-device-video-portrait-relighting" tabindex="-1"><a class="header-anchor" href="#edgerelight360-text-conditioned-360-degree-hdr-image-generation-for-real-time-on-device-video-portrait-relighting"><span>EdgeRelight360: Text-Conditioned 360-Degree HDR Image Generation for Real-Time On-Device Video Portrait Relighting</span></a></h4><p><strong>Authors</strong>: Min-Hui Lin, Mahesh Reddy, Guillaume Berger, Michel Sarkis, Fatih Porikli, Ning Bi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09918v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09918v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this paper, we present EdgeRelight360, an approach for real-time video portrait relighting on mobile devices, utilizing text-conditioned generation of 360-degree high dynamic range image (HDRI) maps. Our method proposes a diffusion-based text-to-360-degree image generation in the HDR domain, taking advantage of the HDR10 standard. This technique facilitates the generation of high-quality, realistic lighting conditions from textual descriptions, offering flexibility and control in portrait video relighting task. Unlike the previous relighting frameworks, our proposed system performs video relighting directly on-device, enabling real-time inference with real 360-degree HDRI maps. This on-device processing ensures both privacy and guarantees low runtime, providing an immediate response to changes in lighting conditions or user inputs. Our approach paves the way for new possibilities in real-time video applications, including video conferencing, gaming, and augmented reality, by allowing dynamic, text-based control of lighting conditions.</p><h4 id="software-development-in-the-age-of-llms-and-xr" tabindex="-1"><a class="header-anchor" href="#software-development-in-the-age-of-llms-and-xr"><span>Software development in the age of LLMs and XR</span></a></h4><p><strong>Authors</strong>: Jesus M. Gonzalez-Barahona</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09789v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09789v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Let&#39;s imagine that in a few years generative AI has changed software development dramatically, taking charge of most of the programming tasks. Let&#39;s also assume that extended reality devices became ubiquitous, being the preferred interface for interacting with computers. This paper proposes how this situation would impact IDEs, by exploring how the development process would be affected, and analyzing which tools would be needed for supporting developers.</p><h4 id="dynamic-ego-velocity-estimation-using-moving-mmwave-radar-a-phase-based-approach" tabindex="-1"><a class="header-anchor" href="#dynamic-ego-velocity-estimation-using-moving-mmwave-radar-a-phase-based-approach"><span>Dynamic Ego-Velocity estimation Using Moving mmWave Radar: A Phase-Based Approach</span></a></h4><p><strong>Authors</strong>: Argha Sen, Soham Chakraborty, Soham Tripathy, Sandip Chakraborty</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09691v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09691v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Precise ego-motion measurement is crucial for various applications, including robotics, augmented reality, and autonomous navigation. In this poster, we propose mmPhase, an odometry framework based on single-chip millimetre-wave (mmWave) radar for robust ego-motion estimation in mobile platforms without requiring additional modalities like the visual, wheel, or inertial odometry. mmPhase leverages a phase-based velocity estimation approach to overcome the limitations of conventional doppler resolution. For real-world evaluations of mmPhase we have developed an ego-vehicle prototype. Compared to the state-of-the-art baselines, mmPhase shows superior performance in ego-velocity estimation.</p><h4 id="aam-vdt-vehicle-digital-twin-for-tele-operations-in-advanced-air-mobility" tabindex="-1"><a class="header-anchor" href="#aam-vdt-vehicle-digital-twin-for-tele-operations-in-advanced-air-mobility"><span>AAM-VDT: Vehicle Digital Twin for Tele-Operations in Advanced Air Mobility</span></a></h4><p><strong>Authors</strong>: Tuan Anh Nguyen, Taeho Kwag, Vinh Pham, Viet Nghia Nguyen, Jeongseok Hyun, Minseok Jang, Jae-Woo Lee</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09621v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09621v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This study advanced tele-operations in Advanced Air Mobility (AAM) through the creation of a Vehicle Digital Twin (VDT) system for eVTOL aircraft, tailored to enhance remote control safety and efficiency, especially for Beyond Visual Line of Sight (BVLOS) operations. By synergizing digital twin technology with immersive Virtual Reality (VR) interfaces, we notably elevate situational awareness and control precision for remote operators. Our VDT framework integrates immersive tele-operation with a high-fidelity aerodynamic database, essential for authentically simulating flight dynamics and control tactics. At the heart of our methodology lies an eVTOL&#39;s high-fidelity digital replica, placed within a simulated reality that accurately reflects physical laws, enabling operators to manage the aircraft via a master-slave dynamic, substantially outperforming traditional 2D interfaces. The architecture of the designed system ensures seamless interaction between the operator, the digital twin, and the actual aircraft, facilitating exact, instantaneous feedback. Experimental assessments, involving propulsion data gathering, simulation database fidelity verification, and tele-operation testing, verify the system&#39;s capability in precise control command transmission and maintaining the digital-physical eVTOL synchronization. Our findings underscore the VDT system&#39;s potential in augmenting AAM efficiency and safety, paving the way for broader digital twin application in autonomous aerial vehicles.</p><h4 id="virtually-enriched-nyu-depth-v2-dataset-for-monocular-depth-estimation-do-we-need-artificial-augmentation" tabindex="-1"><a class="header-anchor" href="#virtually-enriched-nyu-depth-v2-dataset-for-monocular-depth-estimation-do-we-need-artificial-augmentation"><span>Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation: Do We Need Artificial Augmentation?</span></a></h4><p><strong>Authors</strong>: Dmitry Ignatov, Andrey Ignatov, Radu Timofte</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09469v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09469v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We present ANYU, a new virtually augmented version of the NYU depth v2 dataset, designed for monocular depth estimation. In contrast to the well-known approach where full 3D scenes of a virtual world are utilized to generate artificial datasets, ANYU was created by incorporating RGB-D representations of virtual reality objects into the original NYU depth v2 images. We specifically did not match each generated virtual object with an appropriate texture and a suitable location within the real-world image. Instead, an assignment of texture, location, lighting, and other rendering parameters was randomized to maximize a diversity of the training data, and to show that it is randomness that can improve the generalizing ability of a dataset. By conducting extensive experiments with our virtually modified dataset and validating on the original NYU depth v2 and iBims-1 benchmarks, we show that ANYU improves the monocular depth estimation performance and generalization of deep neural networks with considerably different architectures, especially for the current state-of-the-art VPD model. To the best of our knowledge, this is the first work that augments a real-world dataset with randomly generated virtual 3D objects for monocular depth estimation. We make our ANYU dataset publicly available in two training configurations with 10% and 100% additional synthetically enriched RGB-D pairs of training images, respectively, for efficient training and empirical exploration of virtual augmentation at https://github.com/ABrain-One/ANYU</p><h2 id="_2024-04-14" tabindex="-1"><a class="header-anchor" href="#_2024-04-14"><span>2024-04-14</span></a></h2><h4 id="evaluating-the-efficacy-of-haptic-feedback-360°-treadmill-integrated-virtual-reality-framework-and-longitudinal-training-on-decision-making-performance-in-a-complex-search-and-shoot-simulation" tabindex="-1"><a class="header-anchor" href="#evaluating-the-efficacy-of-haptic-feedback-360°-treadmill-integrated-virtual-reality-framework-and-longitudinal-training-on-decision-making-performance-in-a-complex-search-and-shoot-simulation"><span>Evaluating the efficacy of haptic feedback, 360° treadmill-integrated Virtual Reality framework and longitudinal training on decision-making performance in a complex search-and-shoot simulation</span></a></h4><p><strong>Authors</strong>: Akash K Rao, Arnav Bhavsar, Shubhajit Roy Chowdhury, Sushil Chandra, Ramsingh Negi, Prakash Duraisamy, Varun Dutt</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09147v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09147v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Virtual Reality (VR) has made significant strides, offering users a multitude of ways to interact with virtual environments. Each sensory modality in VR provides distinct inputs and interactions, enhancing the user&#39;s immersion and presence. However, the potential of additional sensory modalities, such as haptic feedback and 360{\deg} locomotion, to improve decision-making performance has not been thoroughly investigated. This study addresses this gap by evaluating the impact of a haptic feedback, 360{\deg} locomotion-integrated VR framework and longitudinal, heterogeneous training on decision-making performance in a complex search-and-shoot simulation. The study involved 32 participants from a defence simulation base in India, who were randomly divided into two groups: experimental (haptic feedback, 360{\deg} locomotion-integrated VR framework with longitudinal, heterogeneous training) and placebo control (longitudinal, heterogeneous VR training without extrasensory modalities). The experiment lasted 10 days. On Day 1, all subjects executed a search-and-shoot simulation closely replicating the elements/situations in the real world. From Day 2 to Day 9, the subjects underwent heterogeneous training, imparted by the design of various complexity levels in the simulation using changes in behavioral attributes/artificial intelligence of the enemies. On Day 10, they repeated the search-and-shoot simulation executed on Day 1. The results showed that the experimental group experienced a gradual increase in presence, immersion, and engagement compared to the placebo control group. However, there was no significant difference in decision-making performance between the two groups on day 10. We intend to use these findings to design multisensory VR training frameworks that enhance engagement levels and decision-making performance.</p><h4 id="exploring-generative-ai-for-sim2real-in-driving-data-synthesis" tabindex="-1"><a class="header-anchor" href="#exploring-generative-ai-for-sim2real-in-driving-data-synthesis"><span>Exploring Generative AI for Sim2Real in Driving Data Synthesis</span></a></h4><p><strong>Authors</strong>: Haonan Zhao, Yiting Wang, Thomas Bashford-Rogers, Valentina Donzella, Kurt Debattista</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09111v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09111v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Datasets are essential for training and testing vehicle perception algorithms. However, the collection and annotation of real-world images is time-consuming and expensive. Driving simulators offer a solution by automatically generating various driving scenarios with corresponding annotations, but the simulation-to-reality (Sim2Real) domain gap remains a challenge. While most of the Generative Artificial Intelligence (AI) follows the de facto Generative Adversarial Nets (GANs)-based methods, the recent emerging diffusion probabilistic models have not been fully explored in mitigating Sim2Real challenges for driving data synthesis. To explore the performance, this paper applied three different generative AI methods to leverage semantic label maps from a driving simulator as a bridge for the creation of realistic datasets. A comparative analysis of these methods is presented from the perspective of image quality and perception. New synthetic datasets, which include driving images and auto-generated high-quality annotations, are produced with low costs and high scene variability. The experimental results show that although GAN-based methods are adept at generating high-quality images when provided with manually annotated labels, ControlNet produces synthetic datasets with fewer artefacts and more structural fidelity when using simulator-generated labels. This suggests that the diffusion-based approach may provide improved stability and an alternative method for addressing Sim2Real challenges.</p><h2 id="_2024-04-12" tabindex="-1"><a class="header-anchor" href="#_2024-04-12"><span>2024-04-12</span></a></h2><h4 id="single-image-driven-3d-viewpoint-training-data-augmentation-for-effective-wine-label-recognition" tabindex="-1"><a class="header-anchor" href="#single-image-driven-3d-viewpoint-training-data-augmentation-for-effective-wine-label-recognition"><span>Single-image driven 3d viewpoint training data augmentation for effective wine label recognition</span></a></h4><p><strong>Authors</strong>: Yueh-Cheng Huang, Hsin-Yi Chen, Cheng-Jui Hung, Jen-Hui Chuang, Jenq-Neng Hwang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08820v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08820v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Confronting the critical challenge of insufficient training data in the field of complex image recognition, this paper introduces a novel 3D viewpoint augmentation technique specifically tailored for wine label recognition. This method enhances deep learning model performance by generating visually realistic training samples from a single real-world wine label image, overcoming the challenges posed by the intricate combinations of text and logos. Classical Generative Adversarial Network (GAN) methods fall short in synthesizing such intricate content combination. Our proposed solution leverages time-tested computer vision and image processing strategies to expand our training dataset, thereby broadening the range of training samples for deep learning applications. This innovative approach to data augmentation circumvents the constraints of limited training resources. Using the augmented training images through batch-all triplet metric learning on a Vision Transformer (ViT) architecture, we can get the most discriminative embedding features for every wine label, enabling us to perform one-shot recognition of existing wine labels in the training classes or future newly collected wine labels unavailable in the training. Experimental results show a significant increase in recognition accuracy over conventional 2D data augmentation techniques.</p><h4 id="_3d-human-scan-with-a-moving-event-camera" tabindex="-1"><a class="header-anchor" href="#_3d-human-scan-with-a-moving-event-camera"><span>3D Human Scan With A Moving Event Camera</span></a></h4><p><strong>Authors</strong>: Kai Kohyama, Shintaro Shiba, Yoshimitsu Aoki</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08504v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08504v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Capturing the 3D human body is one of the important tasks in computer vision with a wide range of applications such as virtual reality and sports analysis. However, conventional frame cameras are limited by their temporal resolution and dynamic range, which imposes constraints in real-world application setups. Event cameras have the advantages of high temporal resolution and high dynamic range (HDR), but the development of event-based methods is necessary to handle data with different characteristics. This paper proposes a novel event-based method for 3D pose estimation and human mesh recovery. Prior work on event-based human mesh recovery require frames (images) as well as event data. The proposed method solely relies on events; it carves 3D voxels by moving the event camera around a stationary body, reconstructs the human pose and mesh by attenuated rays, and fit statistical body models, preserving high-frequency details. The experimental results show that the proposed method outperforms conventional frame-based methods in the estimation accuracy of both pose and body mesh. We also demonstrate results in challenging situations where a conventional camera has motion blur. This is the first to demonstrate event-only human mesh recovery, and we hope that it is the first step toward achieving robust and accurate 3D human body scanning from vision sensors.</p><h4 id="occgaussian-3d-gaussian-splatting-for-occluded-human-rendering" tabindex="-1"><a class="header-anchor" href="#occgaussian-3d-gaussian-splatting-for-occluded-human-rendering"><span>OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering</span></a></h4><p><strong>Authors</strong>: Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08449v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08449v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Rendering dynamic 3D human from monocular videos is crucial for various applications such as virtual reality and digital entertainment. Most methods assume the people is in an unobstructed scene, while various objects may cause the occlusion of body parts in real-life scenarios. Previous method utilizing NeRF for surface rendering to recover the occluded areas, but it requiring more than one day to train and several seconds to render, failing to meet the requirements of real-time interactive applications. To address these issues, we propose OccGaussian based on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D Gaussian distributions in the canonical space, and we perform occlusion feature query at occluded regions, the aggregated pixel-align feature is extracted to compensate for the missing information. Then we use Gaussian Feature MLP to further process the feature along with the occlusion-aware loss functions to better perceive the occluded area. Extensive experiments both in simulated and real-world occlusions, demonstrate that our method achieves comparable or even superior performance compared to the state-of-the-art method. And we improving training and inference speeds by 250x and 800x, respectively. Our code will be available for research purposes.</p><h4 id="facefiltersense-a-filter-resistant-face-recognition-and-facial-attribute-analysis-framework" tabindex="-1"><a class="header-anchor" href="#facefiltersense-a-filter-resistant-face-recognition-and-facial-attribute-analysis-framework"><span>FaceFilterSense: A Filter-Resistant Face Recognition and Facial Attribute Analysis Framework</span></a></h4><p><strong>Authors</strong>: Shubham Tiwari, Yash Sethia, Ritesh Kumar, Ashwani Tanwar, Rudresh Dwivedi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08277v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08277v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: With the advent of social media, fun selfie filters have come into tremendous mainstream use affecting the functioning of facial biometric systems as well as image recognition systems. These filters vary from beautification filters and Augmented Reality (AR)-based filters to filters that modify facial landmarks. Hence, there is a need to assess the impact of such filters on the performance of existing face recognition systems. The limitation associated with existing solutions is that these solutions focus more on the beautification filters. However, the current AR-based filters and filters which distort facial key points are in vogue recently and make the faces highly unrecognizable even to the naked eye. Also, the filters considered are mostly obsolete with limited variations. To mitigate these limitations, we aim to perform a holistic impact analysis of the latest filters and propose an user recognition model with the filtered images. We have utilized a benchmark dataset for baseline images, and applied the latest filters over them to generate a beautified/filtered dataset. Next, we have introduced a model FaceFilterNet for beautified user recognition. In this framework, we also utilize our model to comment on various attributes of the person including age, gender, and ethnicity. In addition, we have also presented a filter-wise impact analysis on face recognition, age estimation, gender, and ethnicity prediction. The proposed method affirms the efficacy of our dataset with an accuracy of 87.25% and an optimal accuracy for facial attribute analysis.</p><h4 id="gazepointar-a-context-aware-multimodal-voice-assistant-for-pronoun-disambiguation-in-wearable-augmented-reality" tabindex="-1"><a class="header-anchor" href="#gazepointar-a-context-aware-multimodal-voice-assistant-for-pronoun-disambiguation-in-wearable-augmented-reality"><span>GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality</span></a></h4><p><strong>Authors</strong>: Jaewook Lee, Jun Wang, Elizabeth Brown, Liam Chu, Sebastian S. Rodriguez, Jon E. Froehlich</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08213v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08213v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Voice assistants (VAs) like Siri and Alexa are transforming human-computer interaction; however, they lack awareness of users&#39; spatiotemporal context, resulting in limited performance and unnatural dialogue. We introduce GazePointAR, a fully-functional context-aware VA for wearable augmented reality that leverages eye gaze, pointing gestures, and conversation history to disambiguate speech queries. With GazePointAR, users can ask &quot;what&#39;s over there?&quot; or &quot;how do I solve this math problem?&quot; simply by looking and/or pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1) comparing GazePointAR to two commercial systems; (2) examining GazePointAR&#39;s pronoun disambiguation across three tasks; (3) and an open-ended phase where participants could suggest and try their own context-sensitive queries. Participants appreciated the naturalness and human-like nature of pronoun-driven queries, although sometimes pronoun use was counter-intuitive. We then iterated on GazePointAR and conducted a first-person diary study examining how GazePointAR performs in-the-wild. We conclude by enumerating limitations and design considerations for future context-aware VAs.</p><h4 id="the-clarkston-ar-gateways-project-anchoring-refugee-presence-and-narratives-in-a-small-town" tabindex="-1"><a class="header-anchor" href="#the-clarkston-ar-gateways-project-anchoring-refugee-presence-and-narratives-in-a-small-town"><span>The Clarkston AR Gateways Project: Anchoring Refugee Presence and Narratives in a Small Town</span></a></h4><p><strong>Authors</strong>: Joshua A. Fisher, Fernando Rochaix</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08179v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08179v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This paper outlines the Clarkston AR Gateways Project, a speculative process and artifact entering its second phase, where Augmented Reality (AR) will be used to amplify the diverse narratives of Clarkston, Georgia&#39;s refugee community. Focused on anchoring their stories and presence into the town&#39;s physical and digital landscapes, the project employs a participatory co-design approach, engaging directly with community members. This placemaking effort aims to uplift refugees by teaching them AR development skills that help them more autonomously express and elevate their voices through public art. The result is hoped to be AR experiences that not only challenge prevailing narratives but also celebrate the tapestry of cultures in the small town. This work is supported through AR&#39;s unique affordance for users to situate their experiences as interactive narratives within public spaces. Such site-specific AR interactive stories can encourage interactions within those spaces that shift how they are conceived, perceived, and experienced. This process of refugee-driven AR creation reflexively alters the space and affirms their presence and agency. The project&#39;s second phase aims to establish a model adaptable to diverse, refugee-inclusive communities, demonstrating how AR storytelling can be a powerful tool for cultural orientation and celebration.</p><h2 id="_2024-04-11" tabindex="-1"><a class="header-anchor" href="#_2024-04-11"><span>2024-04-11</span></a></h2><h4 id="rassar-room-accessibility-and-safety-scanning-in-augmented-reality" tabindex="-1"><a class="header-anchor" href="#rassar-room-accessibility-and-safety-scanning-in-augmented-reality"><span>RASSAR: Room Accessibility and Safety Scanning in Augmented Reality</span></a></h4><p><strong>Authors</strong>: Xia Su, Han Zhang, Kaiming Cheng, Jaewook Lee, Qiaochu Liu, Wyatt Olson, Jon Froehlich</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07479v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07479v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The safety and accessibility of our homes is critical to quality of life and evolves as we age, become ill, host guests, or experience life events such as having children. Researchers and health professionals have created assessment instruments such as checklists that enable homeowners and trained experts to identify and mitigate safety and access issues. With advances in computer vision, augmented reality (AR), and mobile sensors, new approaches are now possible. We introduce RASSAR, a mobile AR application for semi-automatically identifying, localizing, and visualizing indoor accessibility and safety issues such as an inaccessible table height or unsafe loose rugs using LiDAR and real-time computer vision. We present findings from three studies: a formative study with 18 participants across five stakeholder groups to inform the design of RASSAR, a technical performance evaluation across ten homes demonstrating state-of-the-art performance, and a user study with six stakeholders. We close with a discussion of future AI-based indoor accessibility assessment tools, RASSAR&#39;s extensibility, and key application scenarios.</p><h2 id="_2024-04-10" tabindex="-1"><a class="header-anchor" href="#_2024-04-10"><span>2024-04-10</span></a></h2><h4 id="mixed-reality-heritage-performance-as-a-decolonising-tool-for-heritage-sites" tabindex="-1"><a class="header-anchor" href="#mixed-reality-heritage-performance-as-a-decolonising-tool-for-heritage-sites"><span>Mixed Reality Heritage Performance As a Decolonising Tool for Heritage Sites</span></a></h4><p><strong>Authors</strong>: Mariza Dima, Damon Daylamani-Zad, Vangelis Lympouridis</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07348v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07348v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this paper we introduce two world-first Mixed Reality (MR) experiences that fuse smart AR glasses and live theatre and take place in a heritage site with the purpose to reveal the site&#39;s hidden and difficult histories about slavery. We term these unique general audience experiences Mixed Reality Heritage Performances (MRHP). Along with the development of our initial two performances we designed and developed a tool and guidelines that can help heritage organisations with their decolonising process by critically engaging the public with under-represented voices and viewpoints of troubled European and colonial narratives. The evaluations showed the embodied and affective potential of MRHP to attract and educate heritage audiences visitors. Insights of the design process are being formulated into an extensive design toolkit that aims to support experience design, theatre and heritage professionals to collaboratively carry out similar projects.</p><h4 id="evaluating-navigation-and-comparison-performance-of-computational-notebooks-on-desktop-and-in-virtual-reality" tabindex="-1"><a class="header-anchor" href="#evaluating-navigation-and-comparison-performance-of-computational-notebooks-on-desktop-and-in-virtual-reality"><span>Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality</span></a></h4><p><strong>Authors</strong>: Sungwon In, Erick Krokos, Kirsten Whitley, Chris North, Yalong Yang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07161v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07161v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The computational notebook serves as a versatile tool for data analysis. However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches. With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows. Virtual reality, in particular, has demonstrated its potential in interactive data visualizations. In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring. We focus on the navigation and comparison aspects as they are primitive components in analysts&#39; workflow. To further improve comparison, we have designed and implemented a Branching&amp;Merging functionality. We tested computational notebooks on the desktop and in VR, both with and without the added Branching&amp;Merging capability. We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison.</p><h4 id="exploring-physiological-responses-in-virtual-reality-based-interventions-for-autism-spectrum-disorder-a-data-driven-investigation" tabindex="-1"><a class="header-anchor" href="#exploring-physiological-responses-in-virtual-reality-based-interventions-for-autism-spectrum-disorder-a-data-driven-investigation"><span>Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorder: A Data-Driven Investigation</span></a></h4><p><strong>Authors</strong>: Gianpaolo Alvari, Ersilia Vallefuoco, Melanie Cristofolini, Elio Salvadori, Marco Dianti, Alessia Moltani, Davide Dal Castello, Paola Venuti, Cesare Furlanello</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07159v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07159v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Virtual Reality (VR) has emerged as a promising tool for enhancing social skills and emotional well-being in individuals with Autism Spectrum Disorder (ASD). Through a technical exploration, this study employs a multiplayer serious gaming environment within VR, engaging 34 individuals diagnosed with ASD and employing high-precision biosensors for a comprehensive view of the participants&#39; arousal and responses during the VR sessions. Participants were subjected to a series of 3 virtual scenarios designed in collaboration with stakeholders and clinical experts to promote socio-cognitive skills and emotional regulation in a controlled and structured virtual environment. We combined the framework with wearable non-invasive sensors for bio-signal acquisition, focusing on the collection of heart rate variability, and respiratory patterns to monitor participants behaviors. Further, behavioral assessments were conducted using observation and semi-structured interviews, with the data analyzed in conjunction with physiological measures to identify correlations and explore digital-intervention efficacy. Preliminary analysis revealed significant correlations between physiological responses and behavioral outcomes, indicating the potential of physiological feedback to enhance VR-based interventions for ASD. The study demonstrated the feasibility of using real-time data to adapt virtual scenarios, suggesting a promising avenue to support personalized therapy. The integration of quantitative physiological feedback into digital platforms represents a forward step in the personalized intervention for ASD. By leveraging real-time data to adjust therapeutic content, this approach promises to enhance the efficacy and engagement of digital-based therapies.</p><h4 id="sara-smart-ai-reading-assistant-for-reading-comprehension" tabindex="-1"><a class="header-anchor" href="#sara-smart-ai-reading-assistant-for-reading-comprehension"><span>SARA: Smart AI Reading Assistant for Reading Comprehension</span></a></h4><p><strong>Authors</strong>: Enkeleda Thaqi, Mohamed Mantawy, Enkelejda Kasneci</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06906v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06906v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: SARA integrates Eye Tracking and state-of-the-art large language models in a mixed reality framework to enhance the reading experience by providing personalized assistance in real-time. By tracking eye movements, SARA identifies the text segments that attract the user&#39;s attention the most and potentially indicate uncertain areas and comprehension issues. The process involves these key steps: text detection and extraction, gaze tracking and alignment, and assessment of detected reading difficulty. The results are customized solutions presented directly within the user&#39;s field of view as virtual overlays on identified difficult text areas. This support enables users to overcome challenges like unfamiliar vocabulary and complex sentences by offering additional context, rephrased solutions, and multilingual help. SARA&#39;s innovative approach demonstrates it has the potential to transform the reading experience and improve reading proficiency.</p><h4 id="dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting" tabindex="-1"><a class="header-anchor" href="#dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting"><span>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting</span></a></h4><p><strong>Authors</strong>: Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06903v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06903v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary &quot;flat&quot; (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: http://dreamscene360.github.io/</p><h4 id="sparse-points-to-dense-clouds-enhancing-3d-detection-with-limited-lidar-data" tabindex="-1"><a class="header-anchor" href="#sparse-points-to-dense-clouds-enhancing-3d-detection-with-limited-lidar-data"><span>Sparse Points to Dense Clouds: Enhancing 3D Detection with Limited LiDAR Data</span></a></h4><p><strong>Authors</strong>: Aakash Kumar, Chen Chen, Ajmal Mian, Neils Lobo, Mubarak Shah</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06715v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06715v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: 3D detection is a critical task that enables machines to identify and locate objects in three-dimensional space. It has a broad range of applications in several fields, including autonomous driving, robotics and augmented reality. Monocular 3D detection is attractive as it requires only a single camera, however, it lacks the accuracy and robustness required for real world applications. High resolution LiDAR on the other hand, can be expensive and lead to interference problems in heavy traffic given their active transmissions. We propose a balanced approach that combines the advantages of monocular and point cloud-based 3D detection. Our method requires only a small number of 3D points, that can be obtained from a low-cost, low-resolution sensor. Specifically, we use only 512 points, which is just 1% of a full LiDAR frame in the KITTI dataset. Our method reconstructs a complete 3D point cloud from this limited 3D information combined with a single image. The reconstructed 3D point cloud and corresponding image can be used by any multi-modal off-the-shelf detector for 3D object detection. By using the proposed network architecture with an off-the-shelf multi-modal 3D detector, the accuracy of 3D detection improves by 20% compared to the state-of-the-art monocular detection methods and 6% to 9% compare to the baseline multi-modal methods on KITTI and JackRabbot datasets.</p><h2 id="_2024-04-09" tabindex="-1"><a class="header-anchor" href="#_2024-04-09"><span>2024-04-09</span></a></h2><h4 id="matching-2d-images-in-3d-metric-relative-pose-from-metric-correspondences" tabindex="-1"><a class="header-anchor" href="#matching-2d-images-in-3d-metric-relative-pose-from-metric-correspondences"><span>Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences</span></a></h4><p><strong>Authors</strong>: Axel Barroso-Laguna, Sowmya Munukutla, Victor Adrian Prisacariu, Eric Brachmann</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06337v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06337v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Given two images, we can estimate the relative camera pose between them by establishing image-to-image correspondences. Usually, correspondences are 2D-to-2D and the pose we estimate is defined only up to scale. Some applications, aiming at instant augmented reality anywhere, require scale-metric pose estimates, and hence, they rely on external depth estimators to recover the scale. We present MicKey, a keypoint matching pipeline that is able to predict metric correspondences in 3D camera space. By learning to match 3D coordinates across images, we are able to infer the metric relative pose without depth measurements. Depth measurements are also not required for training, nor are scene reconstructions or image overlap information. MicKey is supervised only by pairs of images and their relative poses. MicKey achieves state-of-the-art performance on the Map-Free Relocalisation benchmark while requiring less supervision than competing approaches.</p><h4 id="open-source-ai-based-se-tools-opportunities-and-challenges-of-collaborative-software-learning" tabindex="-1"><a class="header-anchor" href="#open-source-ai-based-se-tools-opportunities-and-challenges-of-collaborative-software-learning"><span>Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning</span></a></h4><p><strong>Authors</strong>: Zhihao Lin, Wei Ma, Tao Lin, Yaowen Zheng, Jingquan Ge, Jun Wang, Jacques Klein, Tegawende Bissyande, Yang Liu, Li Li</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06201v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06201v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. Like traditional SE tools, open-source collaboration is key in realising the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data. However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community. Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organisations. Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance.</p><h4 id="streamlined-transmission-a-semantic-aware-xr-deployment-framework-enhanced-by-generative-ai" tabindex="-1"><a class="header-anchor" href="#streamlined-transmission-a-semantic-aware-xr-deployment-framework-enhanced-by-generative-ai"><span>Streamlined Transmission: A Semantic-Aware XR Deployment Framework Enhanced by Generative AI</span></a></h4><p><strong>Authors</strong>: Wanting Yang, Zehui Xiong, Tony Q. S. Quek, Xuemin Shen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06182v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06182v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In the era of 6G, featuring compelling visions of digital twins and metaverses, Extended Reality (XR) has emerged as a vital conduit connecting the digital and physical realms, garnering widespread interest. Ensuring a fully immersive wireless XR experience stands as a paramount technical necessity, demanding the liberation of XR from the confines of wired connections. In this paper, we first introduce the technologies applied in the wireless XR domain, delve into their benefits and limitations, and highlight the ongoing challenges. We then propose a novel deployment framework for a broad XR pipeline, termed &quot;GeSa-XRF&quot;, inspired by the core philosophy of Semantic Communication (SemCom) which shifts the concern from &quot;how&quot; to transmit to &quot;what&quot; to transmit. Particularly, the framework comprises three stages: data collection, data analysis, and data delivery. In each stage, we integrate semantic awareness to achieve streamlined transmission and employ Generative Artificial Intelligence (GAI) to achieve collaborative refinements. For the data collection of multi-modal data with differentiated data volumes and heterogeneous latency requirements, we propose a novel SemCom paradigm based on multi-modal fusion and separation and a GAI-based robust superposition scheme. To perform a comprehensive data analysis, we employ multi-task learning to perform the prediction of field of view and personalized attention and discuss the possible preprocessing approaches assisted by GAI. Lastly, for the data delivery stage, we present a semantic-aware multicast-based delivery strategy aimed at reducing pixel level redundant transmissions and introduce the GAI collaborative refinement approach. The performance gain of the proposed GeSa-XRF is preliminarily demonstrated through a case study.</p><h4 id="eve-enabling-anyone-to-train-robot-using-augmented-reality" tabindex="-1"><a class="header-anchor" href="#eve-enabling-anyone-to-train-robot-using-augmented-reality"><span>EVE: Enabling Anyone to Train Robot using Augmented Reality</span></a></h4><p><strong>Authors</strong>: Jun Wang, Chun-Cheng Chang, Jiafei Duan, Dieter Fox, Ranjay Krishna</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06089v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06089v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators. Consequently, only those with access to physical robots produce demonstrations to train robots. To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$). We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.</p><h2 id="_2024-04-08" tabindex="-1"><a class="header-anchor" href="#_2024-04-08"><span>2024-04-08</span></a></h2><h4 id="the-hallucinations-leaderboard-an-open-effort-to-measure-hallucinations-in-large-language-models" tabindex="-1"><a class="header-anchor" href="#the-hallucinations-leaderboard-an-open-effort-to-measure-hallucinations-in-large-language-models"><span>The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models</span></a></h4><p><strong>Authors</strong>: Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu Zhao, Laura Perez-Beltrachini, Max Ryabinin, Xuanli He, Clémentine Fourrier, Pasquale Minervini</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05904v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05904v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Large Language Models (LLMs) have transformed the Natural Language Processing (NLP) landscape with their remarkable ability to understand and generate human-like text. However, these models are prone to ``hallucinations&#39;&#39; -- outputs that do not align with factual reality or the input context. This paper introduces the Hallucinations Leaderboard, an open initiative to quantitatively measure and compare the tendency of each model to produce hallucinations. The leaderboard uses a comprehensive set of benchmarks focusing on different aspects of hallucinations, such as factuality and faithfulness, across various tasks, including question-answering, summarisation, and reading comprehension. Our analysis provides insights into the performance of different models, guiding researchers and practitioners in choosing the most reliable models for their applications.</p><h4 id="a-realistic-surgical-simulator-for-non-rigid-and-contact-rich-manipulation-in-surgeries-with-the-da-vinci-research-kit" tabindex="-1"><a class="header-anchor" href="#a-realistic-surgical-simulator-for-non-rigid-and-contact-rich-manipulation-in-surgeries-with-the-da-vinci-research-kit"><span>A Realistic Surgical Simulator for Non-Rigid and Contact-Rich Manipulation in Surgeries with the da Vinci Research Kit</span></a></h4><p><strong>Authors</strong>: Yafei Ou, Sadra Zargarzadeh, Paniz Sedighi, Mahdi Tavakoli</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05888v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05888v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Realistic real-time surgical simulators play an increasingly important role in surgical robotics research, such as surgical robot learning and automation, and surgical skills assessment. Although there are a number of existing surgical simulators for research, they generally lack the ability to simulate the diverse types of objects and contact-rich manipulation tasks typically present in surgeries, such as tissue cutting and blood suction. In this work, we introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the da Vinci Research Kit (dVRK) that enables simulating various contact-rich surgical tasks involving different surgical instruments, soft tissue, and body fluids. The real-world dVRK console and the master tool manipulator (MTM) robots are incorporated into the system to allow for teleoperation through virtual reality (VR). To showcase the advantages and potentials of the simulator, we present three examples of surgical tasks, including tissue grasping and deformation, blood suction, and tissue cutting. These tasks are performed using the simulated surgical instruments, including the large needle driver, suction irrigator, and curved scissor, through VR-based teleoperation.</p><h4 id="on-the-fly-robotic-assisted-medical-instrument-planning-and-execution-using-mixed-reality" tabindex="-1"><a class="header-anchor" href="#on-the-fly-robotic-assisted-medical-instrument-planning-and-execution-using-mixed-reality"><span>On the Fly Robotic-Assisted Medical Instrument Planning and Execution Using Mixed Reality</span></a></h4><p><strong>Authors</strong>: Letian Ai, Yihao Liu, Mehran Armand, Amir Kheradmand, Alejandro Martin-Gomez</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05887v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05887v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Robotic-assisted medical systems (RAMS) have gained significant attention for their advantages in alleviating surgeons&#39; fatigue and improving patients&#39; outcomes. These systems comprise a range of human-computer interactions, including medical scene monitoring, anatomical target planning, and robot manipulation. However, despite its versatility and effectiveness, RAMS demands expertise in robotics, leading to a high learning cost for the operator. In this work, we introduce a novel framework using mixed reality technologies to ease the use of RAMS. The proposed framework achieves real-time planning and execution of medical instruments by providing 3D anatomical image overlay, human-robot collision detection, and robot programming interface. These features, integrated with an easy-to-use calibration method for head-mounted display, improve the effectiveness of human-robot interactions. To assess the feasibility of the framework, two medical applications are presented in this work: 1) coil placement during transcranial magnetic stimulation and 2) drill and injector device positioning during femoroplasty. Results from these use cases demonstrate its potential to extend to a wider range of medical scenarios.</p><h4 id="two-person-interaction-augmentation-with-skeleton-priors" tabindex="-1"><a class="header-anchor" href="#two-person-interaction-augmentation-with-skeleton-priors"><span>Two-Person Interaction Augmentation with Skeleton Priors</span></a></h4><p><strong>Authors</strong>: Baiyi Li, Edmond S. L. Ho, Hubert P. H. Shum, He Wang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05490v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05490v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies. Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes. Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions.</p><h4 id="webxr-a-frame-and-networked-aframe-as-a-basis-for-an-open-metaverse-a-conceptual-architecture" tabindex="-1"><a class="header-anchor" href="#webxr-a-frame-and-networked-aframe-as-a-basis-for-an-open-metaverse-a-conceptual-architecture"><span>WebXR, A-Frame and Networked-Aframe as a Basis for an Open Metaverse: A Conceptual Architecture</span></a></h4><p><strong>Authors</strong>: Giuseppe Macario</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05317v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05317v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This work proposes a WebXR-based cross-platform conceptual architecture, leveraging the A-Frame and Networked-Aframe frameworks, in order to facilitate the development of an open, accessible, and interoperable metaverse. By introducing the concept of spatial web app, this research contributes to the discourse on the metaverse, offering an architecture that democratizes access to virtual environments and extended reality through the web, and aligns with Tim Berners-Lee&#39;s original vision of the World Wide Web as an open platform in the digital realm.</p><h4 id="can-edge-computing-fulfill-the-requirements-of-automated-vehicular-services-using-5g-network" tabindex="-1"><a class="header-anchor" href="#can-edge-computing-fulfill-the-requirements-of-automated-vehicular-services-using-5g-network"><span>Can Edge Computing fulfill the requirements of automated vehicular services using 5G network ?</span></a></h4><p><strong>Authors</strong>: Wendlasida Ouedraogo, Andrea Araldo, Badii Jouaber, Hind Castel, Remy Grunblatt</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05296v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05296v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Communication and computation services supporting Connected and Automated Vehicles (CAVs) are characterized by stringent requirements, in terms of response time and reliability. Fulfilling these requirements is crucial for ensuring road safety and traffic optimization. The conceptually simple solution of hosting these services in the vehicles increases their cost (mainly due to the installation and maintenance of computation infrastructure) and may drain their battery excessively. Such disadvantages can be tackled via Multi-Access Edge Computing (MEC), consisting in deploying computation capability in network nodes deployed close to the devices (vehicles in this case), such as to satisfy the stringent CAV requirements. However, it is not yet clear under which conditions MEC can support CAV requirements and for which services. To shed light on this question, we conduct a simulation campaign using well-known open-source simulation tools, namely OMNeT++, Simu5G, Veins, INET, and SUMO. We are thus able to provide a reality check on MEC for CAV, pinpointing what are the computation capacities that must be installed in the MEC, to support the different services, and the amount of vehicles that a single MEC node can support. We find that such parameters must vary a lot, depending on the service considered. This study can serve as a preliminary basis for network operators to plan future deployment of MEC to support CAV.</p><h2 id="_2024-04-07" tabindex="-1"><a class="header-anchor" href="#_2024-04-07"><span>2024-04-07</span></a></h2><h4 id="reduction-of-forgetting-by-contextual-variation-during-encoding-using-360-degree-video-based-immersive-virtual-environments" tabindex="-1"><a class="header-anchor" href="#reduction-of-forgetting-by-contextual-variation-during-encoding-using-360-degree-video-based-immersive-virtual-environments"><span>Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments</span></a></h4><p><strong>Authors</strong>: Takato Mizuho, Takuji Narumi, Hideaki Kuzuoka</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05007v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05007v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recall impairment in a different environmental context from learning is called context-dependent forgetting. Two learning methods have been proposed to prevent context-dependent forgetting: reinstatement and decontextualization. Reinstatement matches the environmental context between learning and retrieval, whereas decontextualization involves repeated learning in various environmental contexts and eliminates the context dependency of memory. Conventionally, these methods have been validated by switching between physical rooms. However, in this study, we use immersive virtual environments (IVEs) as the environmental context assisted by virtual reality (VR), which is known for its low cost and high reproducibility compared to traditional manipulation. Whereas most existing studies using VR have failed to reveal the reinstatement effect, we test its occurrence using a 360-degree video-based IVE with improved familiarity and realism instead of a computer graphics-based IVE. Furthermore, we are the first to address decontextualization using VR. Our experiment showed that repeated learning in the same constant IVE as retrieval did not significantly reduce forgetting compared to repeated learning in different constant IVEs. Conversely, repeated learning in various IVEs significantly reduced forgetting than repeated learning in constant IVEs. These findings contribute to the design of IVEs for VR-based applications, particularly in educational settings.</p><h2 id="_2024-04-06" tabindex="-1"><a class="header-anchor" href="#_2024-04-06"><span>2024-04-06</span></a></h2><h4 id="teleaware-robot-designing-awareness-augmented-telepresence-robot-for-remote-collaborative-locomotion" tabindex="-1"><a class="header-anchor" href="#teleaware-robot-designing-awareness-augmented-telepresence-robot-for-remote-collaborative-locomotion"><span>TeleAware Robot: Designing Awareness-augmented Telepresence Robot for Remote Collaborative Locomotion</span></a></h4><p><strong>Authors</strong>: Ruyi Li, Yaxin Zhu, Min Liu, Yihang Zeng, Shanning Zhuang, Jiayi Fu, Yi Lu, Guyue Zhou, Can Liu, Jiangtao Gong</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.04579v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.04579v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Telepresence robots can be used to support users to navigate an environment remotely and share the visiting experience with their social partners. Although such systems allow users to see and hear the remote environment and communicate with their partners via live video feed, this does not provide enough awareness of the environment and their remote partner&#39;s activities. In this paper, we introduce an awareness framework for collaborative locomotion in scenarios of onsite and remote users visiting a place together. From an observational study of small groups of people visiting exhibitions, we derived four design goals for enhancing the environmental and social awareness between social partners, and developed a set of awareness-enhancing techniques to add to a standard telepresence robot - named TeleAware robot. Through a controlled experiment simulating a guided exhibition visiting task, TeleAware robot showed the ability to lower the workload, facilitate closer social proximity, and improve mutual awareness and social presence compared with the standard one. We discuss the impact of mobility and roles of local and remote users, and provide insights for the future design of awareness-enhancing telepresence robot systems that facilitate collaborative locomotion.</p><h2 id="_2024-04-05" tabindex="-1"><a class="header-anchor" href="#_2024-04-05"><span>2024-04-05</span></a></h2><h4 id="effects-of-multisensory-feedback-on-the-perception-and-performance-of-virtual-reality-hand-retargeted-interaction" tabindex="-1"><a class="header-anchor" href="#effects-of-multisensory-feedback-on-the-perception-and-performance-of-virtual-reality-hand-retargeted-interaction"><span>Effects of Multisensory Feedback on the Perception and Performance of Virtual Reality Hand-Retargeted Interaction</span></a></h4><p><strong>Authors</strong>: Hyunyoung Jang, Jinwook Kim, Jeongmi Lee</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.03899v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.03899v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Retargeting methods that modify the visual representation of real movements have been widely used to expand the interaction space and create engaging virtual reality experiences. For optimal user experience and performance, it is essential to specify the perception of retargeting and utilize the appropriate range of modification parameters. However, previous studies mostly concentrated on whether users perceived the target sense or not and rarely examined the perceptual accuracy and sensitivity to retargeting. Moreover, it is unknown how the perception and performance in hand-retargeted interactions are influenced by multisensory feedback. In this study, we used rigorous psychophysical methods to specify users&#39; perceptual accuracy and sensitivity to hand-retargeting and provide acceptable ranges of retargeting parameters. We also presented different multisensory feedback simultaneously with the retargeting to probe its effect on users&#39; perception and task performance. The experimental results showed that providing continuous multisensory feedback, proportionate to the distance between the virtual hand and the targeted destination, heightened the accuracy of users&#39; perception of hand retargeting without altering their perceptual sensitivity. Furthermore, the utilization of multisensory feedback considerably improved the precision of task performance, particularly at lower gain factors. Based on these findings, we propose design guidelines and potential applications of VR hand-retargeted interactions and multisensory feedback for optimal user experience and performance.</p><h2 id="_2024-04-04" tabindex="-1"><a class="header-anchor" href="#_2024-04-04"><span>2024-04-04</span></a></h2><h4 id="i-did-not-notice-a-comparison-of-immersive-analytics-with-augmented-and-virtual-reality" tabindex="-1"><a class="header-anchor" href="#i-did-not-notice-a-comparison-of-immersive-analytics-with-augmented-and-virtual-reality"><span>I Did Not Notice: A Comparison of Immersive Analytics with Augmented and Virtual Reality</span></a></h4><p><strong>Authors</strong>: Xiaoyan Zhou, Anil Ufuk Batmaz, Adam S. Williams, Dylan Schreiber, Francisco Ortega</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.03814v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.03814v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Immersive environments enable users to engage in embodied interaction, enhancing the sensemaking processes involved in completing tasks such as immersive analytics. Previous comparative studies on immersive analytics using augmented and virtual realities have revealed that users employ different strategies for data interpretation and text-based analytics depending on the environment. Our study seeks to investigate how augmented and virtual reality influences sensemaking processes in quantitative immersive analytics. Our results, derived from a diverse group of participants, indicate that users demonstrate comparable performance in both environments. However, it was observed that users exhibit a higher tolerance for cognitive load in VR and travel further in AR. Based on our findings, we recommend providing users with the option to switch between AR and VR, thereby enabling them to select an environment that aligns with their preferences and task requirements.</p><h4 id="integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work" tabindex="-1"><a class="header-anchor" href="#integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work"><span>Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work</span></a></h4><p><strong>Authors</strong>: Somin Park, Carol C. Menassa, Vineet R. Kamat</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.03498v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.03498v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement. This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants. This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots. By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting. Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs. The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry.</p><h4 id="influence-of-gameplay-duration-hand-tracking-and-controller-based-control-methods-on-ux-in-vr" tabindex="-1"><a class="header-anchor" href="#influence-of-gameplay-duration-hand-tracking-and-controller-based-control-methods-on-ux-in-vr"><span>Influence of Gameplay Duration, Hand Tracking, and Controller Based Control Methods on UX in VR</span></a></h4><p><strong>Authors</strong>: Tanja Kojić, Maurizio Vergari, Simon Knuth, Maximilian Warsinke, Sebastian Möller, Jan-Niklas Voigt-Antons</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.03337v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.03337v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Inside-out tracking is growing popular in consumer VR, enhancing accessibility. It uses HMD camera data and neural networks for effective hand tracking. However, limited user experience studies have compared this method to traditional controllers, with no consensus on the optimal control technique. This paper investigates the impact of control methods and gaming duration on VR user experience, hypothesizing hand tracking might be preferred for short sessions and by users new to VR due to its simplicity. Through a lab study with twenty participants, evaluating presence, emotional response, UX quality, and flow, findings revealed control type and session length affect user experience without significant interaction. Controllers were generally superior, attributed to their reliability, and longer sessions increased presence and realism. The study found that individuals with more VR experience were more inclined to recommend hand tracking to others, which contradicted predictions.</p><h4 id="fusion-of-mixture-of-experts-and-generative-artificial-intelligence-in-mobile-edge-metaverse" tabindex="-1"><a class="header-anchor" href="#fusion-of-mixture-of-experts-and-generative-artificial-intelligence-in-mobile-edge-metaverse"><span>Fusion of Mixture of Experts and Generative Artificial Intelligence in Mobile Edge Metaverse</span></a></h4><p><strong>Authors</strong>: Guangyuan Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Shiwen Mao, Dong In Kim</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.03321v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.03321v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In the digital transformation era, Metaverse offers a fusion of virtual reality (VR), augmented reality (AR), and web technologies to create immersive digital experiences. However, the evolution of the Metaverse is slowed down by the challenges of content creation, scalability, and dynamic user interaction. Our study investigates an integration of Mixture of Experts (MoE) models with Generative Artificial Intelligence (GAI) for mobile edge computing to revolutionize content creation and interaction in the Metaverse. Specifically, we harness an MoE model&#39;s ability to efficiently manage complex data and complex tasks by dynamically selecting the most relevant experts running various sub-models to enhance the capabilities of GAI. We then present a novel framework that improves video content generation quality and consistency, and demonstrate its application through case studies. Our findings underscore the efficacy of MoE and GAI integration to redefine virtual experiences by offering a scalable, efficient pathway to harvest the Metaverse&#39;s full potential.</p><h4 id="exploring-emotions-in-multi-componential-space-using-interactive-vr-games" tabindex="-1"><a class="header-anchor" href="#exploring-emotions-in-multi-componential-space-using-interactive-vr-games"><span>Exploring Emotions in Multi-componential Space using Interactive VR Games</span></a></h4><p><strong>Authors</strong>: Rukshani Somarathna, Gelareh Mohammadi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.03239v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.03239v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Emotion understanding is a complex process that involves multiple components. The ability to recognise emotions not only leads to new context awareness methods but also enhances system interaction&#39;s effectiveness by perceiving and expressing emotions. Despite the attention to discrete and dimensional models, neuroscientific evidence supports those emotions as being complex and multi-faceted. One framework that resonated well with such findings is the Component Process Model (CPM), a theory that considers the complexity of emotions with five interconnected components: appraisal, expression, motivation, physiology and feeling. However, the relationship between CPM and discrete emotions has not yet been fully explored. Therefore, to better understand emotions underlying processes, we operationalised a data-driven approach using interactive Virtual Reality (VR) games and collected multimodal measures (self-reports, physiological and facial signals) from 39 participants. We used Machine Learning (ML) methods to identify the unique contributions of each component to emotion differentiation. Our results showed the role of different components in emotion differentiation, with the model including all components demonstrating the most significant contribution. Moreover, we found that at least five dimensions are needed to represent the variation of emotions in our dataset. These findings also have implications for using VR environments in emotion research and highlight the role of physiological signals in emotion recognition within such environments.</p><h2 id="_2024-04-03" tabindex="-1"><a class="header-anchor" href="#_2024-04-03"><span>2024-04-03</span></a></h2><h4 id="self-supervised-6-dof-robot-grasping-by-demonstration-via-augmented-reality-teleoperation-system" tabindex="-1"><a class="header-anchor" href="#self-supervised-6-dof-robot-grasping-by-demonstration-via-augmented-reality-teleoperation-system"><span>Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System</span></a></h4><p><strong>Authors</strong>: Xiwen Dengxiong, Xueting Wang, Shi Bai, Yunbo Zhang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.03067v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.03067v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area. To this end, we propose a self-supervised 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations. Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration. For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations.</p><h4 id="ai-augmented-automation-for-real-driving-prediction-an-industrial-use-case" tabindex="-1"><a class="header-anchor" href="#ai-augmented-automation-for-real-driving-prediction-an-industrial-use-case"><span>AI-augmented Automation for Real Driving Prediction: an Industrial Use Case</span></a></h4><p><strong>Authors</strong>: Romina Eramo, Hamzeh Eyal Salman, Matteo Spezialetti, Darko Stern, Pierre Quinton, Antonio Cicchetti</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.02841v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.02841v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The risen complexity of automotive systems requires new development strategies and methods to master the upcoming challenges. Traditional methods need thus to be changed by an increased level of automation, and a faster continuous improvement cycle. In this context, current vehicle performance tests represent a very time-consuming and expensive task due to the need to perform the tests in real driving conditions. As a consequence, agile/iterative processes like DevOps are largely hindered by the necessity of triggering frequent tests. This paper reports on a practical experience of developing an AI-augmented solution based on Machine Learning and Model-based Engineering to support continuous vehicle development and testing. In particular, historical data collected in real driving conditions is leveraged to synthesize a high-fidelity driving simulator and hence enable performance tests in virtual environments. Based on this practical experience, this paper also proposes a conceptual framework to support predictions based on real driving behavior.</p><h2 id="_2024-04-02" tabindex="-1"><a class="header-anchor" href="#_2024-04-02"><span>2024-04-02</span></a></h2><h4 id="a-change-of-scenery-transformative-insights-from-retrospective-vr-embodied-perspective-taking-of-conflict-with-a-close-other" tabindex="-1"><a class="header-anchor" href="#a-change-of-scenery-transformative-insights-from-retrospective-vr-embodied-perspective-taking-of-conflict-with-a-close-other"><span>A Change of Scenery: Transformative Insights from Retrospective VR Embodied Perspective-Taking of Conflict With a Close Other</span></a></h4><p><strong>Authors</strong>: Seraphina Yong, Leo Cui, Evan Suma Rosenberg, Svetlana Yarosh</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.02277v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.02277v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Close relationships are irreplaceable social resources, yet prone to high-risk conflict. Building on findings from the fields of HCI, virtual reality, and behavioral therapy, we evaluate the unexplored potential of retrospective VR-embodied perspective-taking to fundamentally influence conflict resolution in close others. We develop a biographically-accurate Retrospective Embodied Perspective-Taking system (REPT) and conduct a mixed-methods evaluation of its influence on close others&#39; reflection and communication, compared to video-based reflection methods currently used in therapy (treatment as usual, or TAU). Our key findings provide evidence that REPT was able to significantly improve communication skills and positive sentiment of both partners during conflict, over TAU. The qualitative data also indicated that REPT surpassed basic perspective-taking by exclusively stimulating users to embody and reflect on both their own and their partner&#39;s experiences at the same level. In light of these findings, we provide implications and an agenda for social embodiment in HCI design: conceptualizing the use of `embodied social cognition,&#39; and envisioning socially-embodied experiences as an interactive context.</p><h4 id="causality-based-transfer-of-driving-scenarios-to-unseen-intersections" tabindex="-1"><a class="header-anchor" href="#causality-based-transfer-of-driving-scenarios-to-unseen-intersections"><span>Causality-based Transfer of Driving Scenarios to Unseen Intersections</span></a></h4><p><strong>Authors</strong>: Christoph Glasmacher, Michael Schuldes, Sleiman El Masri, Lutz Eckstein</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.02046v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.02046v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Scenario-based testing of automated driving functions has become a promising method to reduce time and cost compared to real-world testing. In scenario-based testing automated functions are evaluated in a set of pre-defined scenarios. These scenarios provide information about vehicle behaviors, environmental conditions, or road characteristics using parameters. To create realistic scenarios, parameters and parameter dependencies have to be fitted utilizing real-world data. However, due to the large variety of intersections and movement constellations found in reality, data may not be available for certain scenarios. This paper proposes a methodology to systematically analyze relations between parameters of scenarios. Bayesian networks are utilized to analyze causal dependencies in order to decrease the amount of required data and to transfer causal patterns creating unseen scenarios. Thereby, infrastructural influences on movement patterns are investigated to generate realistic scenarios on unobserved intersections. For evaluation, scenarios and underlying parameters are extracted from the inD dataset. Movement patterns are estimated, transferred and checked against recorded data from those initially unseen intersections.</p><h4 id="_3d-scene-generation-from-scene-graphs-and-self-attention" tabindex="-1"><a class="header-anchor" href="#_3d-scene-generation-from-scene-graphs-and-self-attention"><span>3D scene generation from scene graphs and self-attention</span></a></h4><p><strong>Authors</strong>: Pietro Bonazzi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.01887v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.01887v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Synthesizing realistic and diverse indoor 3D scene layouts in a controllable fashion opens up applications in simulated navigation and virtual reality. As concise and robust representations of a scene, scene graphs have proven to be well-suited as the semantic control on the generated layout. We present a variant of the conditional variational autoencoder (cVAE) model to synthesize 3D scenes from scene graphs and floor plans. We exploit the properties of self-attention layers to capture high-level relationships between objects in a scene, and use these as the building blocks of our model. Our model, leverages graph transformers to estimate the size, dimension and orientation of the objects in a room while satisfying relationships in the given scene graph. Our experiments shows self-attention layers leads to sparser (7.9x compared to Graphto3D) and more diverse scenes (16%).</p><h4 id="generative-ai-for-immersive-communication-the-next-frontier-in-internet-of-senses-through-6g" tabindex="-1"><a class="header-anchor" href="#generative-ai-for-immersive-communication-the-next-frontier-in-internet-of-senses-through-6g"><span>Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G</span></a></h4><p><strong>Authors</strong>: Nassim Sehad, Lina Bariah, Wassim Hamidouche, Hamed Hellaoui, Riku Jäntti, Mérouane Debbah</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.01713v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.01713v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that leverages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical applications of generative AI for immersive media while addressing the challenges and outlining future trajectories.</p><h2 id="_2024-04-01" tabindex="-1"><a class="header-anchor" href="#_2024-04-01"><span>2024-04-01</span></a></h2><h4 id="scalable-scene-modeling-from-perspective-imaging-physics-based-appearance-and-geometry-inference" tabindex="-1"><a class="header-anchor" href="#scalable-scene-modeling-from-perspective-imaging-physics-based-appearance-and-geometry-inference"><span>Scalable Scene Modeling from Perspective Imaging: Physics-based Appearance and Geometry Inference</span></a></h4><p><strong>Authors</strong>: Shuang Song</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.01248v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.01248v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: 3D scene modeling techniques serve as the bedrocks in the geospatial engineering and computer science, which drives many applications ranging from automated driving, terrain mapping, navigation, virtual, augmented, mixed, and extended reality (for gaming and movie industry etc.). This dissertation presents a fraction of contributions that advances 3D scene modeling to its state of the art, in the aspects of both appearance and geometry modeling. In contrast to the prevailing deep learning methods, as a core contribution, this thesis aims to develop algorithms that follow first principles, where sophisticated physic-based models are introduced alongside with simpler learning and inference tasks. The outcomes of these algorithms yield processes that can consume much larger volume of data for highly accurate reconstructing 3D scenes at a scale without losing methodological generality, which are not possible by contemporary complex-model based deep learning methods. Specifically, the dissertation introduces three novel methodologies that address the challenges of inferring appearance and geometry through physics-based modeling. Overall, the research encapsulated in this dissertation marks a series of methodological triumphs in the processing of complex datasets. By navigating the confluence of deep learning, computational geometry, and photogrammetry, this work lays down a robust framework for future exploration and practical application in the rapidly evolving field of 3D scene reconstruction. The outcomes of these studies are evidenced through rigorous experiments and comparisons with existing state-of-the-art methods, demonstrating the efficacy and scalability of the proposed approaches.</p><h4 id="detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms" tabindex="-1"><a class="header-anchor" href="#detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms"><span>Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs</span></a></h4><p><strong>Authors</strong>: Jialou Wang, Manli Zhu, Yulei Li, Honglei Li, Longzhi Yang, Wai Lok Woo</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.01151v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.01151v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Localization plays a crucial role in enhancing the practicality and precision of VQA systems. By enabling fine-grained identification and interaction with specific parts of an object, it significantly improves the system&#39;s ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality. However, traditional systems face challenges in accurately mapping objects within images to generate nuanced and spatially aware responses. In this work, we introduce &quot;Detect2Interact&quot;, which addresses these challenges by introducing an advanced approach for fine-grained object visual key field detection. First, we use the segment anything model (SAM) to generate detailed spatial maps of objects in images. Next, we use Vision Studio to extract semantic object descriptions. Third, we employ GPT-4&#39;s common sense knowledge, bridging the gap between an object&#39;s semantics and its spatial map. As a result, Detect2Interact achieves consistent qualitative results on object key field detection across extensive test cases and outperforms the existing VQA system with object detection by providing a more reasonable and finer visual representation.</p><h4 id="aigcoiqa2024-perceptual-quality-assessment-of-ai-generated-omnidirectional-images" tabindex="-1"><a class="header-anchor" href="#aigcoiqa2024-perceptual-quality-assessment-of-ai-generated-omnidirectional-images"><span>AIGCOIQA2024: Perceptual Quality Assessment of AI Generated Omnidirectional Images</span></a></h4><p><strong>Authors</strong>: Liu Yang, Huiyu Duan, Long Teng, Yucheng Zhu, Xiaohong Liu, Menghan Hu, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.01024v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.01024v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In recent years, the rapid advancement of Artificial Intelligence Generated Content (AIGC) has attracted widespread attention. Among the AIGC, AI generated omnidirectional images hold significant potential for Virtual Reality (VR) and Augmented Reality (AR) applications, hence omnidirectional AIGC techniques have also been widely studied. AI-generated omnidirectional images exhibit unique distortions compared to natural omnidirectional images, however, there is no dedicated Image Quality Assessment (IQA) criteria for assessing them. This study addresses this gap by establishing a large-scale AI generated omnidirectional image IQA database named AIGCOIQA2024 and constructing a comprehensive benchmark. We first generate 300 omnidirectional images based on 5 AIGC models utilizing 25 text prompts. A subjective IQA experiment is conducted subsequently to assess human visual preferences from three perspectives including quality, comfortability, and correspondence. Finally, we conduct a benchmark experiment to evaluate the performance of state-of-the-art IQA models on our database. The database will be released to facilitate future research.</p><h4 id="badpart-unified-black-box-adversarial-patch-attacks-against-pixel-wise-regression-tasks" tabindex="-1"><a class="header-anchor" href="#badpart-unified-black-box-adversarial-patch-attacks-against-pixel-wise-regression-tasks"><span>BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks</span></a></h4><p><strong>Authors</strong>: Zhiyuan Cheng, Zhaoyi Liu, Tengda Guo, Shiwei Feng, Dongfang Liu, Mingjie Tang, Xiangyu Zhang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.00924v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.00924v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Pixel-wise regression tasks (e.g., monocular depth estimation (MDE) and optical flow estimation (OFE)) have been widely involved in our daily life in applications like autonomous driving, augmented reality and video composition. Although certain applications are security-critical or bear societal significance, the adversarial robustness of such models are not sufficiently studied, especially in the black-box scenario. In this work, we introduce the first unified black-box adversarial patch attack framework against pixel-wise regression tasks, aiming to identify the vulnerabilities of these models under query-based black-box attacks. We propose a novel square-based adversarial patch optimization framework and employ probabilistic square sampling and score-based gradient estimation techniques to generate the patch effectively and efficiently, overcoming the scalability problem of previous black-box patch attacks. Our attack prototype, named BadPart, is evaluated on both MDE and OFE tasks, utilizing a total of 7 models. BadPart surpasses 3 baseline methods in terms of both attack performance and efficiency. We also apply BadPart on the Google online service for portrait depth estimation, causing 43.5% relative distance error with 50K queries. State-of-the-art (SOTA) countermeasures cannot defend our attack effectively.</p><h2 id="_2024-03-30" tabindex="-1"><a class="header-anchor" href="#_2024-03-30"><span>2024-03-30</span></a></h2><h4 id="enhancing-empathy-in-virtual-reality-an-embodied-approach-to-mindset-modulation" tabindex="-1"><a class="header-anchor" href="#enhancing-empathy-in-virtual-reality-an-embodied-approach-to-mindset-modulation"><span>Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset Modulation</span></a></h4><p><strong>Authors</strong>: Seoyeon Bae, Yoon Kyung Lee, Jungcheol Lee, Jaeheon Kim, Haeseong Jeon, Seung-Hwan Lim, Byung-Cheol Kim, Sowon Hahn</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.00300v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.00300v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: A growth mindset has shown promising outcomes for increasing empathy ability. However, stimulating a growth mindset in VR-based empathy interventions is under-explored. In the present study, we implemented prosocial VR content, Our Neighbor Hero, focusing on embodying a virtual character to modulate players&#39; mindsets. The virtual body served as a stepping stone, enabling players to identify with the character and cultivate a growth mindset as they followed mission instructions. We considered several implementation factors to assist players in positioning within the VR experience, including positive feedback, content difficulty, background lighting, and multimodal feedback. We conducted an experiment to investigate the intervention&#39;s effectiveness in increasing empathy. Our findings revealed that the VR content and mindset training encouraged participants to improve their growth mindsets and empathic motives. This VR content was developed for college students to enhance their empathy and teamwork skills. It has the potential to improve collaboration in organizational and community environments.</p><h2 id="_2024-03-28" tabindex="-1"><a class="header-anchor" href="#_2024-03-28"><span>2024-03-28</span></a></h2><h4 id="using-deep-learning-to-increase-eye-tracking-robustness-accuracy-and-precision-in-virtual-reality" tabindex="-1"><a class="header-anchor" href="#using-deep-learning-to-increase-eye-tracking-robustness-accuracy-and-precision-in-virtual-reality"><span>Using Deep Learning to Increase Eye-Tracking Robustness, Accuracy, and Precision in Virtual Reality</span></a></h4><p><strong>Authors</strong>: Kevin Barkevich, Reynold Bailey, Gabriel J. Diaz</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2403.19768v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2403.19768v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Algorithms for the estimation of gaze direction from mobile and video-based eye trackers typically involve tracking a feature of the eye that moves through the eye camera image in a way that covaries with the shifting gaze direction, such as the center or boundaries of the pupil. Tracking these features using traditional computer vision techniques can be difficult due to partial occlusion and environmental reflections. Although recent efforts to use machine learning (ML) for pupil tracking have demonstrated superior results when evaluated using standard measures of segmentation performance, little is known of how these networks may affect the quality of the final gaze estimate. This work provides an objective assessment of the impact of several contemporary ML-based methods for eye feature tracking when the subsequent gaze estimate is produced using either feature-based or model-based methods. Metrics include the accuracy and precision of the gaze estimate, as well as drop-out rate.</p><h4 id="mrnab-mixed-reality-based-robot-navigation-interface-using-optical-see-through-mr-beacon" tabindex="-1"><a class="header-anchor" href="#mrnab-mixed-reality-based-robot-navigation-interface-using-optical-see-through-mr-beacon"><span>MRNaB: Mixed Reality-based Robot Navigation Interface using Optical-see-through MR-beacon</span></a></h4><p><strong>Authors</strong>: Eduardo Iglesius, Masato Kobayashi, Yuki Uranishi, Haruo Takemura</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2403.19310v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2403.19310v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recent advancements in robotics have led to the development of numerous interfaces to enhance the intuitiveness of robot navigation. However, the reliance on traditional 2D displays imposes limitations on the simultaneous visualization of information. Mixed Reality (MR) technology addresses this issue by enhancing the dimensionality of information visualization, allowing users to perceive multiple pieces of information concurrently. This paper proposes Mixed reality-based robot navigation interface using an optical-see-through MR-beacon (MRNaB), a novel approach that incorporates an MR-beacon, situated atop the real-world environment, to function as a signal transmitter for robot navigation. This MR-beacon is designed to be persistent, eliminating the need for repeated navigation inputs for the same location. Our system is mainly constructed into four primary functions: &quot;Add&quot;, &quot;Move&quot;, &quot;Delete&quot;, and &quot;Select&quot;. These allow for the addition of a MR-beacon, location movement, its deletion, and the selection of MR-beacon for navigation purposes, respectively. The effectiveness of the proposed method was then validated through experiments by comparing it with the traditional 2D system. As the result, MRNaB was proven to increase the performance of the user when doing navigation to a certain place subjectively and objectively. For additional material, please check: https://mertcookimg.github.io/mrnab</p><h4 id="factoid-factual-entailment-for-hallucination-detection" tabindex="-1"><a class="header-anchor" href="#factoid-factual-entailment-for-hallucination-detection"><span>FACTOID: FACtual enTailment fOr hallucInation Detection</span></a></h4><p><strong>Authors</strong>: Vipula Rawte, S. M Towhidul Islam Tonmoy, Krishnav Rajbangshi, Shravani Nag, Aman Chadha, Amit P. Sheth, Amitava Das</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2403.19113v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2403.19113v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits. However, hallucination is a significant concern. In response, Retrieval Augmented Generation (RAG) has emerged as a highly promising paradigm to improve LLM outputs by grounding them in factual information. RAG relies on textual entailment (TE) or similar methods to check if the text produced by LLMs is supported or contradicted, compared to retrieved documents. This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs. For instance, consider a prompt about the &#39;USA&#39;s stance on the Ukraine war&#39;&#39;. The AI-generated text states, ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...&#39;&#39; However, during the war the U.S. president is Joe Biden which contradicts factual reality. Moreover, current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted. To address this, we introduces a new type of TE called ``Factual Entailment (FE).&#39;&#39;, aims to detect factual inaccuracies in content generated by LLMs while also highlighting the specific text segment that contradicts reality. We present FACTOID (FACTual enTAILment for hallucInation Detection), a benchmark dataset for FE. We propose a multi-task learning (MTL) framework for FE, incorporating state-of-the-art (SoTA) long text embeddings such as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The proposed MTL architecture for FE achieves an avg. 40% improvement in accuracy on the FACTOID benchmark compared to SoTA TE methods. As FE automatically detects hallucinations, we assessed 15 modern LLMs and ranked them using our proposed Auto Hallucination Vulnerability Index (HVI_auto). This index quantifies and offers a comparative scale to evaluate and rank LLMs according to their hallucinations.</p><h2 id="_2024-03-27" tabindex="-1"><a class="header-anchor" href="#_2024-03-27"><span>2024-03-27</span></a></h2><h4 id="the-correlations-of-scene-complexity-workload-presence-and-cybersickness-in-a-task-based-vr-game" tabindex="-1"><a class="header-anchor" href="#the-correlations-of-scene-complexity-workload-presence-and-cybersickness-in-a-task-based-vr-game"><span>The Correlations of Scene Complexity, Workload, Presence, and Cybersickness in a Task-Based VR Game</span></a></h4><p><strong>Authors</strong>: Mohammadamin Sanaei, Stephen B. Gilbert, Nikoo Javadpour, Hila Sabouni, Michael C. Dorneich, Jonathan W. Kelly</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2403.19019v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2403.19019v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This investigation examined the relationships among scene complexity, workload, presence, and cybersickness in virtual reality (VR) environments. Numerous factors can influence the overall VR experience, and existing research on this matter is not yet conclusive, warranting further investigation. In this between-subjects experimental setup, 44 participants engaged in the Pendulum Chair game, with half exposed to a simple scene with lower optic flow and lower familiarity, and the remaining half to a complex scene characterized by higher optic flow and greater familiarity. The study measured the dependent variables workload, presence, and cybersickness and analyzed their correlations. Equivalence testing was also used to compare the simple and complex environments. Results revealed that despite the visible differences between the environments, within the 10% boundaries of the maximum possible value for workload and presence, and 13.6% of the maximum SSQ value, a statistically significant equivalence was observed between the simple and complex scenes. Additionally, a moderate, negative correlation emerged between workload and SSQ scores. The findings suggest two key points: (1) the nature of the task can mitigate the impact of scene complexity factors such as optic flow and familiarity, and (2) the correlation between workload and cybersickness may vary, showing either a positive or negative relationship.</p><h4 id="from-virtual-reality-to-the-emerging-discipline-of-perception-engineering" tabindex="-1"><a class="header-anchor" href="#from-virtual-reality-to-the-emerging-discipline-of-perception-engineering"><span>From Virtual Reality to the Emerging Discipline of Perception Engineering</span></a></h4><p><strong>Authors</strong>: Steven M. LaValle, Evan G. Center, Timo Ojala, Matti Pouke, Nicoletta Prencipe, Basak Sakcak, Markku Suomalainen, Kalle G. Timperi, Vadim K. Weinstein</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2403.18588v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2403.18588v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This paper makes the case that a powerful new discipline, which we term perception engineering, is steadily emerging. It follows from a progression of ideas that involve creating illusions, from historical paintings and film, to video games and virtual reality in modern times. Rather than creating physical artifacts such as bridges, airplanes, or computers, perception engineers create illusory perceptual experiences. The scope is defined over any agent that interacts with the physical world, including both biological organisms (humans, animals) and engineered systems (robots, autonomous systems). The key idea is that an agent, called a producer, alters the environment with the intent to alter the perceptual experience of another agent, called a receiver. Most importantly, the paper introduces a precise mathematical formulation of this process, based on the von Neumann-Morgenstern notion of information, to help scope and define the discipline. It is then applied to the cases of engineered and biological agents with discussion of its implications on existing fields such as virtual reality, robotics, and even social media. Finally, open challenges and opportunities for involvement are identified.</p><h4 id="neighbor-environment-observer-an-intelligent-agent-for-immersive-working-companionship" tabindex="-1"><a class="header-anchor" href="#neighbor-environment-observer-an-intelligent-agent-for-immersive-working-companionship"><span>Neighbor-Environment Observer: An Intelligent Agent for Immersive Working Companionship</span></a></h4><p><strong>Authors</strong>: Zhe Sun, Qixuan Liang, Meng Wang, Zhenliang Zhang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2403.18331v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2403.18331v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Human-computer symbiosis is a crucial direction for the development of artificial intelligence. As intelligent systems become increasingly prevalent in our work and personal lives, it is important to develop strategies to support users across physical and virtual environments. While technological advances in personal digital devices, such as personal computers and virtual reality devices, can provide immersive experiences, they can also disrupt users&#39; awareness of their surroundings and enhance the frustration caused by disturbances. In this paper, we propose a joint observation strategy for artificial agents to support users across virtual and physical environments. We introduce a prototype system, neighbor-environment observer (NEO), that utilizes non-invasive sensors to assist users in dealing with disruptions to their immersive experience. System experiments evaluate NEO from different perspectives and demonstrate the effectiveness of the joint observation strategy. A user study is conducted to evaluate its usability. The results show that NEO could lessen users&#39; workload with the learned user preference. We suggest that the proposed strategy can be applied to various smart home scenarios.</p><h2 id="_2024-03-25" tabindex="-1"><a class="header-anchor" href="#_2024-03-25"><span>2024-03-25</span></a></h2><h4 id="review-ecosystems-to-access-educational-xr-experiences-a-scoping-review" tabindex="-1"><a class="header-anchor" href="#review-ecosystems-to-access-educational-xr-experiences-a-scoping-review"><span>Review Ecosystems to access Educational XR Experiences: a Scoping Review</span></a></h4><p><strong>Authors</strong>: Shaun Bangay, Adam P. A. Cardilini, Sophie McKenzie, Maria Nicholas, Manjeet Singh</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2403.17243v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2403.17243v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Educators, developers, and other stakeholders face challenges when creating, adapting, and utilizing virtual and augmented reality (XR) experiences for teaching curriculum topics. User created reviews of these applications provide important information about their relevance and effectiveness in supporting achievement of educational outcomes. To make these reviews accessible, relevant, and useful, they must be readily available and presented in a format that supports decision-making by educators. This paper identifies best practices for developing a new review ecosystem by analyzing existing approaches to providing reviews of interactive experiences. It focuses on the form and format of these reviews, as well as the mechanisms for sharing information about experiences and identifying which ones are most effective. The paper also examines the incentives that drive review creation and maintenance, ensuring that new experiences receive attention from reviewers and that relevant information is updated when necessary. The strategies and opportunities for developing an educational XR (eduXR) review ecosystem include methods for measuring properties such as quality metrics, engaging a broad range of stakeholders in the review process, and structuring the system as a closed loop managed by feedback and incentive structures to ensure stability and productivity. Computing educators are well-positioned to lead the development of these review ecosystems, which can relate XR experiences to the potential opportunities for teaching and learning that they offer.</p><h2 id="_2024-03-24" tabindex="-1"><a class="header-anchor" href="#_2024-03-24"><span>2024-03-24</span></a></h2><h4 id="designing-upper-body-gesture-interaction-with-and-for-people-with-spinal-muscular-atrophy-in-vr" tabindex="-1"><a class="header-anchor" href="#designing-upper-body-gesture-interaction-with-and-for-people-with-spinal-muscular-atrophy-in-vr"><span>Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR</span></a></h4><p><strong>Authors</strong>: Jingze Tian, Yingna Wang, Keye Yu, Liyi Xu, Junan Xie, Franklin Mingzhe Li, Yafeng Niu, Mingming Fan</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2403.16107v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2403.16107v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recent research proposed gaze-assisted gestures to enhance interaction within virtual reality (VR), providing opportunities for people with motor impairments to experience VR. Compared to people with other motor impairments, those with Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing them with more design space. However, it remains unknown what gaze-assisted upper-body gestures people with SMA would want and be able to perform. We conducted an elicitation study in which 12 VR-experienced people with SMA designed upper-body gestures for 26 VR commands, and collected 312 user-defined gestures. Participants predominantly favored creating gestures with their hands. The type of tasks and participants&#39; abilities influence their choice of body parts for gesture design. Participants tended to enhance their body involvement and preferred gestures that required minimal physical effort, and were aesthetically pleasing. Our research will contribute to creating better gesture-based input methods for people with motor impairments to interact with VR.</p><h2 id="_2024-03-23" tabindex="-1"><a class="header-anchor" href="#_2024-03-23"><span>2024-03-23</span></a></h2><h4 id="perception-and-control-of-surfing-in-virtual-reality-using-a-6-dof-motion-platform" tabindex="-1"><a class="header-anchor" href="#perception-and-control-of-surfing-in-virtual-reality-using-a-6-dof-motion-platform"><span>Perception and Control of Surfing in Virtual Reality using a 6-DoF Motion Platform</span></a></h4><p><strong>Authors</strong>: Premankur Banerjee, Jason Cherin, Jayati Upadhyay, Jason Kutch, Heather Culbertson</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2403.15924v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2403.15924v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The paper presents a system for simulating surfing in Virtual Reality (VR), emphasizing the recreation of aquatic motions and user-initiated propulsive forces using a 6-Degree of Freedom (DoF) motion platform. We present an algorithmic approach to accurately render surfboard kinematics and interactive paddling dynamics, validated through experimental evaluation with (N=17) participants. Results indicate that the system effectively reproduces various acceleration levels, the perception of which is independent of users&#39; body posture. We additionally found that the presence of ocean ripples amplifies the perception of acceleration. This system aims to enhance the realism and interactivity of VR surfing, laying a foundation for future advancements in surf therapy and interactive aquatic VR experiences.</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><!----><!----></div></footer><!----><!--[--><!--[--><div class="my-footer">Copyright © 2024-present OpenDesign Community</div><!--]--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-BHieihQC.js" defer></script>
  </body>
</html>
