<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.9" />
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html,
      body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme')
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia('(prefers-color-scheme: dark)').matches
      if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
        document.documentElement.classList.toggle('dark', true)
      }
    </script>
    <meta property="og:url" content="https://opendesign.world/posts/Generative-Art/paper/Stable%20Diffusion.html"><meta property="og:site_name" content="OpenDesign"><meta property="og:title" content="Stable Diffusion"><meta property="og:description" content="Stable Diffusion 2024-04-15 Equipping Diffusion Models with Differentiable Spatial Entropy for Low-Light Image Enhancement Authors: Wenyi Lian, Wenjing Lian, Ziwei Luo Link: htt..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Stable Diffusion","image":[""],"dateModified":null,"author":[]}</script><script src="/translate.js"></script><script src="/model-viewer.min.js" type="module"></script><meta name="google-site-verification" content="1"><meta name="description" content="OpenDesign - Open Source Makes Design More Transparent. Enhance transparency, collaboration, and innovation in design through our open-source community."><meta name="keywords" content="OpenDesign, open source, design, collaboration, innovation, transparency"><meta name="author" content="OpenDesign Community"><link rel="icon" href="/favicon.ico"><title>Stable Diffusion | OpenDesign</title>
    <link rel="preload" href="/assets/style-Dz7YGj4i.css" as="style"><link rel="stylesheet" href="/assets/style-Dz7YGj4i.css">
    <link rel="modulepreload" href="/assets/app-AeV9POxF.js"><link rel="modulepreload" href="/assets/Stable Diffusion.html-DuEu0MBr.js">
    <link rel="prefetch" href="/assets/index.html-a4L-Fh6i.js" as="script"><link rel="prefetch" href="/assets/contact.html-Bgcj1GXA.js" as="script"><link rel="prefetch" href="/assets/contribute.html-BwJoLEM2.js" as="script"><link rel="prefetch" href="/assets/mission.html-CWlCmBN5.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-DaOYu-h2.js" as="script"><link rel="prefetch" href="/assets/index.html-MtlFv1Ve.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-6D7XDUj7.js" as="script"><link rel="prefetch" href="/assets/index.html-B9QUkkp9.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-CD4WRtFg.js" as="script"><link rel="prefetch" href="/assets/index.html-DbNFUlRR.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-2byFJadj.js" as="script"><link rel="prefetch" href="/assets/index.html-BvSvxfim.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-iS7nLu_9.js" as="script"><link rel="prefetch" href="/assets/index.html-C1Hh9PAG.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-BJQlQo1r.js" as="script"><link rel="prefetch" href="/assets/index.html-CNFm3kbp.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-8551538j.js" as="script"><link rel="prefetch" href="/assets/index.html-h6j6Azjk.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-E9D95vhG.js" as="script"><link rel="prefetch" href="/assets/index.html-5L-BvgyT.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-D1SOAAV_.js" as="script"><link rel="prefetch" href="/assets/index.html-D4YvlEuj.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-Dgoe-c8V.js" as="script"><link rel="prefetch" href="/assets/index.html-6nBCbcX1.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-DgUC0GPy.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-D8RlWQG0.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-DGCFMrxr.js" as="script"><link rel="prefetch" href="/assets/HCI.html-rpDQUsQi.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-DUaId7xT.js" as="script"><link rel="prefetch" href="/assets/Resource.html-DCFuH3aS.js" as="script"><link rel="prefetch" href="/assets/LLM.html-N27zKhTN.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BQxdwZYp.js" as="script"><link rel="prefetch" href="/assets/Resource.html-CcBGLP_K.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BCRbJvq_.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-lp_dv4aG.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-DMFTBcJP.js" as="script"><link rel="prefetch" href="/assets/Resource.html-Dc5alozV.js" as="script"><link rel="prefetch" href="/assets/Resource.html-Dug1mcw6.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-C1BvYPoD.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-Ba6HBTpk.js" as="script"><link rel="prefetch" href="/assets/Resource.html-CEnBbpsQ.js" as="script"><link rel="prefetch" href="/assets/Robot.html-DjwBj2Wa.js" as="script"><link rel="prefetch" href="/assets/Robot Vector.html-CSI0PigB.js" as="script"><link rel="prefetch" href="/assets/Resource.html-DCwhtwxl.js" as="script"><link rel="prefetch" href="/assets/XR.html-BdQYs48W.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BE0pc9Qy.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BngONCMn.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BSMP6SR2.js" as="script"><link rel="prefetch" href="/assets/404.html-Cjx_ObTo.js" as="script"><link rel="prefetch" href="/assets/index.html-rqx2u32z.js" as="script"><link rel="prefetch" href="/assets/index.html-dmtL43Nj.js" as="script"><link rel="prefetch" href="/assets/index.html-DqrvDlMP.js" as="script"><link rel="prefetch" href="/assets/index.html---smyxEj.js" as="script"><link rel="prefetch" href="/assets/index.html-uw35dHuB.js" as="script"><link rel="prefetch" href="/assets/index.html-B0n7tu7l.js" as="script"><link rel="prefetch" href="/assets/index.html-DdhZw8yF.js" as="script"><link rel="prefetch" href="/assets/index.html-BHxk_HeH.js" as="script"><link rel="prefetch" href="/assets/index.html-mAHDnwKY.js" as="script"><link rel="prefetch" href="/assets/index.html-muRDGlDr.js" as="script"><link rel="prefetch" href="/assets/index.html-DjIdSSh7.js" as="script"><link rel="prefetch" href="/assets/index.html-CtyHpdgR.js" as="script"><link rel="prefetch" href="/assets/index.html-CuPLUaOP.js" as="script"><link rel="prefetch" href="/assets/index.html-Cp_m7L69.js" as="script"><link rel="prefetch" href="/assets/index.html-Dzuu4WhB.js" as="script"><link rel="prefetch" href="/assets/index.html-D_8OnlXA.js" as="script"><link rel="prefetch" href="/assets/index.html-CDK-TbW3.js" as="script"><link rel="prefetch" href="/assets/index.html-C_Xws9z8.js" as="script"><link rel="prefetch" href="/assets/index.html-CBmSHWfK.js" as="script"><link rel="prefetch" href="/assets/index.html-CJBbmIIK.js" as="script"><link rel="prefetch" href="/assets/index.html-IEqr83QB.js" as="script"><link rel="prefetch" href="/assets/index.html-Cg_q2RlH.js" as="script"><link rel="prefetch" href="/assets/index.html-BdT2UrhB.js" as="script"><link rel="prefetch" href="/assets/index.html-BWGJp156.js" as="script"><link rel="prefetch" href="/assets/index.html-Cvy1uRLP.js" as="script"><link rel="prefetch" href="/assets/index.html-Bj_ZGZTq.js" as="script"><link rel="prefetch" href="/assets/index.html-Dy5vqWhk.js" as="script"><link rel="prefetch" href="/assets/index.html-bBfbOyfk.js" as="script"><link rel="prefetch" href="/assets/index.html-CSaU1tl2.js" as="script"><link rel="prefetch" href="/assets/index.html-DPrWQAi6.js" as="script"><link rel="prefetch" href="/assets/index.html-BPg6FNW-.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="logo" src="/logo.png" alt="OpenDesign"><span class="site-name can-hide" aria-hidden="true">OpenDesign</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide" aria-label="site navigation"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/knowledge/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/knowledge/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/knowledge/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/knowledge/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/knowledge/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/knowledge/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/knowledge/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/knowledge/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/solutions/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/solutions/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/solutions/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/solutions/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/solutions/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/solutions/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/solutions/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/solutions/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Paper"><span class="title">Paper</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Paper"><span class="title">Paper</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/paper/LLM.html" aria-label="LLM"><!--[--><!--[--><!--]--> LLM <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/paper/Stable%20Diffusion.html" aria-label="Stable Diffusion"><!--[--><!--[--><!--]--> Stable Diffusion <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/paper/HCI.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/paper/Robot.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/paper/XR.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="About"><span class="title">About</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="About"><span class="title">About</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/mission.html" aria-label="Mission"><!--[--><!--[--><!--]--> Mission <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contribute.html" aria-label="How to Contribute"><!--[--><!--[--><!--]--> How to Contribute <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contact.html" aria-label="Contact Us"><!--[--><!--[--><!--]--> Contact Us <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><!--[--><h4 class="navbar-dropdown-subtitle"><span>Contributors</span></h4><ul class="navbar-dropdown-subitem-wrapper"><!--[--><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/AGI/Contributors.html" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Generative-Art/Contributors.html" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HCI/Contributors.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HMI/Contributors.html" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Human-Factor/Contributors.html" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Intelligent-Device/Contributors.html" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Interactive-Installations/Contributors.html" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Robot/Contributors.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Spatial-Interaction/Contributors.html" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/XR/Contributors.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="navbar-item"><a class="external-link" href="https://github.com/OpenDesignGlobal/OpenDesign" rel="noopener noreferrer" target="_blank" aria-label="Github"><!--[--><!--[--><!--]--> Github <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--[--><select id="translateSelectLanguage"><option value="Not Translate" selected="selected">Not Translate</option><option value="Default">Auto Translate</option><option value="english">English</option><option value="chinese_simplified">简体中文</option><option value="chinese_traditional">繁體中文</option><option value="russian">Русский</option><option value="japanese">しろうと</option><option value="korean">한국어</option><option value="deutsch">Deutsch</option><option value="spanish">Español</option><option value="italian">italiano</option><option value="french">Français</option><option value="dutch">nederlands</option><option value="norwegian">Norge</option><option value="filipino">Pilipino</option><option value="lao">ກະຣຸນາ</option><option value="romanian">Română</option><option value="nepali">नेपालीName</option><option value="haitian_creole">Kreyòl ayisyen</option><option value="czech">český</option><option value="swedish">Svenska</option><option value="russian">Русский язык</option><option value="malagasy">Malagasy</option><option value="burmese">ဗာရမ်</option><option value="thai">คนไทย</option><option value="persian">Persian</option><option value="kurdish">Kurdî</option><option value="turkish">Türkçe</option><option value="hindi">हिन्दी</option><option value="bulgarian">български</option><option value="malay">Malay</option><option value="swahili">Kiswahili</option><option value="oriya">ଓଡିଆ</option><option value="irish">Íris</option><option value="gujarati">ગુજરાતી</option><option value="slovak">Slovenská</option><option value="hebrew">היברית</option><option value="hungarian">magyar</option><option value="marathi">मराठीName</option><option value="tamil">தாமில்</option><option value="estonian">eesti keel</option><option value="malayalam">മലമാലം</option><option value="inuktitut">ᐃᓄᒃᑎᑐᑦ</option><option value="arabic">بالعربية</option><option value="slovene">slovenščina</option><option value="bengali">বেঙ্গালী</option><option value="urdu">اوردو</option><option value="azerbaijani">azerbaijani</option><option value="portuguese">português</option><option value="samoan">lifiava</option><option value="afrikaans">afrikaans</option><option value="greek">ελληνικά</option><option value="danish">dansk</option><option value="amharic">amharic</option><option value="albanian">albanian</option><option value="lithuanian">Lietuva</option><option value="vietnamese">Tiếng Việt</option><option value="maltese">Malti</option><option value="finnish">suomi</option><option value="catalan">català</option><option value="croatian">hrvatski</option><option value="bosnian">bosnian</option><option value="polish">Polski</option><option value="latvian">latviešu</option><option value="maori">Maori</option></select><!--]--><!--]--><button class="toggle-color-mode-button" title="toggle color mode"><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items" aria-label="site navigation"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/knowledge/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/knowledge/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/knowledge/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/knowledge/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/knowledge/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/knowledge/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/knowledge/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/knowledge/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/solutions/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/solutions/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/solutions/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/solutions/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/solutions/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/solutions/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/solutions/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/solutions/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Paper"><span class="title">Paper</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Paper"><span class="title">Paper</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/paper/LLM.html" aria-label="LLM"><!--[--><!--[--><!--]--> LLM <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/paper/Stable%20Diffusion.html" aria-label="Stable Diffusion"><!--[--><!--[--><!--]--> Stable Diffusion <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/paper/HCI.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/paper/Robot.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/paper/XR.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="About"><span class="title">About</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="About"><span class="title">About</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/mission.html" aria-label="Mission"><!--[--><!--[--><!--]--> Mission <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contribute.html" aria-label="How to Contribute"><!--[--><!--[--><!--]--> How to Contribute <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contact.html" aria-label="Contact Us"><!--[--><!--[--><!--]--> Contact Us <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><!--[--><h4 class="navbar-dropdown-subtitle"><span>Contributors</span></h4><ul class="navbar-dropdown-subitem-wrapper"><!--[--><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/AGI/Contributors.html" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Generative-Art/Contributors.html" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HCI/Contributors.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HMI/Contributors.html" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Human-Factor/Contributors.html" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Intelligent-Device/Contributors.html" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Interactive-Installations/Contributors.html" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Robot/Contributors.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Spatial-Interaction/Contributors.html" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/XR/Contributors.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="navbar-item"><a class="external-link" href="https://github.com/OpenDesignGlobal/OpenDesign" rel="noopener noreferrer" target="_blank" aria-label="Github"><!--[--><!--[--><!--]--> Github <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading">Stable Diffusion <!----></p><ul style="" class="sidebar-item-children"><!--[--><li><a class="route-link sidebar-item" href="#_2024-04-15" aria-label="2024-04-15"><!--[--><!--[--><!--]--> 2024-04-15 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-14" aria-label="2024-04-14"><!--[--><!--[--><!--]--> 2024-04-14 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-13" aria-label="2024-04-13"><!--[--><!--[--><!--]--> 2024-04-13 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-12" aria-label="2024-04-12"><!--[--><!--[--><!--]--> 2024-04-12 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-11" aria-label="2024-04-11"><!--[--><!--[--><!--]--> 2024-04-11 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-10" aria-label="2024-04-10"><!--[--><!--[--><!--]--> 2024-04-10 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-09" aria-label="2024-04-09"><!--[--><!--[--><!--]--> 2024-04-09 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-08" aria-label="2024-04-08"><!--[--><!--[--><!--]--> 2024-04-08 <!--[--><!--]--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><!--]--><div><h1 id="stable-diffusion" tabindex="-1"><a class="header-anchor" href="#stable-diffusion"><span>Stable Diffusion</span></a></h1><h2 id="_2024-04-15" tabindex="-1"><a class="header-anchor" href="#_2024-04-15"><span>2024-04-15</span></a></h2><h4 id="equipping-diffusion-models-with-differentiable-spatial-entropy-for-low-light-image-enhancement" tabindex="-1"><a class="header-anchor" href="#equipping-diffusion-models-with-differentiable-spatial-entropy-for-low-light-image-enhancement"><span>Equipping Diffusion Models with Differentiable Spatial Entropy for Low-Light Image Enhancement</span></a></h4><p><strong>Authors</strong>: Wenyi Lian, Wenjing Lian, Ziwei Luo</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09735v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09735v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input. However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality. In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values. The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets. To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas. Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss. In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024. All these results illustrate the effectiveness of our statistic-based entropy loss. Code is available at https://github.com/shermanlian/spatial-entropy-loss.</p><h4 id="photo-realistic-image-restoration-in-the-wild-with-controlled-vision-language-models" tabindex="-1"><a class="header-anchor" href="#photo-realistic-image-restoration-in-the-wild-with-controlled-vision-language-models"><span>Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models</span></a></h4><p><strong>Authors</strong>: Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09732v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09732v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets. Typically, diffusion models trained in specific datasets fail to recover images that have out-of-distribution degradations. To address this problem, this work leverages a capable vision-language model and a synthetic degradation pipeline to learn image restoration in the wild (wild IR). More specifically, all low-quality images are simulated with a synthetic degradation pipeline that contains multiple common degradations such as blur, resize, noise, and JPEG compression. Then we introduce robust training for a degradation-aware CLIP model to extract enriched image content features to assist high-quality image restoration. Our base diffusion model is the image restoration SDE (IR-SDE). Built upon it, we further present a posterior sampling strategy for fast noise-free image generation. We evaluate our model on both synthetic and real-world degradation datasets. Moreover, experiments on the unified image restoration task illustrate that the proposed posterior sampling improves image generation quality for various degradations.</p><h4 id="all-in-one-simulation-based-inference" tabindex="-1"><a class="header-anchor" href="#all-in-one-simulation-based-inference"><span>All-in-one simulation-based inference</span></a></h4><p><strong>Authors</strong>: Manuel Gloeckler, Michael Deistler, Christian Weilbach, Frank Wood, Jakob H. Macke</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09636v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09636v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.</p><h4 id="tmpq-dm-joint-timestep-reduction-and-quantization-precision-selection-for-efficient-diffusion-models" tabindex="-1"><a class="header-anchor" href="#tmpq-dm-joint-timestep-reduction-and-quantization-precision-selection-for-efficient-diffusion-models"><span>TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection for Efficient Diffusion Models</span></a></h4><p><strong>Authors</strong>: Haojun Sun, Chen Tang, Zhi Wang, Yuan Meng, Jingyan jiang, Xinzhu Ma, Wenwu Zhu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09532v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09532v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Diffusion models have emerged as preeminent contenders in the realm of generative models. Distinguished by their distinctive sequential generative processes, characterized by hundreds or even thousands of timesteps, diffusion models progressively reconstruct images from pure Gaussian noise, with each timestep necessitating full inference of the entire model. However, the substantial computational demands inherent to these models present challenges for deployment, quantization is thus widely used to lower the bit-width for reducing the storage and computing overheads. Current quantization methodologies primarily focus on model-side optimization, disregarding the temporal dimension, such as the length of the timestep sequence, thereby allowing redundant timesteps to continue consuming computational resources, leaving substantial scope for accelerating the generative process. In this paper, we introduce TMPQ-DM, which jointly optimizes timestep reduction and quantization to achieve a superior performance-efficiency trade-off, addressing both temporal and model optimization aspects. For timestep reduction, we devise a non-uniform grouping scheme tailored to the non-uniform nature of the denoising process, thereby mitigating the explosive combinations of timesteps. In terms of quantization, we adopt a fine-grained layer-wise approach to allocate varying bit-widths to different layers based on their respective contributions to the final generative performance, thus rectifying performance degradation observed in prior studies. To expedite the evaluation of fine-grained quantization, we further devise a super-network to serve as a precision solver by leveraging shared quantization results. These two design components are seamlessly integrated within our framework, enabling rapid joint exploration of the exponentially large decision space via a gradient-free evolutionary search algorithm.</p><h4 id="magic-clothing-controllable-garment-driven-image-synthesis" tabindex="-1"><a class="header-anchor" href="#magic-clothing-controllable-garment-driven-image-synthesis"><span>Magic Clothing: Controllable Garment-Driven Image Synthesis</span></a></h4><p><strong>Authors</strong>: Weifeng Chen, Tao Gu, Yuhao Xu, Chengcai Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09512v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09512v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We propose Magic Clothing, a latent diffusion model (LDM)-based network architecture for an unexplored garment-driven image synthesis task. Aiming at generating customized characters wearing the target garments with diverse text prompts, the image controllability is the most critical issue, i.e., to preserve the garment details and maintain faithfulness to the text prompts. To this end, we introduce a garment extractor to capture the detailed garment features, and employ self-attention fusion to incorporate them into the pretrained LDMs, ensuring that the garment details remain unchanged on the target character. Then, we leverage the joint classifier-free guidance to balance the control of garment features and text prompts over the generated results. Meanwhile, the proposed garment extractor is a plug-in module applicable to various finetuned LDMs, and it can be combined with other extensions like ControlNet and IP-Adapter to enhance the diversity and controllability of the generated characters. Furthermore, we design Matched-Points-LPIPS (MP-LPIPS), a robust metric for evaluating the consistency of the target image to the source garment. Extensive experiments demonstrate that our Magic Clothing achieves state-of-the-art results under various conditional controls for garment-driven image synthesis. Our source code is available at https://github.com/ShineChen1024/MagicClothing.</p><h4 id="physcene-physically-interactable-3d-scene-synthesis-for-embodied-ai" tabindex="-1"><a class="header-anchor" href="#physcene-physically-interactable-3d-scene-synthesis-for-embodied-ai"><span>PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI</span></a></h4><p><strong>Authors</strong>: Yandan Yang, Baoxiong Jia, Peiyuan Zhi, Siyuan Huang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09465v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09465v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: With recent developments in Embodied Artificial Intelligence (EAI) research, there has been a growing demand for high-quality, large-scale interactive scene generation. While prior methods in scene synthesis have prioritized the naturalness and realism of the generated scenes, the physical plausibility and interactivity of scenes have been largely left unexplored. To address this disparity, we introduce PhyScene, a novel method dedicated to generating interactive 3D scenes characterized by realistic layouts, articulated objects, and rich physical interactivity tailored for embodied agents. Based on a conditional diffusion model for capturing scene layouts, we devise novel physics- and interactivity-based guidance mechanisms that integrate constraints from object collision, room layout, and object reachability. Through extensive experiments, we demonstrate that PhyScene effectively leverages these guidance functions for physically interactable scene synthesis, outperforming existing state-of-the-art scene synthesis methods by a large margin. Our findings suggest that the scenes generated by PhyScene hold considerable potential for facilitating diverse skill acquisition among agents within interactive environments, thereby catalyzing further advancements in embodied AI research. Project website: http://physcene.github.io.</p><h4 id="super-resolution-of-biomedical-volumes-with-2d-supervision" tabindex="-1"><a class="header-anchor" href="#super-resolution-of-biomedical-volumes-with-2d-supervision"><span>Super-resolution of biomedical volumes with 2D supervision</span></a></h4><p><strong>Authors</strong>: Cheng Jiang, Alexander Gedeon, Yiwei Lyu, Eric Landgraf, Yufeng Zhang, Xinhai Hou, Akhil Kondepudi, Asadur Chowdury, Honglak Lee, Todd Hollon</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09425v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09425v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Volumetric biomedical microscopy has the potential to increase the diagnostic information extracted from clinical tissue specimens and improve the diagnostic accuracy of both human pathologists and computational pathology models. Unfortunately, barriers to integrating 3-dimensional (3D) volumetric microscopy into clinical medicine include long imaging times, poor depth / z-axis resolution, and an insufficient amount of high-quality volumetric data. Leveraging the abundance of high-resolution 2D microscopy data, we introduce masked slice diffusion for super-resolution (MSDSR), which exploits the inherent equivalence in the data-generating distribution across all spatial dimensions of biological specimens. This intrinsic characteristic allows for super-resolution models trained on high-resolution images from one plane (e.g., XY) to effectively generalize to others (XZ, YZ), overcoming the traditional dependency on orientation. We focus on the application of MSDSR to stimulated Raman histology (SRH), an optical imaging modality for biological specimen analysis and intraoperative diagnosis, characterized by its rapid acquisition of high-resolution 2D images but slow and costly optical z-sectioning. To evaluate MSDSR&#39;s efficacy, we introduce a new performance metric, SliceFID, and demonstrate MSDSR&#39;s superior performance over baseline models through extensive evaluations. Our findings reveal that MSDSR not only significantly enhances the quality and resolution of 3D volumetric data, but also addresses major obstacles hindering the broader application of 3D volumetric microscopy in clinical diagnostics and biomedical research.</p><h4 id="neural-mckean-vlasov-processes-distributional-dependence-in-diffusion-processes" tabindex="-1"><a class="header-anchor" href="#neural-mckean-vlasov-processes-distributional-dependence-in-diffusion-processes"><span>Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion Processes</span></a></h4><p><strong>Authors</strong>: Haoming Yang, Ali Hasan, Yuting Ng, Vahid Tarokh</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09402v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09402v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: McKean-Vlasov stochastic differential equations (MV-SDEs) provide a mathematical description of the behavior of an infinite number of interacting particles by imposing a dependence on the particle density. As such, we study the influence of explicitly including distributional information in the parameterization of the SDE. We propose a series of semi-parametric methods for representing MV-SDEs, and corresponding estimators for inferring parameters from data based on the properties of the MV-SDE. We analyze the characteristics of the different architectures and estimators, and consider their applicability in relevant machine learning problems. We empirically compare the performance of the different architectures and estimators on real and synthetic datasets for time series and probabilistic modeling. The results suggest that explicitly including distributional dependence in the parameterization of the SDE is effective in modeling temporal data with interaction under an exchangeability assumption while maintaining strong performance for standard It^o-SDEs due to the richer class of probability flows associated with MV-SDEs.</p><h4 id="watermark-embedded-adversarial-examples-for-copyright-protection-against-diffusion-models" tabindex="-1"><a class="header-anchor" href="#watermark-embedded-adversarial-examples-for-copyright-protection-against-diffusion-models"><span>Watermark-embedded Adversarial Examples for Copyright Protection against Diffusion Models</span></a></h4><p><strong>Authors</strong>: Peifei Zhu, Tsubasa Takahashi, Hirokatsu Kataoka</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09401v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09401v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Diffusion Models (DMs) have shown remarkable capabilities in various image-generation tasks. However, there are growing concerns that DMs could be used to imitate unauthorized creations and thus raise copyright issues. To address this issue, we propose a novel framework that embeds personal watermarks in the generation of adversarial examples. Such examples can force DMs to generate images with visible watermarks and prevent DMs from imitating unauthorized images. We construct a generator based on conditional adversarial networks and design three losses (adversarial loss, GAN loss, and perturbation loss) to generate adversarial examples that have subtle perturbation but can effectively attack DMs to prevent copyright violations. Training a generator for a personal watermark by our method only requires 5-10 samples within 2-3 minutes, and once the generator is trained, it can generate adversarial examples with that watermark significantly fast (0.2s per image). We conduct extensive experiments in various conditional image-generation scenarios. Compared to existing methods that generate images with chaotic textures, our method adds visible watermarks on the generated images, which is a more straightforward way to indicate copyright violations. We also observe that our adversarial examples exhibit good transferability across unknown generative models. Therefore, this work provides a simple yet powerful way to protect copyright from DM-based imitation.</p><h2 id="_2024-04-14" tabindex="-1"><a class="header-anchor" href="#_2024-04-14"><span>2024-04-14</span></a></h2><h4 id="roofdiffusion-constructing-roofs-from-severely-corrupted-point-data-via-diffusion" tabindex="-1"><a class="header-anchor" href="#roofdiffusion-constructing-roofs-from-severely-corrupted-point-data-via-diffusion"><span>RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via Diffusion</span></a></h4><p><strong>Authors</strong>: Kyle Shih-Huang Lo, Jörg Peters, Eric Spellman</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09290v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09290v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Accurate completion and denoising of roof height maps are crucial to reconstructing high-quality 3D buildings. Repairing sparse points can enhance low-cost sensor use and reduce UAV flight overlap. RoofDiffusion is a new end-to-end self-supervised diffusion technique for robustly completing, in particular difficult, roof height maps. RoofDiffusion leverages widely-available curated footprints and can so handle up to 99% point sparsity and 80% roof area occlusion (regional incompleteness). A variant, No-FP RoofDiffusion, simultaneously predicts building footprints and heights. Both quantitatively outperform state-of-the-art unguided depth completion and representative inpainting methods for Digital Elevation Models (DEM), on both a roof-specific benchmark and the BuildingNet dataset. Qualitative assessments show the effectiveness of RoofDiffusion for datasets with real-world scans including AHN3, Dales3D, and USGS 3DEP LiDAR. Tested with the leading City3D algorithm, preprocessing height maps with RoofDiffusion noticeably improves 3D building reconstruction. RoofDiffusion is complemented by a new dataset of 13k complex roof geometries, focusing on long-tail issues in remote sensing; a novel simulation of tree occlusion; and a wide variety of large-area roof cut-outs for data augmentation and benchmarking.</p><h4 id="fault-detection-in-mobile-networks-using-diffusion-models" tabindex="-1"><a class="header-anchor" href="#fault-detection-in-mobile-networks-using-diffusion-models"><span>Fault Detection in Mobile Networks Using Diffusion Models</span></a></h4><p><strong>Authors</strong>: Mohamad Nabeel, Doumitrou Daniil Nimara, Tahar Zanouda</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09240v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09240v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In today&#39;s hyper-connected world, ensuring the reliability of telecom networks becomes increasingly crucial. Telecom networks encompass numerous underlying and intertwined software and hardware components, each providing different functionalities. To ensure the stability of telecom networks, telecom software, and hardware vendors developed several methods to detect any aberrant behavior in telecom networks and enable instant feedback and alerts. These approaches, although powerful, struggle to generalize due to the unsteady nature of the software-intensive embedded system and the complexity and diversity of multi-standard mobile networks. In this paper, we present a system to detect anomalies in telecom networks using a generative AI model. We evaluate several strategies using diffusion models to train the model for anomaly detection using multivariate time-series data. The contributions of this paper are threefold: (i) A proposal of a framework for utilizing diffusion models for time-series anomaly detection in telecom networks, (ii) A proposal of a particular Diffusion model architecture that outperforms other state-of-the-art techniques, (iii) Experiments on a real-world dataset to demonstrate that our model effectively provides explainable results, exposing some of its limitations and suggesting future research avenues to enhance its capabilities further.</p><h4 id="dreamscape-3d-scene-creation-via-gaussian-splatting-joint-correlation-modeling" tabindex="-1"><a class="header-anchor" href="#dreamscape-3d-scene-creation-via-gaussian-splatting-joint-correlation-modeling"><span>DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation Modeling</span></a></h4><p><strong>Authors</strong>: Xuening Yuan, Hongyu Yang, Yueming Zhao, Di Huang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09227v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09227v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recent progress in text-to-3D creation has been propelled by integrating the potent prior of Diffusion Models from text-to-image generation into the 3D domain. Nevertheless, generating 3D scenes characterized by multiple instances and intricate arrangements remains challenging. In this study, we present DreamScape, a method for creating highly consistent 3D scenes solely from textual descriptions, leveraging the strong 3D representation capabilities of Gaussian Splatting and the complex arrangement abilities of large language models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene representation, consisting of semantic primitives (objects) and their spatial transformations and relationships derived directly from text prompts using LLMs. This compositional representation allows for local-to-global optimization of the entire scene. A progressive scale control is tailored during local object generation, ensuring that objects of different sizes and densities adapt to the scene, which addresses training instability issue arising from simple blending in the subsequent global optimization stage. To mitigate potential biases of LLM priors, we model collision relationships between objects at the global level, enhancing physical correctness and overall realism. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we introduce a sparse initialization and densification strategy. Experiments demonstrate that DreamScape offers high usability and controllability, enabling the generation of high-fidelity 3D scenes from only text prompts and achieving state-of-the-art performance compared to other methods.</p><h4 id="loopanimate-loopable-salient-object-animation" tabindex="-1"><a class="header-anchor" href="#loopanimate-loopable-salient-object-animation"><span>LoopAnimate: Loopable Salient Object Animation</span></a></h4><p><strong>Authors</strong>: Fanyi Wang, Peng Liu, Haotian Hu, Dan Meng, Jingwen Su, Jinjin Xu, Yanhao Zhang, Xiaoming Ren, Zhiwang Zhang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09172v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09172v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Research on diffusion model-based video generation has advanced rapidly. However, limitations in object fidelity and generation length hinder its practical applications. Additionally, specific domains like animated wallpapers require seamless looping, where the first and last frames of the video match seamlessly. To address these challenges, this paper proposes LoopAnimate, a novel method for generating videos with consistent start and end frames. To enhance object fidelity, we introduce a framework that decouples multi-level image appearance and textual semantic information. Building upon an image-to-image diffusion model, our approach incorporates both pixel-level and feature-level information from the input image, injecting image appearance and textual semantic embeddings at different positions of the diffusion model. Existing UNet-based video generation models require to input the entire videos during training to encode temporal and positional information at once. However, due to limitations in GPU memory, the number of frames is typically restricted to 16. To address this, this paper proposes a three-stage training strategy with progressively increasing frame numbers and reducing fine-tuning modules. Additionally, we introduce the Temporal E nhanced Motion Module(TEMM) to extend the capacity for encoding temporal and positional information up to 36 frames. The proposed LoopAnimate, which for the first time extends the single-pass generation length of UNet-based video generation models to 35 frames while maintaining high-quality video generation. Experiments demonstrate that LoopAnimate achieves state-of-the-art performance in both objective metrics, such as fidelity and temporal consistency, and subjective evaluation results.</p><h4 id="rf-diffusion-radio-signal-generation-via-time-frequency-diffusion" tabindex="-1"><a class="header-anchor" href="#rf-diffusion-radio-signal-generation-via-time-frequency-diffusion"><span>RF-Diffusion: Radio Signal Generation via Time-Frequency Diffusion</span></a></h4><p><strong>Authors</strong>: Guoxuan Chi, Zheng Yang, Chenshu Wu, Jingao Xu, Yuchong Gao, Yunhao Liu, Tony Xiao Han</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09140v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09140v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Along with AIGC shines in CV and NLP, its potential in the wireless domain has also emerged in recent years. Yet, existing RF-oriented generative solutions are ill-suited for generating high-quality, time-series RF data due to limited representation capabilities. In this work, inspired by the stellar achievements of the diffusion model in CV and NLP, we adapt it to the RF domain and propose RF-Diffusion. To accommodate the unique characteristics of RF signals, we first introduce a novel Time-Frequency Diffusion theory to enhance the original diffusion model, enabling it to tap into the information within the time, frequency, and complex-valued domains of RF signals. On this basis, we propose a Hierarchical Diffusion Transformer to translate the theory into a practical generative DNN through elaborated design spanning network architecture, functional block, and complex-valued operator, making RF-Diffusion a versatile solution to generate diverse, high-quality, and time-series RF data. Performance comparison with three prevalent generative models demonstrates the RF-Diffusion&#39;s superior performance in synthesizing Wi-Fi and FMCW signals. We also showcase the versatility of RF-Diffusion in boosting Wi-Fi sensing systems and performing channel estimation in 5G networks.</p><h4 id="exploring-generative-ai-for-sim2real-in-driving-data-synthesis" tabindex="-1"><a class="header-anchor" href="#exploring-generative-ai-for-sim2real-in-driving-data-synthesis"><span>Exploring Generative AI for Sim2Real in Driving Data Synthesis</span></a></h4><p><strong>Authors</strong>: Haonan Zhao, Yiting Wang, Thomas Bashford-Rogers, Valentina Donzella, Kurt Debattista</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09111v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09111v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Datasets are essential for training and testing vehicle perception algorithms. However, the collection and annotation of real-world images is time-consuming and expensive. Driving simulators offer a solution by automatically generating various driving scenarios with corresponding annotations, but the simulation-to-reality (Sim2Real) domain gap remains a challenge. While most of the Generative Artificial Intelligence (AI) follows the de facto Generative Adversarial Nets (GANs)-based methods, the recent emerging diffusion probabilistic models have not been fully explored in mitigating Sim2Real challenges for driving data synthesis. To explore the performance, this paper applied three different generative AI methods to leverage semantic label maps from a driving simulator as a bridge for the creation of realistic datasets. A comparative analysis of these methods is presented from the perspective of image quality and perception. New synthetic datasets, which include driving images and auto-generated high-quality annotations, are produced with low costs and high scene variability. The experimental results show that although GAN-based methods are adept at generating high-quality images when provided with manually annotated labels, ControlNet produces synthetic datasets with fewer artefacts and more structural fidelity when using simulator-generated labels. This suggests that the diffusion-based approach may provide improved stability and an alternative method for addressing Sim2Real challenges.</p><h2 id="_2024-04-13" tabindex="-1"><a class="header-anchor" href="#_2024-04-13"><span>2024-04-13</span></a></h2><h4 id="rethinking-iterative-stereo-matching-from-diffusion-bridge-model-perspective" tabindex="-1"><a class="header-anchor" href="#rethinking-iterative-stereo-matching-from-diffusion-bridge-model-perspective"><span>Rethinking Iterative Stereo Matching from Diffusion Bridge Model Perspective</span></a></h4><p><strong>Authors</strong>: Yuguang Shi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09051v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09051v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recently, iteration-based stereo matching has shown great potential. However, these models optimize the disparity map using RNN variants. The discrete optimization process poses a challenge of information loss, which restricts the level of detail that can be expressed in the generated disparity map. In order to address these issues, we propose a novel training approach that incorporates diffusion models into the iterative optimization process. We designed a Time-based Gated Recurrent Unit (T-GRU) to correlate temporal and disparity outputs. Unlike standard recurrent units, we employ Agent Attention to generate more expressive features. We also designed an attention-based context network to capture a large amount of contextual information. Experiments on several public benchmarks show that we have achieved competitive stereo matching performance. Our model ranks first in the Scene Flow dataset, achieving over a 7% improvement compared to competing methods, and requires only 8 iterations to achieve state-of-the-art results.</p><h4 id="theoretical-research-on-generative-diffusion-models-an-overview" tabindex="-1"><a class="header-anchor" href="#theoretical-research-on-generative-diffusion-models-an-overview"><span>Theoretical research on generative diffusion models: an overview</span></a></h4><p><strong>Authors</strong>: Melike Nur Yeğin, Mehmet Fatih Amasyalı</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09016v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09016v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Generative diffusion models showed high success in many fields with a powerful theoretical background. They convert the data distribution to noise and remove the noise back to obtain a similar distribution. Many existing reviews focused on the specific application areas without concentrating on the research about the algorithm. Unlike them we investigated the theoretical developments of the generative diffusion models. These approaches mainly divide into two: training-based and sampling-based. Awakening to this allowed us a clear and understandable categorization for the researchers who will make new developments in the future.</p><h4 id="maskel-a-model-for-human-whole-body-x-rays-generation-from-human-masking-images" tabindex="-1"><a class="header-anchor" href="#maskel-a-model-for-human-whole-body-x-rays-generation-from-human-masking-images"><span>MaSkel: A Model for Human Whole-body X-rays Generation from Human Masking Images</span></a></h4><p><strong>Authors</strong>: Yingjie Xi, Boyuan Cheng, Jingyao Cai, Jian Jun Zhang, Xiaosong Yang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.09000v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.09000v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The human whole-body X-rays could offer a valuable reference for various applications, including medical diagnostics, digital animation modeling, and ergonomic design. The traditional method of obtaining X-ray information requires the use of CT (Computed Tomography) scan machines, which emit potentially harmful radiation. Thus it faces a significant limitation for realistic applications because it lacks adaptability and safety. In our work, We proposed a new method to directly generate the 2D human whole-body X-rays from the human masking images. The predicted images will be similar to the real ones with the same image style and anatomic structure. We employed a data-driven strategy. By leveraging advanced generative techniques, our model MaSkel(Masking image to Skeleton X-rays) could generate a high-quality X-ray image from a human masking image without the need for invasive and harmful radiation exposure, which not only provides a new path to generate highly anatomic and customized data but also reduces health risks. To our knowledge, our model MaSkel is the first work for predicting whole-body X-rays. In this paper, we did two parts of the work. The first one is to solve the data limitation problem, the diffusion-based techniques are utilized to make a data augmentation, which provides two synthetic datasets for preliminary pretraining. Then we designed a two-stage training strategy to train MaSkel. At last, we make qualitative and quantitative evaluations of the generated X-rays. In addition, we invite some professional doctors to assess our predicted data. These evaluations demonstrate the MaSkel&#39;s superior ability to generate anatomic X-rays from human masking images. The related code and links of the dataset are available at https://github.com/2022yingjie/MaSkel.</p><h4 id="multimodal-cross-document-event-coreference-resolution-using-linear-semantic-transfer-and-mixed-modality-ensembles" tabindex="-1"><a class="header-anchor" href="#multimodal-cross-document-event-coreference-resolution-using-linear-semantic-transfer-and-mixed-modality-ensembles"><span>Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles</span></a></h4><p><strong>Authors</strong>: Abhijnan Nath, Huma Jamil, Shafiuddin Rehan Ahmed, George Baker, Rahul Ghosh, James H. Martin, Nathaniel Blanchard, Nikhil Krishnaswamy</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08949v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08949v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Event coreference resolution (ECR) is the task of determining whether distinct mentions of events within a multi-document corpus are actually linked to the same underlying occurrence. Images of the events can help facilitate resolution when language is ambiguous. Here, we propose a multimodal cross-document event coreference resolution method that integrates visual and textual cues with a simple linear map between vision and language models. As existing ECR benchmark datasets rarely provide images for all event mentions, we augment the popular ECB+ dataset with event-centric images scraped from the internet and generated using image diffusion models. We establish three methods that incorporate images and text for coreference: 1) a standard fused model with finetuning, 2) a novel linear mapping method without finetuning and 3) an ensembling approach based on splitting mention pairs by semantic and discourse-level difficulty. We evaluate on 2 datasets: the augmented ECB+, and AIDA Phase 1. Our ensemble systems using cross-modal linear mapping establish an upper limit (91.9 CoNLL F1) on ECB+ ECR performance given the preprocessing assumptions used, and establish a novel baseline on AIDA Phase 1. Our results demonstrate the utility of multimodal information in ECR for certain challenging coreference problems, and highlight a need for more multimodal resources in the coreference resolution space.</p><h4 id="enforcing-paraphrase-generation-via-controllable-latent-diffusion" tabindex="-1"><a class="header-anchor" href="#enforcing-paraphrase-generation-via-controllable-latent-diffusion"><span>Enforcing Paraphrase Generation via Controllable Latent Diffusion</span></a></h4><p><strong>Authors</strong>: Wei Zou, Ziyuan Zhuang, Shujian Huang, Jia Liu, Jiajun Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08938v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08938v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Paraphrase generation aims to produce high-quality and diverse utterances of a given text. Though state-of-the-art generation via the diffusion model reconciles generation quality and diversity, textual diffusion suffers from a truncation issue that hinders efficiency and quality control. In this work, we propose \textit{L}atent \textit{D}iffusion \textit{P}araphraser~(LDP), a novel paraphrase generation by modeling a controllable diffusion process given a learned latent space. LDP achieves superior generation efficiency compared to its diffusion counterparts. It facilitates only input segments to enforce paraphrase semantics, which further improves the results without external features. Experiments show that LDP achieves improved and diverse paraphrase generation compared to baselines. Further analysis shows that our method is also helpful to other similar text generations and domain adaptations. Our code and data are available at https://github.com/NIL-zhuang/ld4pg.</p><h4 id="diffusion-models-meet-remote-sensing-principles-methods-and-perspectives" tabindex="-1"><a class="header-anchor" href="#diffusion-models-meet-remote-sensing-principles-methods-and-perspectives"><span>Diffusion Models Meet Remote Sensing: Principles, Methods, and Perspectives</span></a></h4><p><strong>Authors</strong>: Yidan Liu, Jun Yue, Shaobo Xia, Pedram Ghamisi, Weiying Xie, Leyuan Fang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08926v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08926v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: As a newly emerging advance in deep generative models, diffusion models have achieved state-of-the-art results in many fields, including computer vision, natural language processing, and molecule design. The remote sensing community has also noticed the powerful ability of diffusion models and quickly applied them to a variety of tasks for image processing. Given the rapid increase in research on diffusion models in the field of remote sensing, it is necessary to conduct a comprehensive review of existing diffusion model-based remote sensing papers, to help researchers recognize the potential of diffusion models and provide some directions for further exploration. Specifically, this paper first introduces the theoretical background of diffusion models, and then systematically reviews the applications of diffusion models in remote sensing, including image generation, enhancement, and interpretation. Finally, the limitations of existing remote sensing diffusion models and worthy research directions for further exploration are discussed and summarized.</p><h4 id="changeanywhere-sample-generation-for-remote-sensing-change-detection-via-semantic-latent-diffusion-model" tabindex="-1"><a class="header-anchor" href="#changeanywhere-sample-generation-for-remote-sensing-change-detection-via-semantic-latent-diffusion-model"><span>ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model</span></a></h4><p><strong>Authors</strong>: Kai Tang, Jin Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08892v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08892v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Remote sensing change detection (CD) is a pivotal technique that pinpoints changes on a global scale based on multi-temporal images. With the recent expansion of deep learning, supervised deep learning-based CD models have shown satisfactory performance. However, CD sample labeling is very time-consuming as it is densely labeled and requires expert knowledge. To alleviate this problem, we introduce ChangeAnywhere, a novel CD sample generation method using the semantic latent diffusion model and single-temporal images. Specifically, ChangeAnywhere leverages the relative ease of acquiring large single-temporal semantic datasets to generate large-scale, diverse, and semantically annotated bi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD samples, i.e., change implies semantically different, and non-change implies reasonable change under the same semantic constraints. We generated ChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD samples based on the proposed method. The ChangeAnywhere-100K significantly improved both zero-shot and few-shot performance on two CD benchmark datasets for various deep learning-based CD models, as demonstrated by transfer experiments. This paper delineates the enormous potential of ChangeAnywhere for CD sample generation and demonstrates the subsequent enhancement of model performance. Therefore, ChangeAnywhere offers a potent tool for remote sensing CD. All codes and pre-trained models will be available at https://github.com/tangkai-RS/ChangeAnywhere.</p><h2 id="_2024-04-12" tabindex="-1"><a class="header-anchor" href="#_2024-04-12"><span>2024-04-12</span></a></h2><h4 id="semantic-approach-to-quantifying-the-consistency-of-diffusion-model-image-generation" tabindex="-1"><a class="header-anchor" href="#semantic-approach-to-quantifying-the-consistency-of-diffusion-model-image-generation"><span>Semantic Approach to Quantifying the Consistency of Diffusion Model Image Generation</span></a></h4><p><strong>Authors</strong>: Brinnae Bent</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08799v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08799v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this study, we identify the need for an interpretable, quantitative score of the repeatability, or consistency, of image generation in diffusion models. We propose a semantic approach, using a pairwise mean CLIP (Contrastive Language-Image Pretraining) score as our semantic consistency score. We applied this metric to compare two state-of-the-art open-source image generation diffusion models, Stable Diffusion XL and PixArt-{\alpha}, and we found statistically significant differences between the semantic consistency scores for the models. Agreement between the Semantic Consistency Score selected model and aggregated human annotations was 94%. We also explored the consistency of SDXL and a LoRA-fine-tuned version of SDXL and found that the fine-tuned model had significantly higher semantic consistency in generated images. The Semantic Consistency Score proposed here offers a measure of image generation alignment, facilitating the evaluation of model architectures for specific tasks and aiding in informed decision-making regarding model selection.</p><h4 id="lossy-image-compression-with-foundation-diffusion-models" tabindex="-1"><a class="header-anchor" href="#lossy-image-compression-with-foundation-diffusion-models"><span>Lossy Image Compression with Foundation Diffusion Models</span></a></h4><p><strong>Authors</strong>: Lucas Relic, Roberto Azevedo, Markus Gross, Christopher Schroers</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08580v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08580v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Incorporating diffusion models in the image compression domain has the potential to produce realistic and detailed reconstructions, especially at extremely low bitrates. Previous methods focus on using diffusion models as expressive decoders robust to quantization errors in the conditioning signals, yet achieving competitive results in this manner requires costly training of the diffusion model and long inference times due to the iterative generative process. In this work we formulate the removal of quantization error as a denoising task, using diffusion to recover lost information in the transmitted image latent. Our approach allows us to perform less than 10% of the full diffusion generative process and requires no architectural changes to the diffusion model, enabling the use of foundation models as a strong prior without additional fine tuning of the backbone. Our proposed codec outperforms previous methods in quantitative realism metrics, and we verify that our reconstructions are qualitatively preferred by end users, even when other methods use twice the bitrate.</p><h4 id="balanced-mixed-type-tabular-data-synthesis-with-diffusion-models" tabindex="-1"><a class="header-anchor" href="#balanced-mixed-type-tabular-data-synthesis-with-diffusion-models"><span>Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models</span></a></h4><p><strong>Authors</strong>: Zeyu Yang, Peikun Guo, Khadija Zanna, Akane Sano</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08254v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08254v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Diffusion models have emerged as a robust framework for various generative tasks, such as image and audio synthesis, and have also demonstrated a remarkable ability to generate mixed-type tabular data comprising both continuous and discrete variables. However, current approaches to training diffusion models on mixed-type tabular data tend to inherit the imbalanced distributions of features present in the training dataset, which can result in biased sampling. In this research, we introduce a fair diffusion model designed to generate balanced data on sensitive attributes. We present empirical evidence demonstrating that our method effectively mitigates the class imbalance in training data while maintaining the quality of the generated samples. Furthermore, we provide evidence that our approach outperforms existing methods for synthesizing tabular data in terms of performance and fairness.</p><h2 id="_2024-04-11" tabindex="-1"><a class="header-anchor" href="#_2024-04-11"><span>2024-04-11</span></a></h2><h4 id="openbias-open-set-bias-detection-in-text-to-image-generative-models" tabindex="-1"><a class="header-anchor" href="#openbias-open-set-bias-detection-in-text-to-image-generative-models"><span>OpenBias: Open-set Bias Detection in Text-to-Image Generative Models</span></a></h4><p><strong>Authors</strong>: Moreno D&#39;Incà, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, Nicu Sebe</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07990v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07990v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previously proposed biases. We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before. Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement.</p><h4 id="rethinking-artistic-copyright-infringements-in-the-era-of-text-to-image-generative-models" tabindex="-1"><a class="header-anchor" href="#rethinking-artistic-copyright-infringements-in-the-era-of-text-to-image-generative-models"><span>Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models</span></a></h4><p><strong>Authors</strong>: Mazda Moayeri, Samyadeep Basu, Sriram Balasubramanian, Priyatham Kattakinda, Atoosa Chengini, Robert Brauneis, Soheil Feizi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.08030v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.08030v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy &quot;artistic style&quot; is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly. In our paper, we first reformulate the problem of &quot;artistic copyright infringement&quot; to a classification problem over image sets, instead of probing image-wise similarities. We then introduce ArtSavant, a practical (i.e., efficient and easy to understand) tool to (i) determine the unique style of an artist by comparing it to a reference dataset of works from 372 artists curated from WikiArt, and (ii) recognize if the identified style reappears in generated images. We leverage two complementary methods to perform artistic style classification over image sets, includingTagMatch, which is a novel inherently interpretable and attributable method, making it more suitable for broader use by non-technical stake holders (artists, lawyers, judges, etc). Leveraging ArtSavant, we then perform a large-scale empirical study to provide quantitative insight on the prevalence of artistic style copying across 3 popular text-to-image generative models. Namely, amongst a dataset of prolific artists (including many famous ones), only 20% of them appear to have their styles be at a risk of copying via simple prompting of today&#39;s popular text-to-image generative models.</p><h4 id="controlnet-improving-conditional-controls-with-efficient-consistency-feedback" tabindex="-1"><a class="header-anchor" href="#controlnet-improving-conditional-controls-with-efficient-consistency-feedback"><span>ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback</span></a></h4><p><strong>Authors</strong>: Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, Chen Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07987v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07987v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions.</p><h4 id="view-selection-for-3d-captioning-via-diffusion-ranking" tabindex="-1"><a class="header-anchor" href="#view-selection-for-3d-captioning-via-diffusion-ranking"><span>View Selection for 3D Captioning via Diffusion Ranking</span></a></h4><p><strong>Authors</strong>: Tiange Luo, Justin Johnson, Honglak Lee</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07984v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07984v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Scalable annotation approaches are crucial for constructing extensive 3D-text datasets, facilitating a broader range of applications. However, existing methods sometimes lead to the generation of hallucinated captions, compromising caption quality. This paper explores the issue of hallucination in 3D object captioning, with a focus on Cap3D method, which renders 3D objects into 2D views for captioning using pre-trained models. We pinpoint a major challenge: certain rendered views of 3D objects are atypical, deviating from the training data of standard image captioning models and causing hallucinations. To tackle this, we present DiffuRank, a method that leverages a pre-trained text-to-3D model to assess the alignment between 3D objects and their 2D rendered views, where the view with high alignment closely represent the object&#39;s characteristics. By ranking all rendered views and feeding the top-ranked ones into GPT4-Vision, we enhance the accuracy and detail of captions, enabling the correction of 200k captions in the Cap3D dataset and extending it to 1 million captions across Objaverse and Objaverse-XL datasets. Additionally, we showcase the adaptability of DiffuRank by applying it to pre-trained text-to-image models for a Visual Question Answering task, where it outperforms the CLIP model.</p><h4 id="taming-stable-diffusion-for-text-to-360°-panorama-image-generation" tabindex="-1"><a class="header-anchor" href="#taming-stable-diffusion-for-text-to-360°-panorama-image-generation"><span>Taming Stable Diffusion for Text to 360° Panorama Image Generation</span></a></h4><p><strong>Authors</strong>: Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, Jianfei Cai</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07949v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07949v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts. Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt. We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at https://chengzhag.github.io/publication/panfusion.</p><h4 id="consistencydet-robust-object-detector-with-denoising-paradigm-of-consistency-model" tabindex="-1"><a class="header-anchor" href="#consistencydet-robust-object-detector-with-denoising-paradigm-of-consistency-model"><span>ConsistencyDet: Robust Object Detector with Denoising Paradigm of Consistency Model</span></a></h4><p><strong>Authors</strong>: Lifan Jiang, Zhihui Wang, Changmiao Wang, Ming Li, Jiaxu Leng, Xindong Wu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07773v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07773v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Object detection, a quintessential task in the realm of perceptual computing, can be tackled using a generative methodology. In the present study, we introduce a novel framework designed to articulate object detection as a denoising diffusion process, which operates on perturbed bounding boxes of annotated entities. This framework, termed ConsistencyDet, leverages an innovative denoising concept known as the Consistency Model. The hallmark of this model is its self-consistency feature, which empowers the model to map distorted information from any temporal stage back to its pristine state, thereby realizing a ``one-step denoising&#39;&#39; mechanism. Such an attribute markedly elevates the operational efficiency of the model, setting it apart from the conventional Diffusion Model. Throughout the training phase, ConsistencyDet initiates the diffusion sequence with noise-infused boxes derived from the ground-truth annotations and conditions the model to perform the denoising task. Subsequently, in the inference stage, the model employs a denoising sampling strategy that commences with bounding boxes randomly sampled from a normal distribution. Through iterative refinement, the model transforms an assortment of arbitrarily generated boxes into the definitive detections. Comprehensive evaluations employing standard benchmarks, such as MS-COCO and LVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in performance metrics.</p><h4 id="an-overview-of-diffusion-models-applications-guided-generation-statistical-rates-and-optimization" tabindex="-1"><a class="header-anchor" href="#an-overview-of-diffusion-models-applications-guided-generation-statistical-rates-and-optimization"><span>An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization</span></a></h4><p><strong>Authors</strong>: Minshuo Chen, Song Mei, Jianqing Fan, Mengdi Wang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07771v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07771v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Diffusion models, a powerful and universal generative AI technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls. Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models.</p><h4 id="joint-conditional-diffusion-model-for-image-restoration-with-mixed-degradations" tabindex="-1"><a class="header-anchor" href="#joint-conditional-diffusion-model-for-image-restoration-with-mixed-degradations"><span>Joint Conditional Diffusion Model for Image Restoration with Mixed Degradations</span></a></h4><p><strong>Authors</strong>: Yufeng Yue, Meng Yu, Luojie Yang, Yi Yang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07770v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07770v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Image restoration is rather challenging in adverse weather conditions, especially when multiple degradations occur simultaneously. Blind image decomposition was proposed to tackle this issue, however, its effectiveness heavily relies on the accurate estimation of each component. Although diffusion-based models exhibit strong generative abilities in image restoration tasks, they may generate irrelevant contents when the degraded images are severely corrupted. To address these issues, we leverage physical constraints to guide the whole restoration process, where a mixed degradation model based on atmosphere scattering model is constructed. Then we formulate our Joint Conditional Diffusion Model (JCDM) by incorporating the degraded image and degradation mask to provide precise guidance. To achieve better color and detail recovery results, we further integrate a refinement network to reconstruct the restored image, where Uncertainty Estimation Block (UEB) is employed to enhance the features. Extensive experiments performed on both multi-weather and weather-specific datasets demonstrate the superiority of our method over state-of-the-art competing methods.</p><h4 id="generating-synthetic-satellite-imagery-with-deep-learning-text-to-image-models-technical-challenges-and-implications-for-monitoring-and-verification" tabindex="-1"><a class="header-anchor" href="#generating-synthetic-satellite-imagery-with-deep-learning-text-to-image-models-technical-challenges-and-implications-for-monitoring-and-verification"><span>Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification</span></a></h4><p><strong>Authors</strong>: Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07754v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07754v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.</p><h4 id="diffusing-in-someone-else-s-shoes-robotic-perspective-taking-with-diffusion" tabindex="-1"><a class="header-anchor" href="#diffusing-in-someone-else-s-shoes-robotic-perspective-taking-with-diffusion"><span>Diffusing in Someone Else&#39;s Shoes: Robotic Perspective Taking with Diffusion</span></a></h4><p><strong>Authors</strong>: Josua Spisak, Matthias Kerzel, Stefan Wermter</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07735v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07735v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Humanoid robots can benefit from their similarity to the human shape by learning from humans. When humans teach other humans how to perform actions, they often demonstrate the actions and the learning human can try to imitate the demonstration. Being able to mentally transfer from a demonstration seen from a third-person perspective to how it should look from a first-person perspective is fundamental for this ability in humans. As this is a challenging task, it is often simplified for robots by creating a demonstration in the first-person perspective. Creating these demonstrations requires more effort but allows for an easier imitation. We introduce a novel diffusion model aimed at enabling the robot to directly learn from the third-person demonstrations. Our model is capable of learning and generating the first-person perspective from the third-person perspective by translating the size and rotations of objects and the environment between two perspectives. This allows us to utilise the benefits of easy-to-produce third-person demonstrations and easy-to-imitate first-person demonstrations. The model can either represent the first-person perspective in an RGB image or calculate the joint values. Our approach significantly outperforms other image-to-image models in this task.</p><h4 id="applying-guidance-in-a-limited-interval-improves-sample-and-distribution-quality-in-diffusion-models" tabindex="-1"><a class="header-anchor" href="#applying-guidance-in-a-limited-interval-improves-sample-and-distribution-quality-in-diffusion-models"><span>Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models</span></a></h4><p><strong>Authors</strong>: Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07724v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07724v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance.</p><h4 id="implicit-and-explicit-language-guidance-for-diffusion-based-visual-perception" tabindex="-1"><a class="header-anchor" href="#implicit-and-explicit-language-guidance-for-diffusion-based-visual-perception"><span>Implicit and Explicit Language Guidance for Diffusion-based Visual Perception</span></a></h4><p><strong>Authors</strong>: Hefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07600v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07600v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-image diffusion models have shown powerful ability on conditional image synthesis. With large-scale vision-language pre-training, diffusion models are able to generate high-quality images with rich texture and reasonable structure under different text prompts. However, it is an open problem to adapt the pre-trained diffusion model for visual perception. In this paper, we propose an implicit and explicit language guidance framework for diffusion-based perception, named IEDP. Our IEDP comprises of an implicit language guidance branch and an explicit language guidance branch. The implicit branch employs frozen CLIP image encoder to directly generate implicit text embeddings that are fed to diffusion model, without using explicit text prompts. The explicit branch utilizes the ground-truth labels of corresponding images as text prompts to condition feature extraction of diffusion model. During training, we jointly train diffusion model by sharing the model weights of these two branches. As a result, implicit and explicit branches can jointly guide feature learning. During inference, we only employ implicit branch for final prediction, which does not require any ground-truth labels. Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation. Our IEDP achieves promising performance on both tasks. For semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K validation set, which outperforms the baseline method VPD by 2.2%. For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 10.2%.</p><h4 id="diffusion-posterior-sampling-for-simulation-based-inference-in-tall-data-settings" tabindex="-1"><a class="header-anchor" href="#diffusion-posterior-sampling-for-simulation-based-inference-in-tall-data-settings"><span>Diffusion posterior sampling for simulation-based inference in tall data settings</span></a></h4><p><strong>Authors</strong>: Julia Linhart, Gabriel Victorino Cardoso, Alexandre Gramfort, Sylvain Le Corff, Pedro L. C. Rodrigues</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07593v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07593v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Determining which parameters of a non-linear model could best describe a set of experimental data is a fundamental problem in science and it has gained much traction lately with the rise of complex large-scale simulators (a.k.a. black-box simulators). The likelihood of such models is typically intractable, which is why classical MCMC methods can not be used. Simulation-based inference (SBI) stands out in this context by only requiring a dataset of simulations to train deep generative models capable of approximating the posterior distribution that relates input parameters to a given observation. In this work, we consider a tall data extension in which multiple observations are available and one wishes to leverage their shared information to better infer the parameters of the model. The method we propose is built upon recent developments from the flourishing score-based diffusion literature and allows us to estimate the tall data posterior distribution simply using information from the score network trained on individual observations. We compare our method to recently proposed competing approaches on various numerical experiments and demonstrate its superiority in terms of numerical stability and computational cost.</p><h4 id="objblur-a-curriculum-learning-approach-with-progressive-object-level-blurring-for-improved-layout-to-image-generation" tabindex="-1"><a class="header-anchor" href="#objblur-a-curriculum-learning-approach-with-progressive-object-level-blurring-for-improved-layout-to-image-generation"><span>ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring for Improved Layout-to-Image Generation</span></a></h4><p><strong>Authors</strong>: Stanislav Frolov, Brian B. Moser, Sebastian Palacio, Andreas Dengel</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07564v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07564v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We present ObjBlur, a novel curriculum learning approach to improve layout-to-image generation models, where the task is to produce realistic images from layouts composed of boxes and labels. Our method is based on progressive object-level blurring, which effectively stabilizes training and enhances the quality of generated images. This curriculum learning strategy systematically applies varying degrees of blurring to individual objects or the background during training, starting from strong blurring to progressively cleaner images. Our findings reveal that this approach yields significant performance improvements, stabilized training, smoother convergence, and reduced variance between multiple runs. Moreover, our technique demonstrates its versatility by being compatible with generative adversarial networks and diffusion models, underlining its applicability across various generative modeling paradigms. With ObjBlur, we reach new state-of-the-art results on the complex COCO and Visual Genome datasets.</p><h4 id="cat-contrastive-adapter-training-for-personalized-image-generation" tabindex="-1"><a class="header-anchor" href="#cat-contrastive-adapter-training-for-personalized-image-generation"><span>CAT: Contrastive Adapter Training for Personalized Image Generation</span></a></h4><p><strong>Authors</strong>: Jae Wan Park, Sang Hyun Park, Jun Young Koh, Junha Lee, Min Song</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07554v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07554v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The emergence of various adapters, including Low-Rank Adaptation (LoRA) applied from the field of natural language processing, has allowed diffusion models to personalize image generation at a low cost. However, due to the various challenges including limited datasets and shortage of regularization and computation resources, adapter training often results in unsatisfactory outcomes, leading to the corruption of the backbone model&#39;s prior knowledge. One of the well known phenomena is the loss of diversity in object generation, especially within the same class which leads to generating almost identical objects with minor variations. This poses challenges in generation capabilities. To solve this issue, we present Contrastive Adapter Training (CAT), a simple yet effective strategy to enhance adapter training through the application of CAT loss. Our approach facilitates the preservation of the base model&#39;s original knowledge when the model initiates adapters. Furthermore, we introduce the Knowledge Preservation Score (KPS) to evaluate CAT&#39;s ability to keep the former information. We qualitatively and quantitatively compare CAT&#39;s improvement. Finally, we mention the possibility of CAT in the aspects of multi-concept adapter and optimization.</p><h2 id="_2024-04-10" tabindex="-1"><a class="header-anchor" href="#_2024-04-10"><span>2024-04-10</span></a></h2><h4 id="object-conditioned-energy-based-attention-map-alignment-in-text-to-image-diffusion-models" tabindex="-1"><a class="header-anchor" href="#object-conditioned-energy-based-attention-map-alignment-in-text-to-image-diffusion-models"><span>Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models</span></a></h4><p><strong>Authors</strong>: Yasi Zhang, Peiyu Yu, Ying Nian Wu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07389v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07389v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-image diffusion models have shown great success in generating high-quality text-guided images. Yet, these models may still fail to semantically align generated images with the provided text prompts, leading to problems like incorrect attribute binding and/or catastrophic object neglect. Given the pervasive object-oriented structure underlying text prompts, we introduce a novel object-conditioned Energy-Based Attention Map Alignment (EBAMA) method to address the aforementioned problems. We show that an object-centric attribute binding loss naturally emerges by approximately maximizing the log-likelihood of a $z$-parameterized energy-based model with the help of the negative sampling technique. We further propose an object-centric intensity regularizer to prevent excessive shifts of objects attention towards their attributes. Extensive qualitative and quantitative experiments, including human evaluation, on several challenging benchmarks demonstrate the superior performance of our method over previous strong counterparts. With better aligned attention maps, our approach shows great promise in further enhancing the text-controlled image editing ability of diffusion models.</p><h4 id="solving-masked-jigsaw-puzzles-with-diffusion-vision-transformers" tabindex="-1"><a class="header-anchor" href="#solving-masked-jigsaw-puzzles-with-diffusion-vision-transformers"><span>Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers</span></a></h4><p><strong>Authors</strong>: Jinyang Liu, Wondmgezahu Teshome, Sandesh Ghimire, Mario Sznaier, Octavia Camps</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07292v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07292v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Solving image and video jigsaw puzzles poses the challenging task of rearranging image fragments or video frames from unordered sequences to restore meaningful images and video sequences. Existing approaches often hinge on discriminative models tasked with predicting either the absolute positions of puzzle elements or the permutation actions applied to the original data. Unfortunately, these methods face limitations in effectively solving puzzles with a large number of elements. In this paper, we propose JPDVT, an innovative approach that harnesses diffusion transformers to address this challenge. Specifically, we generate positional information for image patches or video frames, conditioned on their underlying visual content. This information is then employed to accurately assemble the puzzle pieces in their correct positions, even in scenarios involving missing pieces. Our method achieves state-of-the-art performance on several datasets.</p><h4 id="gooddrag-towards-good-practices-for-drag-editing-with-diffusion-models" tabindex="-1"><a class="header-anchor" href="#gooddrag-towards-good-practices-for-drag-editing-with-diffusion-models"><span>GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models</span></a></h4><p><strong>Authors</strong>: Zewei Zhang, Huan Liu, Jun Chen, Xiangyu Xu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07206v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07206v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing. Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the diffusion process, effectively improving the fidelity of the result. We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction. In addition, we contribute to the benchmarking of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and Gemini Score, utilizing Large Multimodal Models. Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively. The project page is https://gooddrag.github.io.</p><h4 id="realmdreamer-text-driven-3d-scene-generation-with-inpainting-and-depth-diffusion" tabindex="-1"><a class="header-anchor" href="#realmdreamer-text-driven-3d-scene-generation-with-inpainting-and-depth-diffusion"><span>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion</span></a></h4><p><strong>Authors</strong>: Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07199v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07199v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image.</p><h4 id="instantmesh-efficient-3d-mesh-generation-from-a-single-image-with-sparse-view-large-reconstruction-models" tabindex="-1"><a class="header-anchor" href="#instantmesh-efficient-3d-mesh-generation-from-a-single-image-with-sparse-view-large-reconstruction-models"><span>InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models</span></a></h4><p><strong>Authors</strong>: Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, Ying Shan</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07191v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07191v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators.</p><h4 id="dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting" tabindex="-1"><a class="header-anchor" href="#dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting"><span>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting</span></a></h4><p><strong>Authors</strong>: Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06903v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06903v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary &quot;flat&quot; (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: http://dreamscene360.github.io/</p><h4 id="fine-color-guidance-in-diffusion-models-and-its-application-to-image-compression-at-extremely-low-bitrates" tabindex="-1"><a class="header-anchor" href="#fine-color-guidance-in-diffusion-models-and-its-application-to-image-compression-at-extremely-low-bitrates"><span>Fine color guidance in diffusion models and its application to image compression at extremely low bitrates</span></a></h4><p><strong>Authors</strong>: Tom Bordin, Thomas Maugey</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06865v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06865v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This study addresses the challenge of, without training or fine-tuning, controlling the global color aspect of images generated with a diffusion model. We rewrite the guidance equations to ensure that the outputs are closer to a known color map, and this without hindering the quality of the generation. Our method leads to new guidance equations. We show in the color guidance context that, the scaling of the guidance should not decrease but remains high throughout the diffusion process. In a second contribution, our guidance is applied in a compression framework, we combine both semantic and general color information on the image to decode the images at low cost. We show that our method is effective at improving fidelity and realism of compressed images at extremely low bit rates, when compared to other classical or more semantic oriented approaches.</p><h4 id="tuning-free-adaptive-style-incorporation-for-structure-consistent-text-driven-style-transfer" tabindex="-1"><a class="header-anchor" href="#tuning-free-adaptive-style-incorporation-for-structure-consistent-text-driven-style-transfer"><span>Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer</span></a></h4><p><strong>Authors</strong>: Yanqi Ge, Jiaqi Liu, Qingnan Fan, Xi Jiang, Ye Huang, Shuai Qin, Hong Gu, Wen Li, Lixin Duan</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06835v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06835v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this work, we target the task of text-driven style transfer in the context of text-to-image (T2I) diffusion models. The main challenge is consistent structure preservation while enabling effective style transfer effects. The past approaches in this field directly concatenate the content and style prompts for a prompt-level style injection, leading to unavoidable structure distortions. In this work, we propose a novel solution to the text-driven style transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve fine-grained feature-level style incorporation. It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and style features, and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and style information from a structure-consistent manner. Experimentally, our method exhibits much better performance in both structure preservation and stylized effects.</p><h4 id="zero-shot-point-cloud-completion-via-2d-priors" tabindex="-1"><a class="header-anchor" href="#zero-shot-point-cloud-completion-via-2d-priors"><span>Zero-shot Point Cloud Completion Via 2D Priors</span></a></h4><p><strong>Authors</strong>: Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06814v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06814v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: 3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training %, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a zero-shot framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion models to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data.</p><h4 id="urban-architect-steerable-3d-urban-scene-generation-with-layout-prior" tabindex="-1"><a class="header-anchor" href="#urban-architect-steerable-3d-urban-scene-generation-with-layout-prior"><span>Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior</span></a></h4><p><strong>Authors</strong>: Fan Lu, Kwan-Yee Lin, Yan Xu, Hongsheng Li, Guang Chen, Changjun Jiang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06780v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06780v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-3D generation has achieved remarkable success via large-scale text-to-image diffusion models. Nevertheless, there is no paradigm for scaling up the methodology to urban scale. Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization. In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior. It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation. Upon this, we propose two modifications -- (1) We introduce Layout-Guided Variational Score Distillation to address model optimization inadequacies. It conditions the score distillation sampling process with geometric and semantic constraints of 3D layouts. (2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes. Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time. We also present various scene editing demonstrations, showing the powers of steerable urban scene generation. Website: https://urbanarchitect.github.io.</p><h4 id="diffusiondialog-a-diffusion-model-for-diverse-dialog-generation-with-latent-space" tabindex="-1"><a class="header-anchor" href="#diffusiondialog-a-diffusion-model-for-diverse-dialog-generation-with-latent-space"><span>DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space</span></a></h4><p><strong>Authors</strong>: Jianxiang Xiang, Zhenhua Liu, Haodong Liu, Yin Bai, Jia Cheng, Wenliang Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06760v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06760v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In real-life conversations, the content is diverse, and there exists the one-to-many problem that requires diverse generation. Previous studies attempted to introduce discrete or Gaussian-based continuous latent variables to address the one-to-many problem, but the diversity is limited. Recently, diffusion models have made breakthroughs in computer vision, and some attempts have been made in natural language processing. In this paper, we propose DiffusionDialog, a novel approach to enhance the diversity of dialogue generation with the help of diffusion model. In our approach, we introduce continuous latent variables into the diffusion model. The problem of using latent variables in the dialog task is how to build both an effective prior of the latent space and an inferring process to obtain the proper latent given the context. By combining the encoder and latent-based diffusion model, we encode the response&#39;s latent representation in a continuous space as the prior, instead of fixed Gaussian distribution or simply discrete ones. We then infer the latent by denoising step by step with the diffusion model. The experimental results show that our model greatly enhances the diversity of dialog responses while maintaining coherence. Furthermore, in further analysis, we find that our diffusion model achieves high inference efficiency, which is the main challenge of applying diffusion models in natural language processing.</p><h4 id="disguised-copyright-infringement-of-latent-diffusion-models" tabindex="-1"><a class="header-anchor" href="#disguised-copyright-infringement-of-latent-diffusion-models"><span>Disguised Copyright Infringement of Latent Diffusion Models</span></a></h4><p><strong>Authors</strong>: Yiwei Lu, Matthew Y. R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06737v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06737v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access.</p><h4 id="voiceshop-a-unified-speech-to-speech-framework-for-identity-preserving-zero-shot-voice-editing" tabindex="-1"><a class="header-anchor" href="#voiceshop-a-unified-speech-to-speech-framework-for-identity-preserving-zero-shot-voice-editing"><span>VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing</span></a></h4><p><strong>Authors</strong>: Philip Anastassiou, Zhenyu Tang, Kainan Peng, Dongya Jia, Jiaxin Li, Ming Tu, Yuping Wang, Yuxuan Wang, Mingbo Ma</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06674v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06674v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We present VoiceShop, a novel speech-to-speech framework that can modify multiple attributes of speech, such as age, gender, accent, and speech style, in a single forward pass while preserving the input speaker&#39;s timbre. Previous works have been constrained to specialized models that can only edit these attributes individually and suffer from the following pitfalls: the magnitude of the conversion effect is weak, there is no zero-shot capability for out-of-distribution speakers, or the synthesized outputs exhibit undesirable timbre leakage. Our work proposes solutions for each of these issues in a simple modular framework based on a conditional diffusion backbone model with optional normalizing flow-based and sequence-to-sequence speaker attribute-editing modules, whose components can be combined or removed during inference to meet a wide array of tasks without additional model finetuning. Audio samples are available at \url{https://voiceshopai.github.io}.</p><h4 id="safegen-mitigating-unsafe-content-generation-in-text-to-image-models" tabindex="-1"><a class="header-anchor" href="#safegen-mitigating-unsafe-content-generation-in-text-to-image-models"><span>SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models</span></a></h4><p><strong>Authors</strong>: Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06666v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06666v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexual scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block explicit NSFW-related content (e.g., naked or sexy) but may still be vulnerable to adversarial prompts inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate unsafe content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate unsafe visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets demonstrate SafeGen&#39;s effectiveness in mitigating unsafe content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.</p><h2 id="_2024-04-09" tabindex="-1"><a class="header-anchor" href="#_2024-04-09"><span>2024-04-09</span></a></h2><h4 id="training-free-open-vocabulary-segmentation-with-offline-diffusion-augmented-prototype-generation" tabindex="-1"><a class="header-anchor" href="#training-free-open-vocabulary-segmentation-with-offline-diffusion-augmented-prototype-generation"><span>Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation</span></a></h4><p><strong>Authors</strong>: Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06542v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06542v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form. Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal alignments. However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts. Further, training on large-scale datasets inevitably brings significant computational costs. In this paper, we propose FreeDA, a training-free diffusion-augmented method for open-vocabulary semantic segmentation, which leverages the ability of diffusion models to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts. At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities. Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training.</p><h4 id="magic-boost-boost-3d-generation-with-mutli-view-conditioned-diffusion" tabindex="-1"><a class="header-anchor" href="#magic-boost-boost-3d-generation-with-mutli-view-conditioned-diffusion"><span>Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion</span></a></h4><p><strong>Authors</strong>: Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Jiashi Feng, Guosheng Lin</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06429v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06429v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Benefiting from the rapid development of 2D diffusion models, 3D content creation has made significant progress recently. One promising solution involves the fine-tuning of pre-trained 2D diffusion models to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models. However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries. To solve this problem, we propose Magic-Boost, a multi-view conditioned diffusion model that significantly refines coarse generative results through a brief period of SDS optimization ($\sim15$min). Compared to the previous text or single image based diffusion models, Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images. It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both geometry and texture of the initial generative results. Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details. (Project Page: https://magic-research.github.io/magic-boost/)</p><h4 id="diffharmony-latent-diffusion-model-meets-image-harmonization" tabindex="-1"><a class="header-anchor" href="#diffharmony-latent-diffusion-model-meets-image-harmonization"><span>DiffHarmony: Latent Diffusion Model Meets Image Harmonization</span></a></h4><p><strong>Authors</strong>: Pengfei Zhou, Fangxiang Feng, Xiaojie Wang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06139v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06139v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Image harmonization, which involves adjusting the foreground of a composite image to attain a unified visual consistency with the background, can be conceptualized as an image-to-image translation task. Diffusion models have recently promoted the rapid development of image-to-image translation tasks . However, training diffusion models from scratch is computationally intensive. Fine-tuning pre-trained latent diffusion models entails dealing with the reconstruction error induced by the image compression autoencoder, making it unsuitable for image generation tasks that involve pixel-level evaluation metrics. To deal with these issues, in this paper, we first adapt a pre-trained latent diffusion model to the image harmonization task to generate the harmonious but potentially blurry initial images. Then we implement two strategies: utilizing higher-resolution images during inference and incorporating an additional refinement stage, to further enhance the clarity of the initially harmonized images. Extensive experiments on iHarmony4 datasets demonstrate the superiority of our proposed method. The code and model will be made publicly available at https://github.com/nicecv/DiffHarmony .</p><h4 id="hash3d-training-free-acceleration-for-3d-generation" tabindex="-1"><a class="header-anchor" href="#hash3d-training-free-acceleration-for-3d-generation"><span>Hash3D: Training-free Acceleration for 3D Generation</span></a></h4><p><strong>Authors</strong>: Xingyi Yang, Xinchao Wang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06091v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06091v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model&#39;s inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D&#39;s versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D&#39;s integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at https://adamdad.github.io/hash3D/.</p><h4 id="greedy-dim-greedy-algorithms-for-unreasonably-effective-face-morphs" tabindex="-1"><a class="header-anchor" href="#greedy-dim-greedy-algorithms-for-unreasonably-effective-face-morphs"><span>Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs</span></a></h4><p><strong>Authors</strong>: Zander W. Blasingame, Chen Liu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06025v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06025v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Morphing attacks are an emerging threat to state-of-the-art Face Recognition (FR) systems, which aim to create a single image that contains the biometric information of multiple identities. Diffusion Morphs (DiM) are a recently proposed morphing attack that has achieved state-of-the-art performance for representation-based morphing attacks. However, none of the existing research on DiMs have leveraged the iterative nature of DiMs and left the DiM model as a black box, treating it no differently than one would a Generative Adversarial Network (GAN) or Varational AutoEncoder (VAE). We propose a greedy strategy on the iterative sampling process of DiM models which searches for an optimal step guided by an identity-based heuristic function. We compare our proposed algorithm against ten other state-of-the-art morphing algorithms using the open-source SYN-MAD 2022 competition dataset. We find that our proposed algorithm is unreasonably effective, fooling all of the tested FR systems with an MMPMR of 100%, outperforming all other morphing algorithms compared.</p><h4 id="tackling-structural-hallucination-in-image-translation-with-local-diffusion" tabindex="-1"><a class="header-anchor" href="#tackling-structural-hallucination-in-image-translation-with-local-diffusion"><span>Tackling Structural Hallucination in Image Translation with Local Diffusion</span></a></h4><p><strong>Authors</strong>: Seunghoi Kim, Chen Jin, Tom Diethe, Matteo Figini, Henry F. J. Tregidgo, Asher Mullokandov, Philip Teare, Daniel C. Alexander</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05980v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05980v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recent developments in diffusion models have advanced conditioned image generation, yet they struggle with reconstructing out-of-distribution (OOD) images, such as unseen tumors in medical images, causing <code>image hallucination&#39;&#39; and risking misdiagnosis. We hypothesize such hallucinations result from local OOD regions in the conditional images. We verify that partitioning the OOD region and conducting separate image generations alleviates hallucinations in several applications. From this, we propose a training-free diffusion framework that reduces hallucination with multiple Local Diffusion processes. Our approach involves OOD estimation followed by two modules: a </code>branching&#39;&#39; module generates locally both within and outside OOD regions, and a ``fusion&#39;&#39; module integrates these predictions into one. Our evaluation shows our method mitigates hallucination over baseline models quantitatively and qualitatively, reducing misdiagnosis by 40% and 25% in the real-world medical and natural image datasets, respectively. It also demonstrates compatibility with various pre-trained diffusion models.</p><h2 id="_2024-04-08" tabindex="-1"><a class="header-anchor" href="#_2024-04-08"><span>2024-04-08</span></a></h2><h4 id="moma-multimodal-llm-adapter-for-fast-personalized-image-generation" tabindex="-1"><a class="header-anchor" href="#moma-multimodal-llm-adapter-for-fast-personalized-image-generation"><span>MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</span></a></h4><p><strong>Authors</strong>: Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05674v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05674v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements.</p><h4 id="yaart-yet-another-art-rendering-technology" tabindex="-1"><a class="header-anchor" href="#yaart-yet-another-art-rendering-technology"><span>YaART: Yet Another ART Rendering Technology</span></a></h4><p><strong>Authors</strong>: Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05666v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05666v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier. This study introduces YaART, a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF). During the development of YaART, we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before. In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice. Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training. From the quality perspective, YaART is consistently preferred by users over many existing state-of-the-art models.</p><h4 id="a-training-free-plug-and-play-watermark-framework-for-stable-diffusion" tabindex="-1"><a class="header-anchor" href="#a-training-free-plug-and-play-watermark-framework-for-stable-diffusion"><span>A Training-Free Plug-and-Play Watermark Framework for Stable Diffusion</span></a></h4><p><strong>Authors</strong>: Guokai Zhang, Lanjun Wang, Yuting Su, An-An Liu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05607v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05607v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Nowadays, the family of Stable Diffusion (SD) models has gained prominence for its high quality outputs and scalability. This has also raised security concerns on social media, as malicious users can create and disseminate harmful content. Existing approaches involve training components or entire SDs to embed a watermark in generated images for traceability and responsibility attribution. However, in the era of AI-generated content (AIGC), the rapid iteration of SDs renders retraining with watermark models costly. To address this, we propose a training-free plug-and-play watermark framework for SDs. Without modifying any components of SDs, we embed diverse watermarks in the latent space, adapting to the denoising process. Our experimental findings reveal that our method effectively harmonizes image quality and watermark invisibility. Furthermore, it performs robustly under various attacks. We also have validated that our method is generalized to multiple versions of SDs, even without retraining the watermark model.</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><!----><!----></div></footer><!----><!--[--><!--[--><div class="my-footer">Copyright © 2024-present OpenDesign Community</div><!--]--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-AeV9POxF.js" defer></script>
  </body>
</html>
