<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.9" />
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html,
      body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme')
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia('(prefers-color-scheme: dark)').matches
      if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
        document.documentElement.classList.toggle('dark', true)
      }
    </script>
    <meta property="og:url" content="https://opendesign.world/posts/Generative-Art/paper/Stable%20Diffusion.html"><meta property="og:site_name" content="OpenDesign"><meta property="og:title" content="Stable Diffusion"><meta property="og:description" content="Stable Diffusion 2024-04-11 OpenBias: Open-set Bias Detection in Text-to-Image Generative Models Authors: Moreno D'Incà, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Stable Diffusion","image":[""],"dateModified":null,"author":[]}</script><script src="/translate.js"></script><script src="/model-viewer.min.js" type="module"></script><meta name="google-site-verification" content="1"><meta name="description" content="OpenDesign - Open Source Makes Design More Transparent. Enhance transparency, collaboration, and innovation in design through our open-source community."><meta name="keywords" content="OpenDesign, open source, design, collaboration, innovation, transparency"><meta name="author" content="OpenDesign Community"><link rel="icon" href="/favicon.ico"><title>Stable Diffusion | OpenDesign</title>
    <link rel="preload" href="/assets/style-Dz7YGj4i.css" as="style"><link rel="stylesheet" href="/assets/style-Dz7YGj4i.css">
    <link rel="modulepreload" href="/assets/app-b0nPsb4c.js"><link rel="modulepreload" href="/assets/Stable Diffusion.html-C_ZL2kPt.js">
    <link rel="prefetch" href="/assets/index.html-RYES8llX.js" as="script"><link rel="prefetch" href="/assets/contact.html-DJeCFcz0.js" as="script"><link rel="prefetch" href="/assets/contribute.html-DBC0t8sj.js" as="script"><link rel="prefetch" href="/assets/mission.html-5TArRTXc.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-qLXNmfba.js" as="script"><link rel="prefetch" href="/assets/index.html-C22vorTc.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-BikkUWpl.js" as="script"><link rel="prefetch" href="/assets/index.html-DuZGdo9x.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-BB9Ms0D1.js" as="script"><link rel="prefetch" href="/assets/index.html-DfWKxhp_.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-DaVeC4yx.js" as="script"><link rel="prefetch" href="/assets/index.html-QVL1lfZk.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-D9kO4TNm.js" as="script"><link rel="prefetch" href="/assets/index.html-D3tPx5PR.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-D6LoSBIq.js" as="script"><link rel="prefetch" href="/assets/index.html-Cbnfwb4B.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-C3_dSDLa.js" as="script"><link rel="prefetch" href="/assets/index.html-CkbDHWVJ.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-BRgwLfNL.js" as="script"><link rel="prefetch" href="/assets/index.html-C9fTtnTq.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-C0qW8AIC.js" as="script"><link rel="prefetch" href="/assets/index.html-D6kdia-E.js" as="script"><link rel="prefetch" href="/assets/Contributors.html-crwo22RP.js" as="script"><link rel="prefetch" href="/assets/index.html-DOZXp1SS.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-DZIIK2RO.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BHioDkoh.js" as="script"><link rel="prefetch" href="/assets/Resource.html-DvSDRnxq.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-CZiQapWO.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-bvcK1AKW.js" as="script"><link rel="prefetch" href="/assets/HCI.html-dXxH7LEe.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BMCMOew_.js" as="script"><link rel="prefetch" href="/assets/Resource.html-Wk9xeSCC.js" as="script"><link rel="prefetch" href="/assets/LLM.html-C2H3SHqD.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-CQjJA2vl.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-DjV8h2Kn.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-Cnendn5X.js" as="script"><link rel="prefetch" href="/assets/Resource.html-BYLE-_9V.js" as="script"><link rel="prefetch" href="/assets/Resource.html-B5-6O62m.js" as="script"><link rel="prefetch" href="/assets/Resource.html-HL0qNbxx.js" as="script"><link rel="prefetch" href="/assets/Robot.html-D-pVc6KC.js" as="script"><link rel="prefetch" href="/assets/Robot Vector.html-BFimH0Ya.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-BKK3fEcw.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-C6dlBEPT.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-xUaM2oPN.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-6jdoUJyp.js" as="script"><link rel="prefetch" href="/assets/Resource.html-YAMDUGXv.js" as="script"><link rel="prefetch" href="/assets/XR.html-BMSNQVzU.js" as="script"><link rel="prefetch" href="/assets/Contribute.html-GPEB0RFe.js" as="script"><link rel="prefetch" href="/assets/404.html-C_to2sbM.js" as="script"><link rel="prefetch" href="/assets/index.html-2kORSMOk.js" as="script"><link rel="prefetch" href="/assets/index.html-Be807kVt.js" as="script"><link rel="prefetch" href="/assets/index.html-C1TqMnZZ.js" as="script"><link rel="prefetch" href="/assets/index.html-BIdn8Oj6.js" as="script"><link rel="prefetch" href="/assets/index.html-CeNWwi7l.js" as="script"><link rel="prefetch" href="/assets/index.html-BSkFxRQs.js" as="script"><link rel="prefetch" href="/assets/index.html-B4epvfgr.js" as="script"><link rel="prefetch" href="/assets/index.html-BlVIe8Pu.js" as="script"><link rel="prefetch" href="/assets/index.html-CS9qdxB5.js" as="script"><link rel="prefetch" href="/assets/index.html-Dqi27vPV.js" as="script"><link rel="prefetch" href="/assets/index.html-CI1jTyQF.js" as="script"><link rel="prefetch" href="/assets/index.html-CGHs-pOL.js" as="script"><link rel="prefetch" href="/assets/index.html-BjGrkHEJ.js" as="script"><link rel="prefetch" href="/assets/index.html-ljvd00LE.js" as="script"><link rel="prefetch" href="/assets/index.html-N0JHcjnQ.js" as="script"><link rel="prefetch" href="/assets/index.html-oAjo7AFT.js" as="script"><link rel="prefetch" href="/assets/index.html-B4xR-2O4.js" as="script"><link rel="prefetch" href="/assets/index.html-DYCjxfu7.js" as="script"><link rel="prefetch" href="/assets/index.html-DS23g6tN.js" as="script"><link rel="prefetch" href="/assets/index.html-B4P_XdhI.js" as="script"><link rel="prefetch" href="/assets/index.html-9QUZMbww.js" as="script"><link rel="prefetch" href="/assets/index.html-Dk-2rews.js" as="script"><link rel="prefetch" href="/assets/index.html-dG6HYoS4.js" as="script"><link rel="prefetch" href="/assets/index.html-DrsHPc46.js" as="script"><link rel="prefetch" href="/assets/index.html-CuORxLNR.js" as="script"><link rel="prefetch" href="/assets/index.html-DTv0iMl4.js" as="script"><link rel="prefetch" href="/assets/index.html-BsvI3519.js" as="script"><link rel="prefetch" href="/assets/index.html-B7nKsjyY.js" as="script"><link rel="prefetch" href="/assets/index.html-D6SDrLMc.js" as="script"><link rel="prefetch" href="/assets/index.html-CWQrUzIZ.js" as="script"><link rel="prefetch" href="/assets/index.html-DE1rrYMr.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="logo" src="/logo.png" alt="OpenDesign"><span class="site-name can-hide" aria-hidden="true">OpenDesign</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide" aria-label="site navigation"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/knowledge/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/knowledge/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/knowledge/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/knowledge/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/knowledge/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/knowledge/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/knowledge/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/knowledge/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/solutions/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/solutions/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/solutions/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/solutions/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/solutions/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/solutions/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/solutions/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/solutions/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Paper"><span class="title">Paper</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Paper"><span class="title">Paper</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/paper/LLM.html" aria-label="LLM"><!--[--><!--[--><!--]--> LLM <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/paper/Stable%20Diffusion.html" aria-label="Stable Diffusion"><!--[--><!--[--><!--]--> Stable Diffusion <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/paper/HCI.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/paper/Robot.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/paper/XR.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="About"><span class="title">About</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="About"><span class="title">About</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/mission.html" aria-label="Mission"><!--[--><!--[--><!--]--> Mission <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contribute.html" aria-label="How to Contribute"><!--[--><!--[--><!--]--> How to Contribute <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contact.html" aria-label="Contact Us"><!--[--><!--[--><!--]--> Contact Us <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><!--[--><h4 class="navbar-dropdown-subtitle"><span>Contributors</span></h4><ul class="navbar-dropdown-subitem-wrapper"><!--[--><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/AGI/Contributors.html" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Generative-Art/Contributors.html" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HCI/Contributors.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HMI/Contributors.html" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Human-Factor/Contributors.html" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Intelligent-Device/Contributors.html" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Interactive-Installations/Contributors.html" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Robot/Contributors.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Spatial-Interaction/Contributors.html" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/XR/Contributors.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="navbar-item"><a class="external-link" href="https://github.com/OpenDesignGlobal/OpenDesign" rel="noopener noreferrer" target="_blank" aria-label="Github"><!--[--><!--[--><!--]--> Github <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--[--><select id="translateSelectLanguage"><option value="Not Translate" selected="selected">Not Translate</option><option value="Default">Auto Translate</option><option value="english">English</option><option value="chinese_simplified">简体中文</option><option value="chinese_traditional">繁體中文</option><option value="russian">Русский</option><option value="japanese">しろうと</option><option value="korean">한국어</option><option value="deutsch">Deutsch</option><option value="spanish">Español</option><option value="italian">italiano</option><option value="french">Français</option><option value="dutch">nederlands</option><option value="norwegian">Norge</option><option value="filipino">Pilipino</option><option value="lao">ກະຣຸນາ</option><option value="romanian">Română</option><option value="nepali">नेपालीName</option><option value="haitian_creole">Kreyòl ayisyen</option><option value="czech">český</option><option value="swedish">Svenska</option><option value="russian">Русский язык</option><option value="malagasy">Malagasy</option><option value="burmese">ဗာရမ်</option><option value="thai">คนไทย</option><option value="persian">Persian</option><option value="kurdish">Kurdî</option><option value="turkish">Türkçe</option><option value="hindi">हिन्दी</option><option value="bulgarian">български</option><option value="malay">Malay</option><option value="swahili">Kiswahili</option><option value="oriya">ଓଡିଆ</option><option value="irish">Íris</option><option value="gujarati">ગુજરાતી</option><option value="slovak">Slovenská</option><option value="hebrew">היברית</option><option value="hungarian">magyar</option><option value="marathi">मराठीName</option><option value="tamil">தாமில்</option><option value="estonian">eesti keel</option><option value="malayalam">മലമാലം</option><option value="inuktitut">ᐃᓄᒃᑎᑐᑦ</option><option value="arabic">بالعربية</option><option value="slovene">slovenščina</option><option value="bengali">বেঙ্গালী</option><option value="urdu">اوردو</option><option value="azerbaijani">azerbaijani</option><option value="portuguese">português</option><option value="samoan">lifiava</option><option value="afrikaans">afrikaans</option><option value="greek">ελληνικά</option><option value="danish">dansk</option><option value="amharic">amharic</option><option value="albanian">albanian</option><option value="lithuanian">Lietuva</option><option value="vietnamese">Tiếng Việt</option><option value="maltese">Malti</option><option value="finnish">suomi</option><option value="catalan">català</option><option value="croatian">hrvatski</option><option value="bosnian">bosnian</option><option value="polish">Polski</option><option value="latvian">latviešu</option><option value="maori">Maori</option></select><!--]--><!--]--><button class="toggle-color-mode-button" title="toggle color mode"><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items" aria-label="site navigation"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Knowledge"><span class="title">Knowledge</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/knowledge/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/knowledge/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/knowledge/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/knowledge/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/knowledge/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/knowledge/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/knowledge/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/knowledge/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Solutions"><span class="title">Solutions</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/solutions/" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/solutions/" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/knowledge/" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HMI/solutions/" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Human-Factor/solutions/" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Intelligent-Device/solutions/" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Interactive-Installations/solutions/" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/solutions/" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Spatial-Interaction/solutions/" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/solutions/" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Paper"><span class="title">Paper</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Paper"><span class="title">Paper</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/AGI/paper/LLM.html" aria-label="LLM"><!--[--><!--[--><!--]--> LLM <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Generative-Art/paper/Stable%20Diffusion.html" aria-label="Stable Diffusion"><!--[--><!--[--><!--]--> Stable Diffusion <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/HCI/paper/HCI.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Robot/paper/Robot.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/XR/paper/XR.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="About"><span class="title">About</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="About"><span class="title">About</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/mission.html" aria-label="Mission"><!--[--><!--[--><!--]--> Mission <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contribute.html" aria-label="How to Contribute"><!--[--><!--[--><!--]--> How to Contribute <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/About/contact.html" aria-label="Contact Us"><!--[--><!--[--><!--]--> Contact Us <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><!--[--><h4 class="navbar-dropdown-subtitle"><span>Contributors</span></h4><ul class="navbar-dropdown-subitem-wrapper"><!--[--><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/AGI/Contributors.html" aria-label="AGI"><!--[--><!--[--><!--]--> AGI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Generative-Art/Contributors.html" aria-label="Generative Art"><!--[--><!--[--><!--]--> Generative Art <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HCI/Contributors.html" aria-label="HCI"><!--[--><!--[--><!--]--> HCI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/HMI/Contributors.html" aria-label="HMI"><!--[--><!--[--><!--]--> HMI <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Human-Factor/Contributors.html" aria-label="Human Factor"><!--[--><!--[--><!--]--> Human Factor <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Intelligent-Device/Contributors.html" aria-label="Intelligent Device"><!--[--><!--[--><!--]--> Intelligent Device <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Interactive-Installations/Contributors.html" aria-label="Interactive Installations"><!--[--><!--[--><!--]--> Interactive Installations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Robot/Contributors.html" aria-label="Robot"><!--[--><!--[--><!--]--> Robot <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/Spatial-Interaction/Contributors.html" aria-label="Spatial Interaction"><!--[--><!--[--><!--]--> Spatial Interaction <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-subitem"><a class="route-link" href="/posts/XR/Contributors.html" aria-label="XR"><!--[--><!--[--><!--]--> XR <!--[--><!--]--><!--]--></a></li><!--]--></ul><!--]--></li><!--]--></ul></div></div><div class="navbar-item"><a class="external-link" href="https://github.com/OpenDesignGlobal/OpenDesign" rel="noopener noreferrer" target="_blank" aria-label="Github"><!--[--><!--[--><!--]--> Github <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading">Stable Diffusion <!----></p><ul style="" class="sidebar-item-children"><!--[--><li><a class="route-link sidebar-item" href="#_2024-04-11" aria-label="2024-04-11"><!--[--><!--[--><!--]--> 2024-04-11 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-10" aria-label="2024-04-10"><!--[--><!--[--><!--]--> 2024-04-10 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-09" aria-label="2024-04-09"><!--[--><!--[--><!--]--> 2024-04-09 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-04-08" aria-label="2024-04-08"><!--[--><!--[--><!--]--> 2024-04-08 <!--[--><!--]--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><!--]--><div><h1 id="stable-diffusion" tabindex="-1"><a class="header-anchor" href="#stable-diffusion"><span>Stable Diffusion</span></a></h1><h2 id="_2024-04-11" tabindex="-1"><a class="header-anchor" href="#_2024-04-11"><span>2024-04-11</span></a></h2><h4 id="openbias-open-set-bias-detection-in-text-to-image-generative-models" tabindex="-1"><a class="header-anchor" href="#openbias-open-set-bias-detection-in-text-to-image-generative-models"><span>OpenBias: Open-set Bias Detection in Text-to-Image Generative Models</span></a></h4><p><strong>Authors</strong>: Moreno D&#39;Incà, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, Nicu Sebe</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07990v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07990v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previously proposed biases. We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before. Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement.</p><h4 id="controlnet-improving-conditional-controls-with-efficient-consistency-feedback" tabindex="-1"><a class="header-anchor" href="#controlnet-improving-conditional-controls-with-efficient-consistency-feedback"><span>ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback</span></a></h4><p><strong>Authors</strong>: Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, Chen Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07987v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07987v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions.</p><h4 id="view-selection-for-3d-captioning-via-diffusion-ranking" tabindex="-1"><a class="header-anchor" href="#view-selection-for-3d-captioning-via-diffusion-ranking"><span>View Selection for 3D Captioning via Diffusion Ranking</span></a></h4><p><strong>Authors</strong>: Tiange Luo, Justin Johnson, Honglak Lee</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07984v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07984v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Scalable annotation approaches are crucial for constructing extensive 3D-text datasets, facilitating a broader range of applications. However, existing methods sometimes lead to the generation of hallucinated captions, compromising caption quality. This paper explores the issue of hallucination in 3D object captioning, with a focus on Cap3D method, which renders 3D objects into 2D views for captioning using pre-trained models. We pinpoint a major challenge: certain rendered views of 3D objects are atypical, deviating from the training data of standard image captioning models and causing hallucinations. To tackle this, we present DiffuRank, a method that leverages a pre-trained text-to-3D model to assess the alignment between 3D objects and their 2D rendered views, where the view with high alignment closely represent the object&#39;s characteristics. By ranking all rendered views and feeding the top-ranked ones into GPT4-Vision, we enhance the accuracy and detail of captions, enabling the correction of 200k captions in the Cap3D dataset and extending it to 1 million captions across Objaverse and Objaverse-XL datasets. Additionally, we showcase the adaptability of DiffuRank by applying it to pre-trained text-to-image models for a Visual Question Answering task, where it outperforms the CLIP model.</p><h4 id="lyapunov-stable-neural-control-for-state-and-output-feedback-a-novel-formulation-for-efficient-synthesis-and-verification" tabindex="-1"><a class="header-anchor" href="#lyapunov-stable-neural-control-for-state-and-output-feedback-a-novel-formulation-for-efficient-synthesis-and-verification"><span>Lyapunov-stable Neural Control for State and Output Feedback: A Novel Formulation for Efficient Synthesis and Verification</span></a></h4><p><strong>Authors</strong>: Lujie Yang, Hongkai Dai, Zhouxing Shi, Cho-Jui Hsieh, Russ Tedrake, Huan Zhang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07956v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07956v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Learning-based neural network (NN) control policies have shown impressive empirical performance in a wide range of tasks in robotics and control. However, formal (Lyapunov) stability guarantees over the region-of-attraction (ROA) for NN controllers with nonlinear dynamical systems are challenging to obtain, and most existing approaches rely on expensive solvers such as sums-of-squares (SOS), mixed-integer programming (MIP), or satisfiability modulo theories (SMT). In this paper, we demonstrate a new framework for learning NN controllers together with Lyapunov certificates using fast empirical falsification and strategic regularizations. We propose a novel formulation that defines a larger verifiable region-of-attraction (ROA) than shown in the literature, and refines the conventional restrictive constraints on Lyapunov derivatives to focus only on certifiable ROAs. The Lyapunov condition is rigorously verified post-hoc using branch-and-bound with scalable linear bound propagation-based NN verification techniques. The approach is efficient and flexible, and the full training and verification procedure is accelerated on GPUs without relying on expensive solvers for SOS, MIP, nor SMT. The flexibility and efficiency of our framework allow us to demonstrate Lyapunov-stable output feedback control with synthesized NN-based controllers and NN-based observers with formal stability guarantees, for the first time in literature. Source code at https://github.com/Verified-Intelligence/Lyapunov_Stable_NN_Controllers.</p><h4 id="taming-stable-diffusion-for-text-to-360°-panorama-image-generation" tabindex="-1"><a class="header-anchor" href="#taming-stable-diffusion-for-text-to-360°-panorama-image-generation"><span>Taming Stable Diffusion for Text to 360° Panorama Image Generation</span></a></h4><p><strong>Authors</strong>: Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, Jianfei Cai</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07949v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07949v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts. Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt. We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at https://chengzhag.github.io/publication/panfusion.</p><h4 id="active-carpets-in-floating-viscous-films" tabindex="-1"><a class="header-anchor" href="#active-carpets-in-floating-viscous-films"><span>Active Carpets in floating viscous films</span></a></h4><p><strong>Authors</strong>: Felipe A. Barros, Hugo N. Ulloa, Gabriel Aguayo, Arnold J. T. M. Mathijssen, Francisca Guzmán-Lastra</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07856v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07856v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Earth&#39;s aquatic environments are inherently stratified layered systems where interfaces between layers serve as ecological niches for microbial swimmers, forming colonies known as Active Carpet (AC). Previous theoretical studies have explored the hydrodynamic fluctuations exerted by ACs in semi-infinite fluid media, demonstrating their capability to enhance thermal diffusion and mass transport in aquatic systems. Yet, little is understood about the fluid dynamics and impact of ACs residing in confined layered environments, like slicks floating on water bodies. In this study, we report novel solutions for the hydrodynamic fluctuations induced by ACs geometrically confined between a free surface and a fluid-fluid interface characterized by a jump in fluid viscosity. Combining theory and numerical experiments, we investigate the topology of the biogenic hydrodynamic fluctuations in a confined, thin fluid environment. We reveal that within this thin layer, ACs gives shape to three characteristic regions: Region I is the closest zone to the AC and the fluid-fluid interface, where hydrodynamic fluctuations are dominantly vertical; Region II is further up from the AC and is characterized by isotropic hydrodynamic fluctuations; Region III is the furthest region, near the free surface and is dominated by horizontal flow fluctuations. We demonstrate that the extent of these regions depends strongly on the degree of confinement, i.e. the layer thickness and the strength of the viscosity jump. Lastly, we show that confinement fosters the emergence of large-scale flow structures within the layer housing the ACs--not previously reported. Our findings shed light on the complex interplay between confinement and hydrodynamics in floating viscous film biological systems, providing valuable insights with implications spanning from ecological conservation to bio-inspired engineering.</p><h4 id="adaptive-hyperbolic-cross-space-mapped-jacobi-method-on-unbounded-domains-with-applications-to-solving-multidimensional-spatiotemporal-integrodifferential-equations" tabindex="-1"><a class="header-anchor" href="#adaptive-hyperbolic-cross-space-mapped-jacobi-method-on-unbounded-domains-with-applications-to-solving-multidimensional-spatiotemporal-integrodifferential-equations"><span>Adaptive Hyperbolic-cross-space Mapped Jacobi Method on Unbounded Domains with Applications to Solving Multidimensional Spatiotemporal Integrodifferential Equations</span></a></h4><p><strong>Authors</strong>: Yunhong Deng, Sihong Shao, Alex Mogilner, Mingtao Xia</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07844v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07844v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this paper, we develop a new adaptive hyperbolic-cross-space mapped Jacobi (AHMJ) method for solving multidimensional spatiotemporal integrodifferential equations in unbounded domains. By devising adaptive techniques for sparse mapped Jacobi spectral expansions defined in a hyperbolic cross space, our proposed AHMJ method can efficiently solve various spatiotemporal integrodifferential equations such as the anomalous diffusion model with reduced numbers of basis functions. Our analysis of the AHMJ method gives a uniform upper error bound for solving a class of spatiotemporal integrodifferential equations, leading to effective error control.</p><h4 id="the-cattaneo-christov-approximation-of-fourier-heat-conductive-compressible-fluids" tabindex="-1"><a class="header-anchor" href="#the-cattaneo-christov-approximation-of-fourier-heat-conductive-compressible-fluids"><span>The Cattaneo-Christov approximation of Fourier heat-conductive compressible fluids</span></a></h4><p><strong>Authors</strong>: Timothée Crin-Barat, Shuichi Kawashima, Jiang Xu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07809v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07809v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We investigate the Navier-Stokes-Cattaneo-Christov (NSC) system in $\mathbb{R}^d$ ($d\geq3$), a model of heat-conductive compressible flows serving as a finite speed of propagation approximation of the Navier-Stokes-Fourier (NSF) system. Due to the presence of Oldroyd&#39;s upper-convected derivatives, the system (NSC) exhibits a \textit{lack of hyperbolicity} which makes it challenging to establish its well-posedness, especially in multi-dimensional contexts. In this paper, within a critical regularity functional framework, we prove the global-in-time well-posedness of (NSC) for initial data that are small perturbations of constant equilibria, uniformly with respect to the approximation parameter $\varepsilon&gt;0$. Then, building upon this result, we obtain the sharp large-time asymptotic behaviour of (NSC) and, for all time $t&gt;0$, we derive quantitative error estimates between the solutions of (NSC) and (NSF). To the best of our knowledge, our work provides the first strong convergence result for this relaxation procedure in the three-dimensional setting and for ill-prepared data. The (NSC) system is partially dissipative and incorporates both partial diffusion and partial damping mechanisms. To address these aspects and ensure the large-time stability of the solutions, we construct localized-in-frequency perturbed energy functionals based on the hypocoercivity theory. More precisely, our analysis relies on partitioning the frequency space into \textit{three} distinct regimes: low, medium and high frequencies. Within each frequency regime, we introduce effective unknowns and Lyapunov functionals, revealing the spectrally expected dissipative structures.</p><h4 id="consistencydet-robust-object-detector-with-denoising-paradigm-of-consistency-model" tabindex="-1"><a class="header-anchor" href="#consistencydet-robust-object-detector-with-denoising-paradigm-of-consistency-model"><span>ConsistencyDet: Robust Object Detector with Denoising Paradigm of Consistency Model</span></a></h4><p><strong>Authors</strong>: Lifan Jiang, Zhihui Wang, Changmiao Wang, Ming Li, Jiaxu Leng, Xindong Wu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07773v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07773v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Object detection, a quintessential task in the realm of perceptual computing, can be tackled using a generative methodology. In the present study, we introduce a novel framework designed to articulate object detection as a denoising diffusion process, which operates on perturbed bounding boxes of annotated entities. This framework, termed ConsistencyDet, leverages an innovative denoising concept known as the Consistency Model. The hallmark of this model is its self-consistency feature, which empowers the model to map distorted information from any temporal stage back to its pristine state, thereby realizing a ``one-step denoising&#39;&#39; mechanism. Such an attribute markedly elevates the operational efficiency of the model, setting it apart from the conventional Diffusion Model. Throughout the training phase, ConsistencyDet initiates the diffusion sequence with noise-infused boxes derived from the ground-truth annotations and conditions the model to perform the denoising task. Subsequently, in the inference stage, the model employs a denoising sampling strategy that commences with bounding boxes randomly sampled from a normal distribution. Through iterative refinement, the model transforms an assortment of arbitrarily generated boxes into the definitive detections. Comprehensive evaluations employing standard benchmarks, such as MS-COCO and LVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in performance metrics.</p><h4 id="an-overview-of-diffusion-models-applications-guided-generation-statistical-rates-and-optimization" tabindex="-1"><a class="header-anchor" href="#an-overview-of-diffusion-models-applications-guided-generation-statistical-rates-and-optimization"><span>An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization</span></a></h4><p><strong>Authors</strong>: Minshuo Chen, Song Mei, Jianqing Fan, Mengdi Wang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07771v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07771v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Diffusion models, a powerful and universal generative AI technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls. Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models.</p><h4 id="joint-conditional-diffusion-model-for-image-restoration-with-mixed-degradations" tabindex="-1"><a class="header-anchor" href="#joint-conditional-diffusion-model-for-image-restoration-with-mixed-degradations"><span>Joint Conditional Diffusion Model for Image Restoration with Mixed Degradations</span></a></h4><p><strong>Authors</strong>: Yufeng Yue, Meng Yu, Luojie Yang, Yi Yang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07770v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07770v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Image restoration is rather challenging in adverse weather conditions, especially when multiple degradations occur simultaneously. Blind image decomposition was proposed to tackle this issue, however, its effectiveness heavily relies on the accurate estimation of each component. Although diffusion-based models exhibit strong generative abilities in image restoration tasks, they may generate irrelevant contents when the degraded images are severely corrupted. To address these issues, we leverage physical constraints to guide the whole restoration process, where a mixed degradation model based on atmosphere scattering model is constructed. Then we formulate our Joint Conditional Diffusion Model (JCDM) by incorporating the degraded image and degradation mask to provide precise guidance. To achieve better color and detail recovery results, we further integrate a refinement network to reconstruct the restored image, where Uncertainty Estimation Block (UEB) is employed to enhance the features. Extensive experiments performed on both multi-weather and weather-specific datasets demonstrate the superiority of our method over state-of-the-art competing methods.</p><h4 id="generating-synthetic-satellite-imagery-with-deep-learning-text-to-image-models-technical-challenges-and-implications-for-monitoring-and-verification" tabindex="-1"><a class="header-anchor" href="#generating-synthetic-satellite-imagery-with-deep-learning-text-to-image-models-technical-challenges-and-implications-for-monitoring-and-verification"><span>Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification</span></a></h4><p><strong>Authors</strong>: Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07754v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07754v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.</p><h4 id="diffusing-in-someone-else-s-shoes-robotic-perspective-taking-with-diffusion" tabindex="-1"><a class="header-anchor" href="#diffusing-in-someone-else-s-shoes-robotic-perspective-taking-with-diffusion"><span>Diffusing in Someone Else&#39;s Shoes: Robotic Perspective Taking with Diffusion</span></a></h4><p><strong>Authors</strong>: Josua Spisak, Matthias Kerzel, Stefan Wermter</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07735v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07735v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Humanoid robots can benefit from their similarity to the human shape by learning from humans. When humans teach other humans how to perform actions, they often demonstrate the actions and the learning human can try to imitate the demonstration. Being able to mentally transfer from a demonstration seen from a third-person perspective to how it should look from a first-person perspective is fundamental for this ability in humans. As this is a challenging task, it is often simplified for robots by creating a demonstration in the first-person perspective. Creating these demonstrations requires more effort but allows for an easier imitation. We introduce a novel diffusion model aimed at enabling the robot to directly learn from the third-person demonstrations. Our model is capable of learning and generating the first-person perspective from the third-person perspective by translating the size and rotations of objects and the environment between two perspectives. This allows us to utilise the benefits of easy-to-produce third-person demonstrations and easy-to-imitate first-person demonstrations. The model can either represent the first-person perspective in an RGB image or calculate the joint values. Our approach significantly outperforms other image-to-image models in this task.</p><h4 id="log-t-frac-2-3-superdiffusivity-for-the-2d-stochastic-burgers-equation" tabindex="-1"><a class="header-anchor" href="#log-t-frac-2-3-superdiffusivity-for-the-2d-stochastic-burgers-equation"><span>$(\log t)^\frac{2}{3}$-superdiffusivity for the 2d stochastic Burgers equation</span></a></h4><p><strong>Authors</strong>: Damiano De Gaspari, Levi Haunschmid-Sibitz</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07728v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07728v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The Stochastic Burgers equation was introduced in [H. van Beijeren, R. Kutner and H. Spohn, Excess noise for driven diffusive systems, PRL, 1985] as a continuous approximation of the fluctuations of the asymmetric simple exclusion process. It is formally given by $$\partial_t\eta =\frac{1}{2}\Delta\eta+ \mathfrak w\cdot\nabla(\eta^2) + \nabla\cdot\xi,$$ where $\xi$ is $d$-dimensional space time white noise and $\mathfrak w$ is a fixed non-zero vector. In the critical dimension $d=2$ at stationarity, we show that this system exhibits superdiffusve behaviour: more specifically, its bulk diffusion coefficient behaves like $(\log t)^\frac23$, in a Tauberian sense, up to $\log\log\log t$ corrections. This confirms a prediction made in the physics literature and complements [G. Cannizzarro, M. Gubinelli, F. Toninelli, Gaussian Fluctuations for the stochastic Burgers equation in dimension $d\geq 2$, CMP, 2024], where the same equation was studied in the weak-coupling regime. Furthermore this model can be seen as a continuous analogue to [H.T. Yau, $(\log t)^\frac{2}{3}$ law of the two dimensional asymmetric simple exclusion process, Annals of Mathematics, 2004].</p><h4 id="applying-guidance-in-a-limited-interval-improves-sample-and-distribution-quality-in-diffusion-models" tabindex="-1"><a class="header-anchor" href="#applying-guidance-in-a-limited-interval-improves-sample-and-distribution-quality-in-diffusion-models"><span>Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models</span></a></h4><p><strong>Authors</strong>: Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07724v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07724v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance.</p><h4 id="the-classical-quantum-hybrid-canonical-dynamics-and-its-difficulties-with-special-and-general-relativity" tabindex="-1"><a class="header-anchor" href="#the-classical-quantum-hybrid-canonical-dynamics-and-its-difficulties-with-special-and-general-relativity"><span>The classical-quantum hybrid canonical dynamics and its difficulties with special and general relativity</span></a></h4><p><strong>Authors</strong>: Lajos Diósi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07723v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07723v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We discuss the Hamiltonian hybrid coupling between a classical and a quantum subsystem. If applicable to classical gravity coupled to quantized matter, this hybrid theory might realize a captivating `postquantum&#39; alternative to full quantum-gravity. We summarize the nonrelativistic hybrid dynamics in improved formalism adequate to Hamiltonian systems. The mandatory decoherence and diffusion terms become divergent in special and general relativistic extensions. It is not yet known if any renormalization method might reconcile Markovian decoherence and diffusion with relativity. Postquantum gravity could previously only be realized in the Newtonian approximation. We argue that pending problems of the recently proposed general relativistic postquantum theory will not be solved if Markovian diffusion/decoherence are truly incompatible with relativity.</p><h4 id="two-liquid-states-of-distinguishable-helium-4-the-existence-of-another-non-superfluid-frozen-by-heating" tabindex="-1"><a class="header-anchor" href="#two-liquid-states-of-distinguishable-helium-4-the-existence-of-another-non-superfluid-frozen-by-heating"><span>Two liquid states of distinguishable helium-4: the existence of another non-superfluid frozen by heating</span></a></h4><p><strong>Authors</strong>: Momoko Tsujimoto, Kenichi Kinugawa</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07716v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07716v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We demonstrate that there can exist two liquid states in distinguishable helium-4 ($^4$He) obeying Boltzmann statistics. This is an indication of quantum liquid polyamorphism induced by nuclear quantum effect. For 0.08-3.3 K and 1-500 bar, we extensively conducted the isothermal-isobaric path integral centroid molecular dynamics simulations to explore not only possible states and state diagram but the state characteristics. The distinguishable $^4$He below 25 bar does not freeze down to 0.08 K even though it includes no Bosonic exchange effect and therefore no Bose condensation. One liquid state, low quantum-dispersion liquid (LQDL), is nearly identical to normal liquid He-I of real $^4$He. The other is high quantum-dispersion liquid (HQDL) consisting of atoms with longer quantum wavelength. This is another non-superfluid existing below 0.5 K or the temperatures of LQDL. The HQDL is also a low-entropy and fragile liquid to exhibit, unlike conventional liquids, rather gas-like relaxation of velocity autocorrelation function, while there the atoms diffuse without noticeable contribution from quantum tunneling. The LQDL-HQDL transition is not a thermodynamic phase transition but a continuous crossover accompanied by the change of the expansion factor of quantum wavelength. Freezing of HQDL into the low quantum-dispersion amorphous solid occurs by heating from 0.2 to 0.3 K at 40-50 bar, while this $P$-$T$ condition coincides with the Kim-Chan normal-supersolid phase boundary of real $^4$He. It is suggested that HQDL has relevance to the non-superfluid states of confined subnano-scale $^4$He systems which are semi-Boltzmann-like owing to the suppression of Bosonic correlation.</p><h4 id="optimal-run-and-tumble-in-slit-like-confinement" tabindex="-1"><a class="header-anchor" href="#optimal-run-and-tumble-in-slit-like-confinement"><span>Optimal run-and-tumble in slit-like confinement</span></a></h4><p><strong>Authors</strong>: T. Pietrangeli, C. Ybert, C. Cottin-Bizonne, F. Detcheverry</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07680v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07680v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Run-and-tumble is a basic model of persistent motion and a motility strategy widespread in micro-organisms and individual cells. In many natural settings, movement occurs in the presence of confinement. While accumulation at the surface has been extensively studied, the transport parallel to the boundary has received less attention. We consider a run-and-tumble particle confined inside a slit, where motion in the bulk alternates with intermittent sojourns at the wall. We first propose a discrete-direction model that is fully tractable and obtain the exact diffusion coefficient characterizing the long-time exploration of the slit. We then use numerical simulations to show that with an adequate choice of parameters, our analytical prediction provides a useful approximation for the diffusion coefficient of run-and-tumble with continuous direction. Finally, we identify the conditions that maximize diffusion within the slit and discuss the optimal mean run time. For swimming bacteria, we find that the optimum is typically reached when the mean run length is comparable to the confinement size.</p><h4 id="diffusion-probabilistic-multi-cue-level-set-for-reducing-edge-uncertainty-in-pancreas-segmentation" tabindex="-1"><a class="header-anchor" href="#diffusion-probabilistic-multi-cue-level-set-for-reducing-edge-uncertainty-in-pancreas-segmentation"><span>Diffusion Probabilistic Multi-cue Level Set for Reducing Edge Uncertainty in Pancreas Segmentation</span></a></h4><p><strong>Authors</strong>: Yue Gou, Yuming Xing, Shengzhu Shi, Zhichang Guo</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07620v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07620v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Accurately segmenting the pancreas remains a huge challenge. Traditional methods encounter difficulties in semantic localization due to the small volume and distorted structure of the pancreas, while deep learning methods encounter challenges in obtaining accurate edges because of low contrast and organ overlapping. To overcome these issues, we propose a multi-cue level set method based on the diffusion probabilistic model, namely Diff-mcs. Our method adopts a coarse-to-fine segmentation strategy. We use the diffusion probabilistic model in the coarse segmentation stage, with the obtained probability distribution serving as both the initial localization and prior cues for the level set method. In the fine segmentation stage, we combine the prior cues with grayscale cues and texture cues to refine the edge by maximizing the difference between probability distributions of the cues inside and outside the level set curve. The method is validated on three public datasets and achieves state-of-the-art performance, which can obtain more accurate segmentation results with lower uncertainty segmentation edges. In addition, we conduct ablation studies and uncertainty analysis to verify that the diffusion probability model provides a more appropriate initialization for the level set method. Furthermore, when combined with multiple cues, the level set method can better obtain edges and improve the overall accuracy. Our code is available at https://github.com/GOUYUEE/Diff-mcs.</p><h4 id="optimal-state-equation-for-the-control-of-a-diffusion-with-two-distinct-dynamics" tabindex="-1"><a class="header-anchor" href="#optimal-state-equation-for-the-control-of-a-diffusion-with-two-distinct-dynamics"><span>Optimal State Equation for the Control of a Diffusion with Two Distinct Dynamics</span></a></h4><p><strong>Authors</strong>: Zengjing Chen, Panyu Wu, Xiaowen Zhou</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07618v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07618v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We consider a class of stochastic control problems which has been widely used in optimal foraging theory. The state processes have two distinct dynamics, characterized by two pairs of drift and diffusion coefficients, depending on whether it takes values bigger or smaller than a threshold value. Adopting a perturbation type approach, we find an expression for potential measure of the optimal state process. We then obtain an expression for the transition density of the optimal state process by inverting the associated Laplace transform. Properties including the stationary distribution of the optimal state process are discussed. Finally, the expression of the value function is given for this class stochastic control problems.</p><h4 id="implicit-and-explicit-language-guidance-for-diffusion-based-visual-perception" tabindex="-1"><a class="header-anchor" href="#implicit-and-explicit-language-guidance-for-diffusion-based-visual-perception"><span>Implicit and Explicit Language Guidance for Diffusion-based Visual Perception</span></a></h4><p><strong>Authors</strong>: Hefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07600v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07600v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-image diffusion models have shown powerful ability on conditional image synthesis. With large-scale vision-language pre-training, diffusion models are able to generate high-quality images with rich texture and reasonable structure under different text prompts. However, it is an open problem to adapt the pre-trained diffusion model for visual perception. In this paper, we propose an implicit and explicit language guidance framework for diffusion-based perception, named IEDP. Our IEDP comprises of an implicit language guidance branch and an explicit language guidance branch. The implicit branch employs frozen CLIP image encoder to directly generate implicit text embeddings that are fed to diffusion model, without using explicit text prompts. The explicit branch utilizes the ground-truth labels of corresponding images as text prompts to condition feature extraction of diffusion model. During training, we jointly train diffusion model by sharing the model weights of these two branches. As a result, implicit and explicit branches can jointly guide feature learning. During inference, we only employ implicit branch for final prediction, which does not require any ground-truth labels. Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation. Our IEDP achieves promising performance on both tasks. For semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K validation set, which outperforms the baseline method VPD by 2.2%. For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 10.2%.</p><h4 id="diffusion-posterior-sampling-for-simulation-based-inference-in-tall-data-settings" tabindex="-1"><a class="header-anchor" href="#diffusion-posterior-sampling-for-simulation-based-inference-in-tall-data-settings"><span>Diffusion posterior sampling for simulation-based inference in tall data settings</span></a></h4><p><strong>Authors</strong>: Julia Linhart, Gabriel Victorino Cardoso, Alexandre Gramfort, Sylvain Le Corff, Pedro L. C. Rodrigues</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07593v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07593v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Determining which parameters of a non-linear model could best describe a set of experimental data is a fundamental problem in science and it has gained much traction lately with the rise of complex large-scale simulators (a.k.a. black-box simulators). The likelihood of such models is typically intractable, which is why classical MCMC methods can not be used. Simulation-based inference (SBI) stands out in this context by only requiring a dataset of simulations to train deep generative models capable of approximating the posterior distribution that relates input parameters to a given observation. In this work, we consider a tall data extension in which multiple observations are available and one wishes to leverage their shared information to better infer the parameters of the model. The method we propose is built upon recent developments from the flourishing score-based diffusion literature and allows us to estimate the tall data posterior distribution simply using information from the score network trained on individual observations. We compare our method to recently proposed competing approaches on various numerical experiments and demonstrate its superiority in terms of numerical stability and computational cost.</p><h4 id="objblur-a-curriculum-learning-approach-with-progressive-object-level-blurring-for-improved-layout-to-image-generation" tabindex="-1"><a class="header-anchor" href="#objblur-a-curriculum-learning-approach-with-progressive-object-level-blurring-for-improved-layout-to-image-generation"><span>ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring for Improved Layout-to-Image Generation</span></a></h4><p><strong>Authors</strong>: Stanislav Frolov, Brian B. Moser, Sebastian Palacio, Andreas Dengel</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07564v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07564v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We present ObjBlur, a novel curriculum learning approach to improve layout-to-image generation models, where the task is to produce realistic images from layouts composed of boxes and labels. Our method is based on progressive object-level blurring, which effectively stabilizes training and enhances the quality of generated images. This curriculum learning strategy systematically applies varying degrees of blurring to individual objects or the background during training, starting from strong blurring to progressively cleaner images. Our findings reveal that this approach yields significant performance improvements, stabilized training, smoother convergence, and reduced variance between multiple runs. Moreover, our technique demonstrates its versatility by being compatible with generative adversarial networks and diffusion models, underlining its applicability across various generative modeling paradigms. With ObjBlur, we reach new state-of-the-art results on the complex COCO and Visual Genome datasets.</p><h4 id="effects-of-phase-separation-on-extinction-times-in-population-models" tabindex="-1"><a class="header-anchor" href="#effects-of-phase-separation-on-extinction-times-in-population-models"><span>Effects of phase separation on extinction times in population models</span></a></h4><p><strong>Authors</strong>: Janik Schüttler, Robert L. Jack, Michael E. Cates</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07563v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07563v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We study the effect of phase separating diffusive dynamics on the mean time to extinction in several reaction-diffusion models with slow reactions. We consider a continuum theory similar to model AB, and a simple model where individual particles on two sites undergo on-site reactions and hopping between the sites. In the slow-reaction limit, we project the models&#39; dynamics onto suitable one-dimensional reaction coordinates, which allows derivation of quasi-equilibrium effective free energies. For weak noise, this enables characterisation of the mean time to extinction. This time can be enhanced or suppressed by the addition of phase separation, compared with homogeneous reference cases. We also discuss how Allee effects can be affected by phase separation.</p><h4 id="cat-contrastive-adapter-training-for-personalized-image-generation" tabindex="-1"><a class="header-anchor" href="#cat-contrastive-adapter-training-for-personalized-image-generation"><span>CAT: Contrastive Adapter Training for Personalized Image Generation</span></a></h4><p><strong>Authors</strong>: Jae Wan Park, Sang Hyun Park, Jun Young Koh, Junha Lee, Min Song</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07554v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07554v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The emergence of various adapters, including Low-Rank Adaptation (LoRA) applied from the field of natural language processing, has allowed diffusion models to personalize image generation at a low cost. However, due to the various challenges including limited datasets and shortage of regularization and computation resources, adapter training often results in unsatisfactory outcomes, leading to the corruption of the backbone model&#39;s prior knowledge. One of the well known phenomena is the loss of diversity in object generation, especially within the same class which leads to generating almost identical objects with minor variations. This poses challenges in generation capabilities. To solve this issue, we present Contrastive Adapter Training (CAT), a simple yet effective strategy to enhance adapter training through the application of CAT loss. Our approach facilitates the preservation of the base model&#39;s original knowledge when the model initiates adapters. Furthermore, we introduce the Knowledge Preservation Score (KPS) to evaluate CAT&#39;s ability to keep the former information. We qualitatively and quantitatively compare CAT&#39;s improvement. Finally, we mention the possibility of CAT in the aspects of multi-concept adapter and optimization.</p><h4 id="reduced-dimensional-modelling-for-nonlinear-convection-dominated-flow-in-cylindric-domains" tabindex="-1"><a class="header-anchor" href="#reduced-dimensional-modelling-for-nonlinear-convection-dominated-flow-in-cylindric-domains"><span>Reduced-dimensional modelling for nonlinear convection-dominated flow in cylindric domains</span></a></h4><p><strong>Authors</strong>: Taras Mel&#39;nyk, Christian Rohde</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07538v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07538v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The aim of the paper is to construct and justify asymptotic approximations for solutions to quasilinear convection-diffusion problems with a predominance of nonlinear convective flow in a thin cylinder, where an inhomogeneous nonlinear Robin-type boundary condition involving convective and diffusive fluxes is imposed on the lateral surface. The limit problem for vanishing diffusion and the cylinder shrinking to an interval is a nonlinear first-order conservation law. For a time span that allows for a classical solution of this limit problem corresponding uniform pointwise and energy estimates are proven. They provide precise model error estimates with respect to the small parameter that controls the double viscosity-geometric limit. In addition, other problems with more higher P&#39;eclet numbers are also considered.</p><h4 id="wettability-dependent-dissolution-dynamics-of-oxygen-bubbles-on-ti64-substrates" tabindex="-1"><a class="header-anchor" href="#wettability-dependent-dissolution-dynamics-of-oxygen-bubbles-on-ti64-substrates"><span>Wettability-dependent dissolution dynamics of oxygen bubbles on Ti64 substrates</span></a></h4><p><strong>Authors</strong>: Hongfei Dai, Xuegeng Yang, Karin Schwarzenberger, Julian Heinrich, Kerstin Eckert</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07483v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07483v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this study, the dissolution of a single oxygen bubble on a solid surface, here Titianium alloy Ti64, in ultrapure water with different oxygen undersaturation levels is investigated. For that purpose, a combination of shadowgraph technique and planar laser-induced fluorescence is used to measure simultaneously the changes in bubble geometry and in the dissolved oxygen concentration around the bubble. Two different wettabilities of the Ti64 surface are adjusted by using plasma-enhanced chemical vapour deposition. The dissolution process on the solid surface involves two distinct phases, namely bouncing of the oxygen bubble at the Ti64 surface and the subsequent dissolution of the bubble, primarily by diffusion. By investigating the features of oxygen bubbles bouncing, it was found that the boundary layer of dissolved oxygen surrounding the bubble surface is redistributed by the vortices emerging during bouncing. This establishes the initial conditions for the subsequent second dissolution phase of the oxygen bubbles on the Ti64 surfaces. In this phase, the mass transfer of O2 proceeds non-homogenously across the bubble surface, leading to an oxygen accumulation close to the Ti64 surface. We further show that the main factor influencing the differences in the dynamics of O2 bubble dissolution is the variation in the surface area of the bubbles available for mass transfer, which is determined by the substrate wettability. As a result, dissolution proceeds faster at the hydrophilic Ti64 surface due to the smaller contact angle, which provokes a larger surface area.</p><h4 id="neutral-current-background-induced-by-atmospheric-neutrinos-at-large-liquid-scintillator-detectors-iii-quantitative-calculations-for-reactor-neutrinos" tabindex="-1"><a class="header-anchor" href="#neutral-current-background-induced-by-atmospheric-neutrinos-at-large-liquid-scintillator-detectors-iii-quantitative-calculations-for-reactor-neutrinos"><span>Neutral-current background induced by atmospheric neutrinos at large liquid-scintillator detectors: III. Quantitative calculations for reactor neutrinos</span></a></h4><p><strong>Authors</strong>: Jie Cheng, Min Li, Yu-Feng Li, Gao-Song Li, Hao-Qi Lu, Liang-Jian Wen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07429v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07429v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Atmospheric neutrinos contribute significantly to irreducible backgrounds through their neutral-current (NC) interactions with $^{12}$C nuclei in liquid-scintillator detectors, impacting diffuse supernova neutrino background, nucleon decay, and reactor neutrinos. This paper extends our prior work by systematically studying the NC backgrounds towards the MeV region of reactor neutrinos. We employ contemporary neutrino generator models from GENIE and NuWro for calculations, with a focus on predicting NC background in experimental searches for inverse-beta-decay signals below 100 MeV visible energy. We estimate the systematic uncertainty to our estimation of the NC background using various data-driven neutrino generator models, addressing factors such as the initial neutrino-nucleon NC interaction, the nuclear model, the final-state interaction model, the nucleus deexcitation, and the secondary interaction on final-state particles.</p><h4 id="statistical-analysis-of-high-frequency-whistler-waves-at-earth-s-bow-shock-further-support-for-stochastic-shock-drift-acceleration" tabindex="-1"><a class="header-anchor" href="#statistical-analysis-of-high-frequency-whistler-waves-at-earth-s-bow-shock-further-support-for-stochastic-shock-drift-acceleration"><span>Statistical Analysis of High-frequency Whistler Waves at Earth&#39;s Bow Shock: Further Support for Stochastic Shock Drift Acceleration</span></a></h4><p><strong>Authors</strong>: Takanobu Amano, Miki Masuda, Mitsuo Oka, Naritoshi Kitamura, Olivier Le Contel, Daniel J. Gershman</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07404v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07404v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We statistically investigate high-frequency whistler waves (with frequencies higher than $\sim 10$ % of the local elect ron cyclotron frequency) at Earth&#39;s bow shock using Magnetospheric Multi-Scale (MMS) spacecraft observations. We focus specifically on the wave power within the shock transition layer, where we expect electron acceleration via stochastic sh ock drift acceleration (SSDA) to occur associated with efficient pitch-angle scattering by whistler waves. We find that the wave power is positively correlated with both the Alfv&#39;en Mach number in the normal incidence frame $M_{\rm A}$ and in the de Hoffmann-Teller frame $M_{\rm A}/\cos \theta_{Bn}$. The empirical relation with $M_{\rm A}/\cos \theta_{Bn}$ is compared with the theory of SSDA that predicts a threshold wave power proportional to $(M_{\rm A}/\cos \theta_{Bn})^{-2}$. The result suggests that the wave power exceeds the theoretical threshold for $M_{\rm A} / \cos \theta_{Bn} \gtrsim 30-60$, beyond which efficient electron acceleration is expected. This aligns very well with previous statistical analysis of electron acceleration at Earth&#39;s bow shock (M. Oka, G eophys.~Res.~Lett., 33, 5, 2006). Therefore, we consider that this study provides further support for SSDA as the mechanism of electron acceleration at Earth&#39;s bow shock. At higher-Mach-number astrophysical shocks, SSDA will be able to inject electrons into the diffusive shock acceleration process for subsequent acceleration to cosmic-ray energies.</p><h2 id="_2024-04-10" tabindex="-1"><a class="header-anchor" href="#_2024-04-10"><span>2024-04-10</span></a></h2><h4 id="object-conditioned-energy-based-attention-map-alignment-in-text-to-image-diffusion-models" tabindex="-1"><a class="header-anchor" href="#object-conditioned-energy-based-attention-map-alignment-in-text-to-image-diffusion-models"><span>Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models</span></a></h4><p><strong>Authors</strong>: Yasi Zhang, Peiyu Yu, Ying Nian Wu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07389v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07389v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-image diffusion models have shown great success in generating high-quality text-guided images. Yet, these models may still fail to semantically align generated images with the provided text prompts, leading to problems like incorrect attribute binding and/or catastrophic object neglect. Given the pervasive object-oriented structure underlying text prompts, we introduce a novel object-conditioned Energy-Based Attention Map Alignment (EBAMA) method to address the aforementioned problems. We show that an object-centric attribute binding loss naturally emerges by approximately maximizing the log-likelihood of a $z$-parameterized energy-based model with the help of the negative sampling technique. We further propose an object-centric intensity regularizer to prevent excessive shifts of objects attention towards their attributes. Extensive qualitative and quantitative experiments, including human evaluation, on several challenging benchmarks demonstrate the superior performance of our method over previous strong counterparts. With better aligned attention maps, our approach shows great promise in further enhancing the text-controlled image editing ability of diffusion models.</p><h4 id="lock-key-microfluidics-simulating-nematic-colloid-advection-along-wavy-walled-channels" tabindex="-1"><a class="header-anchor" href="#lock-key-microfluidics-simulating-nematic-colloid-advection-along-wavy-walled-channels"><span>Lock-Key Microfluidics: Simulating Nematic Colloid Advection along Wavy-Walled Channels</span></a></h4><p><strong>Authors</strong>: Karolina Wamsler, Louise C. Head, Tyler N. Shendruk</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07367v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07367v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Liquid crystalline media mediate interactions between suspended particles and confining geometries, which not only has potential to guide patterning and bottom-up colloidal assembly, but can also control colloidal migration in microfluidic devices. However, simulating such dynamics is challenging because nemato-elasticity, diffusivity and hydrodynamic interactions must all be accounted for within complex boundaries. We model the advection of colloids dispersed in flowing and fluctuating nematic fluids confined within 2D wavy channels. A lock-key mechanism between colloids and troughs is found to be stronger for planar anchoring compared to homeotropic anchoring due to the relative location of the colloid-associated defects. Sufficiently large amplitudes result in stick-slip trajectories and even permanent locking of colloids in place. These results demonstrate that wavy walls not only have potential to direct colloids to specific docking sites but also to control site-specific resting duration and intermittent elution.</p><h4 id="jwst-miri-detection-of-suprathermal-oh-rotational-emissions-probing-the-dissociation-of-the-water-by-lyman-alpha-photons-near-the-protostar-hops-370" tabindex="-1"><a class="header-anchor" href="#jwst-miri-detection-of-suprathermal-oh-rotational-emissions-probing-the-dissociation-of-the-water-by-lyman-alpha-photons-near-the-protostar-hops-370"><span>JWST/MIRI detection of suprathermal OH rotational emissions: probing the dissociation of the water by Lyman alpha photons near the protostar HOPS 370</span></a></h4><p><strong>Authors</strong>: David A. Neufeld, P. Manoj, Himanshu Tyagi, Mayank Narang, Dan M. Watson, S. Thomas Megeath, Ewine F. Van Dishoeck, Robert A. Gutermuth, Thomas Stanke, Yao-Lun Yang, Adam E. Rubinstein, Guillem Anglada, Henrik Beuther, Alessio Caratti o Garatti, Neal J. Evans II, Samuel Federman, William J. Fischer, Joel Green, Pamela Klaassen, Leslie W. Looney, Mayra Osorio, Pooneh Nazari, John J. Tobin, Lukasz Tychoniec, Scott Wolk</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07299v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07299v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Using the MIRI/MRS spectrometer on JWST, we have detected pure rotational, suprathermal OH emissions from the vicinity of the intermediate-mass protostar HOPS 370 (OMC2/FIR3). These emissions are observed from shocked knots in a jet/outflow, and originate in states of rotational quantum number as high as 46 that possess excitation energies as large as $E_U/k = 4.65 \times 10^4$ K. The relative strengths of the observed OH lines provide a powerful diagnostic of the ultraviolet radiation field in a heavily-extinguished region ($A_V \sim 10 - 20$) where direct UV observations are impossible. To high precision, the OH line strengths are consistent with a picture in which the suprathermal OH states are populated following the photodissociation of water in its $\tilde B - X$ band by ultraviolet radiation produced by fast ($\sim 80,\rm km,s^{-1}$) shocks along the jet. The observed dominance of emission from symmetric ($A^\prime$) OH states over that from antisymmetric ($A^{\prime\prime}$) states provides a distinctive signature of this particular population mechanism. Moreover, the variation of intensity with rotational quantum number suggests specifically that Ly$\alpha$ radiation is responsible for the photodissociation of water, an alternative model with photodissociation by a 10$^4$ K blackbody being disfavored at a high level of significance. Using measurements of the Br$\alpha$ flux to estimate the Ly$\alpha$ production rate, we find that $\sim 4%$ of the Ly$\alpha$ photons are absorbed by water. Combined with direct measurements of water emissions in the $\nu_2 = 1 -0$ band, the OH observations promise to provide key constraints on future models for the diffusion of Ly$\alpha$ photons in the vicinity of a shock front.</p><h4 id="solving-masked-jigsaw-puzzles-with-diffusion-vision-transformers" tabindex="-1"><a class="header-anchor" href="#solving-masked-jigsaw-puzzles-with-diffusion-vision-transformers"><span>Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers</span></a></h4><p><strong>Authors</strong>: Jinyang Liu, Wondmgezahu Teshome, Sandesh Ghimire, Mario Sznaier, Octavia Camps</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07292v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07292v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Solving image and video jigsaw puzzles poses the challenging task of rearranging image fragments or video frames from unordered sequences to restore meaningful images and video sequences. Existing approaches often hinge on discriminative models tasked with predicting either the absolute positions of puzzle elements or the permutation actions applied to the original data. Unfortunately, these methods face limitations in effectively solving puzzles with a large number of elements. In this paper, we propose JPDVT, an innovative approach that harnesses diffusion transformers to address this challenge. Specifically, we generate positional information for image patches or video frames, conditioned on their underlying visual content. This information is then employed to accurately assemble the puzzle pieces in their correct positions, even in scenarios involving missing pieces. Our method achieves state-of-the-art performance on several datasets.</p><h4 id="magnetically-driven-turbulence-in-the-inner-regions-of-protoplanetary-disks" tabindex="-1"><a class="header-anchor" href="#magnetically-driven-turbulence-in-the-inner-regions-of-protoplanetary-disks"><span>Magnetically Driven Turbulence in the Inner Regions of Protoplanetary Disks</span></a></h4><p><strong>Authors</strong>: David G. Rea, Jacob B. Simon, Daniel Carrera, Geoffroy Lesur, Wladimir Lyra, Debanjan Sengupta, Chao-Chin Yang, Andrew N. Youdin</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07265v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07265v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Given the important role turbulence plays in the settling and growth of dust grains in protoplanetary disks, it is crucial that we determine whether these disks are turbulent and to what extent. Protoplanetary disks are weakly ionized near the mid-plane, which has led to a paradigm in which largely laminar magnetic field structures prevail deeper in the disk, with angular momentum being transported via magnetically launched winds. Yet, there has been little exploration on the precise behavior of the gas within the bulk of the disk. We carry out 3D, local shearing box simulations that include all three low-ionization effects (Ohmic diffusion, ambipolar diffusion, and the Hall effect) to probe the nature of magnetically driven gas dynamics 1-30 AU from the central star. We find that gas turbulence can persist with a generous yet physically motivated ionization prescription (order unity Elsasser numbers). The gas velocity fluctuations range from 0.03-0.09 of the sound speed $c_s$ at the disk mid-plane to $\sim c_s$ near the disk surface, and are dependent on the initial magnetic field strength. However, the turbulent velocities do not appear to be strongly dependent on the field polarity, and thus appear to be insensitive to the Hall effect. The mid-plane turbulence has the potential to drive dust grains to collision velocities exceeding their fragmentation limit, and likely reduces the efficacy of particle clumping in the mid-plane, though it remains to be seen if this level of turbulence persists in disks with lower ionization levels.</p><h4 id="gooddrag-towards-good-practices-for-drag-editing-with-diffusion-models" tabindex="-1"><a class="header-anchor" href="#gooddrag-towards-good-practices-for-drag-editing-with-diffusion-models"><span>GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models</span></a></h4><p><strong>Authors</strong>: Zewei Zhang, Huan Liu, Jun Chen, Xiangyu Xu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07206v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07206v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing. Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising operations within the diffusion process, effectively improving the fidelity of the result. We also propose an information-preserving motion supervision operation that maintains the original features of the starting point for precise manipulation and artifact reduction. In addition, we contribute to the benchmarking of drag editing by introducing a new dataset, Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy Index and Gemini Score, utilizing Large Multimodal Models. Extensive experiments demonstrate that the proposed GoodDrag compares favorably against the state-of-the-art approaches both qualitatively and quantitatively. The project page is https://gooddrag.github.io.</p><h4 id="realmdreamer-text-driven-3d-scene-generation-with-inpainting-and-depth-diffusion" tabindex="-1"><a class="header-anchor" href="#realmdreamer-text-driven-3d-scene-generation-with-inpainting-and-depth-diffusion"><span>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion</span></a></h4><p><strong>Authors</strong>: Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07199v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07199v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image.</p><h4 id="instantmesh-efficient-3d-mesh-generation-from-a-single-image-with-sparse-view-large-reconstruction-models" tabindex="-1"><a class="header-anchor" href="#instantmesh-efficient-3d-mesh-generation-from-a-single-image-with-sparse-view-large-reconstruction-models"><span>InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models</span></a></h4><p><strong>Authors</strong>: Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, Ying Shan</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07191v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07191v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators.</p><h4 id="move-anything-with-layered-scene-diffusion" tabindex="-1"><a class="header-anchor" href="#move-anything-with-layered-scene-diffusion"><span>Move Anything with Layered Scene Diffusion</span></a></h4><p><strong>Authors</strong>: Jiawei Ren, Mengmeng Xu, Jui-Chieh Wu, Ziwei Liu, Tao Xiang, Antoine Toisoul</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07178v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07178v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Diffusion models generate images with an unprecedented level of quality, but how can we freely rearrange image layouts? Recent works generate controllable scenes via learning spatially disentangled latent codes, but these methods do not apply to diffusion models due to their fixed forward process. In this work, we propose SceneDiffusion to optimize a layered scene representation during the diffusion sampling process. Our key insight is that spatial disentanglement can be obtained by jointly denoising scene renderings at different spatial layouts. Our generated scenes support a wide range of spatial editing operations, including moving, resizing, cloning, and layer-wise appearance editing operations, including object restyling and replacing. Moreover, a scene can be generated conditioned on a reference image, thus enabling object moving for in-the-wild images. Notably, this approach is training-free, compatible with general text-to-image diffusion models, and responsive in less than a second.</p><h4 id="understanding-dynamics-in-coarse-grained-models-iv-connection-of-fine-grained-and-coarse-grained-dynamics-with-the-stokes-einstein-and-stokes-einstein-debye-relations" tabindex="-1"><a class="header-anchor" href="#understanding-dynamics-in-coarse-grained-models-iv-connection-of-fine-grained-and-coarse-grained-dynamics-with-the-stokes-einstein-and-stokes-einstein-debye-relations"><span>Understanding Dynamics in Coarse-Grained Models: IV. Connection of Fine-Grained and Coarse-Grained Dynamics with the Stokes-Einstein and Stokes-Einstein-Debye Relations</span></a></h4><p><strong>Authors</strong>: Jaehyeok Jin, Gregory A. Voth</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07156v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07156v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Applying an excess entropy scaling formalism to the coarse-grained (CG) dynamics of liquids, we discovered that missing rotational motions during the CG process are responsible for artificially accelerated CG dynamics. In the context of the dynamic representability between the fine-grained (FG) and CG dynamics, this work introduces the well-known Stokes-Einstein and Stokes-Einstein-Debye relations to unravel the rotational dynamics underlying FG trajectories, thereby allowing for an indirect evaluation of the effective rotations based only on the translational information at the reduced CG resolution. Since the representability issue in CG modeling limits a direct evaluation of the shear stress appearing in the Stokes-Einstein and Stokes-Einstein-Debye relations, we introduce a translational relaxation time as a proxy to employ these relations, and we demonstrate that these relations hold for the ambient conditions studied in our series of work. Additional theoretical links to our previous work are also established. First, we demonstrate that the effective hard sphere radius determined by the classical perturbation theory can approximate the complex hydrodynamic radius value reasonably well. Also, we present a simple derivation of an excess entropy scaling relationship for viscosity by estimating the elliptical integral of molecules. In turn, since the translational and rotational motions at the FG level are correlated to each other, we conclude that the &quot;entropy-free&quot; CG diffusion only depends on the shape of the reference molecule. Our results and analyses impart an alternative way of recovering the FG diffusion from the CG description by coupling the translational and rotational motions at the hydrodynamic level.</p><h4 id="adaptive-behavior-with-stable-synapses" tabindex="-1"><a class="header-anchor" href="#adaptive-behavior-with-stable-synapses"><span>Adaptive behavior with stable synapses</span></a></h4><p><strong>Authors</strong>: Cristiano Capone, Luca Falorsi, Maurizio Mattia</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07150v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07150v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Behavioral changes in animals and humans, as a consequence of an error or a verbal instruction, can be extremely rapid. Improvement in behavioral performances are usually associated in machine learning and reinforcement learning to synaptic plasticity, and, in general, to changes and optimization of network parameters. However, such rapid changes are not coherent with the timescales of synaptic plasticity, suggesting that the mechanism responsible for that could be a dynamical network reconfiguration. In the last few years, similar capabilities have been observed in transformers, foundational architecture in the field of machine learning that are widely used in applications such as natural language and image processing. Transformers are capable of in-context learning, the ability to adapt and acquire new information dynamically within the context of the task or environment they are currently engaged in, without the need for significant changes to their underlying parameters. Building upon the notion of something unique within transformers enabling the emergence of this property, we claim that it could also be supported by input segregation and dendritic amplification, features extensively observed in biological networks. We propose an architecture composed of gain-modulated recurrent networks that excels at in-context learning, showing abilities inaccessible to standard networks. We argue that such a framework can describe the psychometry of context-dependent tasks on humans and other species, solving the incoherence of plasticity timescales. When the context is changed, the network is dynamically reconfigured, and the predicted output undergoes dynamic updates until it aligns with the information embedded in the context.</p><h4 id="a-conservative-eulerian-finite-element-method-for-transport-and-diffusion-in-moving-domains" tabindex="-1"><a class="header-anchor" href="#a-conservative-eulerian-finite-element-method-for-transport-and-diffusion-in-moving-domains"><span>A conservative Eulerian finite element method for transport and diffusion in moving domains</span></a></h4><p><strong>Authors</strong>: Maxim Olshanskii, Henry von Wahl</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07130v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07130v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The paper introduces a finite element method for an Eulerian formulation of partial differential equations governing the transport and diffusion of a scalar quantity in a time-dependent domain. The method follows the idea from Lehrenfeld &amp; Olshanskii [ESAIM: M2AN, 53(2): 585-614, 2019] of a solution extension to realise the Eulearian time-stepping scheme. However, a reformulation of the partial differential equation is suggested to derive a scheme which conserves the quantity under consideration exactly on the discrete level. For the spatial discretisation, the paper considers an unfitted finite element method. Ghost-penalty stabilisation is used to release the discrete solution extension and gives a scheme robust against arbitrary intersections between the mesh and geometry interface. The stability is analysed for both first- and second-order backward differentiation formula versions of the scheme. Several numerical examples in two and three spatial dimensions are included to illustrate the potential of this method.</p><h4 id="open-reaction-diffusion-systems-bridging-probabilistic-theory-across-scales" tabindex="-1"><a class="header-anchor" href="#open-reaction-diffusion-systems-bridging-probabilistic-theory-across-scales"><span>Open reaction-diffusion systems: bridging probabilistic theory across scales</span></a></h4><p><strong>Authors</strong>: Mauricio J. del Razo</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07119v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07119v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Reaction-diffusion processes are the foundational model for a diverse range of complex systems, ranging from biochemical reactions to social agent-based phenomena. The underlying dynamics of these systems occur at the individual particle/agent level, and in realistic applications, they often display interaction with their environment through energy or material exchange with a reservoir. This requires intricate mathematical considerations, especially in the case of material exchange since the varying number of particles/agents results in ``on-the-fly&#39;&#39; modification of the system dimension. In this work, we first overview the probabilistic description of reaction-diffusion processes at the particle level, which readily handles varying numbers of particles. We then extend this model to consistently incorporate interactions with macroscopic material reservoirs. Based on the resulting expressions, we bridge the probabilistic description with macroscopic concentration-based descriptions for linear and nonlinear reaction-diffusion systems, as well as for an archetypal open reaction-diffusion system. This establishes a methodological workflow to bridge particle-based probabilistic descriptions with macroscopic concentration-based descriptions of reaction-diffusion in open settings, laying the foundations for a multiscale theoretical framework upon which to construct theory and simulation schemes that are consistent across scales.</p><h4 id="diffusion-based-inpainting-of-incomplete-euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-brownian-motion" tabindex="-1"><a class="header-anchor" href="#diffusion-based-inpainting-of-incomplete-euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-brownian-motion"><span>Diffusion-based inpainting of incomplete Euclidean distance matrices of trajectories generated by a fractional Brownian motion</span></a></h4><p><strong>Authors</strong>: Alexander Lobashev, Kirill Polovnikov</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07029v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07029v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Fractional Brownian trajectories (fBm) feature both randomness and strong scale-free correlations, challenging generative models to reproduce the intrinsic memory characterizing the underlying process. Here we test a diffusion probabilistic model on a specific dataset of corrupted images corresponding to incomplete Euclidean distance matrices of fBm at various memory exponents $H$. Our dataset implies uniqueness of the data imputation in the regime of low missing ratio, where the remaining partial graph is rigid, providing the ground truth for the inpainting. We find that the conditional diffusion generation stably reproduces the statistics of missing fBm-distributed distances for different values of $H$ exponent. Furthermore, while diffusion models have been recently shown to remember samples from the training database, we show that diffusion-based inpainting behaves qualitatively different from the database search with the increasing database size. Finally, we apply our fBm-trained diffusion model with $H=1/3$ for completion of chromosome distance matrices obtained in single-cell microscopy experiments, showing its superiority over the standard bioinformatics algorithms. Our source code is available on GitHub at https://github.com/alobashev/diffusion_fbm.</p><h4 id="on-the-conjugate-interface-conditions-and-galilean-invariance" tabindex="-1"><a class="header-anchor" href="#on-the-conjugate-interface-conditions-and-galilean-invariance"><span>On the conjugate interface conditions and Galilean invariance</span></a></h4><p><strong>Authors</strong>: Yang Hu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07025v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07025v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In the referred paper(&quot;H. Karani, C. Huber, Physical Review E, 91(2)(2015) 023304&quot;), a total heat flux continuity condition for conjugate heat transfer problems with moving interfaces was proposed. The authors asserted both conductive and advective heat fluxes are conserved simultaneously in their formulation. This condition had been cited by many subsequent studies. However, it is found that the total heat flux continuity condition violates Galilean invariance. The original diffusion heat flux continuity condition is reasonable for both stationary and moving interfaces.</p><h4 id="non-degenerate-one-time-pad-and-the-integrity-of-perfectly-secret-messages" tabindex="-1"><a class="header-anchor" href="#non-degenerate-one-time-pad-and-the-integrity-of-perfectly-secret-messages"><span>Non-Degenerate One-Time Pad and the integrity of perfectly secret messages</span></a></h4><p><strong>Authors</strong>: Alex Shafarenko</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07022v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07022v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We present a new construction of a One Time Pad (OTP) with inherent diffusive properties and a redundancy injection mechanism that benefits from them. The construction is based on interpreting the plaintext and key as members of a permutation group in the Lehmer code representation after conversion to factoradic. The so constructed OTP translates any perturbation of the ciphertext to an unpredictable, metrically large random perturbation of the plaintext. This allows us to provide unconditional integrity assurance without extra key material. The redundancy is injected using Foata&#39;s &quot;pun&quot;: the reading of the one-line representation as the cyclic one; we call this Pseudo Foata Injection. We obtain algorithms of quadratic complexity that implement both mechanisms.</p><h4 id="learned-finite-time-consensus-for-distributed-optimization" tabindex="-1"><a class="header-anchor" href="#learned-finite-time-consensus-for-distributed-optimization"><span>Learned Finite-Time Consensus for Distributed Optimization</span></a></h4><p><strong>Authors</strong>: Aaron Fainman, Stefan Vlaski</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.07018v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.07018v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Most algorithms for decentralized learning employ a consensus or diffusion mechanism to drive agents to a common solution of a global optimization problem. Generally this takes the form of linear averaging, at a rate of contraction determined by the mixing rate of the underlying network topology. For very sparse graphs this can yield a bottleneck, slowing down the convergence of the learning algorithm. We show that a sequence of matrices achieving finite-time consensus can be learned for unknown graph topologies in a decentralized manner by solving a constrained matrix factorization problem. We demonstrate numerically the benefit of the resulting scheme in both structured and unstructured graphs.</p><h4 id="embedding-economic-incentives-in-social-networks-shape-the-diffusion-of-digital-technological-innovation" tabindex="-1"><a class="header-anchor" href="#embedding-economic-incentives-in-social-networks-shape-the-diffusion-of-digital-technological-innovation"><span>Embedding Economic Incentives in Social Networks Shape the Diffusion of Digital Technological Innovation</span></a></h4><p><strong>Authors</strong>: Zhe Li, Tianfang Zhao, Hongjun Zhu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06973v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06973v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The digital innovation accompanied by explicit economic incentives have fundamentally changed the process of innovation diffusion. As a representative of digital innovation, NFTs provide a decentralized and secure way to authenticate and trade digital assets, offering the potential for new revenue streams in the digital space. However, current researches about NFTs mainly focus on their transaction networks and community culture, leaving the interplay among diffusion dynamics, economic dynamics, and social constraints on Twitter. By collecting and analyzing NFTs-related tweet dataset, the motivations of retweeters, the information mechanisms behind emojis, and the networked-based diffusion dynamics is systematically investigated. Results indicate that Retweeting is fueled by Freemint and trading information, with the higher economic incentives as a major motivation and some potential organizational tendencies. The diffusion of NFT is primarily driven by a &#39;Ringed-layered&#39; information mechanism involving individual promoters and speculators. Both the frequency and presentation of content contribute positively to the growth of the retweet network. This study contributes to the innovation diffusion theory with economic incentives embedded.</p><h4 id="dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting" tabindex="-1"><a class="header-anchor" href="#dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting"><span>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting</span></a></h4><p><strong>Authors</strong>: Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06903v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06903v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary &quot;flat&quot; (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: http://dreamscene360.github.io/</p><h4 id="precession-and-split-of-tilted-geometrically-thin-accretion-disk-an-analytical-study" tabindex="-1"><a class="header-anchor" href="#precession-and-split-of-tilted-geometrically-thin-accretion-disk-an-analytical-study"><span>Precession and Split of Tilted, Geometrically Thin Accretion Disk: an Analytical Study</span></a></h4><p><strong>Authors</strong>: Ye Shen, Bin Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06898v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06898v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: It has been observed that many relativistic jets display a kind of cork-screw-like precession. Numerical simulations has suggested that such kind of precession may originate from the precession of the disk. In this work, we introduce an analytical model to describe the precession and split of a tilted, geometrically thin disk. We consider the Lense-Thirring effect from the central (primary) black hole (BH) and the gravitational effect from the companion (secondary) BH far away from the center, both of which could induce the precession of the accretion disk around the spin axis of central black hole. We propose the splitting conditions that when the rate of viscous diffusion cannot catch up with the dynamical frequency at a certain layer of fluid, the disk would split into two parts which precess independently. We presume that the precessions of the inner and outer disks are in accord with the rotation and precession of jet, respectively. By matching the frequencies of the disks to the observed frequencies of jet in the cork-screw-like precession and considering the splitting condition, we are allowed to read four parameters, the innermost radius ($r_{\rm in}$), the outermost radius ($r_{\rm out}$) of the disk, the initial splitting radius ($r_{\rm sp,0}$), and the inflow speed magnitude($\beta$), of the disk. We apply this model to OJ 287. Moreover, considering the inward shrinking of the disks, we find the time variation of the precession angle of jet. This time variation presents a unique feature of our model, which could be distinguishable in the future observation.</p><h4 id="fine-color-guidance-in-diffusion-models-and-its-application-to-image-compression-at-extremely-low-bitrates" tabindex="-1"><a class="header-anchor" href="#fine-color-guidance-in-diffusion-models-and-its-application-to-image-compression-at-extremely-low-bitrates"><span>Fine color guidance in diffusion models and its application to image compression at extremely low bitrates</span></a></h4><p><strong>Authors</strong>: Tom Bordin, Thomas Maugey</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06865v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06865v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This study addresses the challenge of, without training or fine-tuning, controlling the global color aspect of images generated with a diffusion model. We rewrite the guidance equations to ensure that the outputs are closer to a known color map, and this without hindering the quality of the generation. Our method leads to new guidance equations. We show in the color guidance context that, the scaling of the guidance should not decrease but remains high throughout the diffusion process. In a second contribution, our guidance is applied in a compression framework, we combine both semantic and general color information on the image to decode the images at low cost. We show that our method is effective at improving fidelity and realism of compressed images at extremely low bit rates, when compared to other classical or more semantic oriented approaches.</p><h4 id="order-isomorphisms-of-sup-stable-function-spaces-continuous-lipschitz-c-convex-and-beyond" tabindex="-1"><a class="header-anchor" href="#order-isomorphisms-of-sup-stable-function-spaces-continuous-lipschitz-c-convex-and-beyond"><span>Order isomorphisms of sup-stable function spaces: continuous, Lipschitz, c-convex, and beyond</span></a></h4><p><strong>Authors</strong>: Pierre-Cyril Aubin-Frankowski, Stéphane Gaubert</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06857v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06857v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: There have been many parallel streams of research studying order isomorphisms of some specific sets $\mathcal{G}$ of functions from a set $\mathcal{X}$ to $\mathbb{R}\cup{\pm\infty}$, such as the sets of convex or Lipschitz functions. We provide in this article a unified abstract approach inspired by $c$-convex functions. Our results are obtained highlighting the role of inf and sup-irreducible elements of $\mathcal{G}$ and the usefulness of characterizing them, to subsequently derive the structure of order isomorphisms, and in particular of those commuting with the addition of scalars. We show that in many cases all these isomorphisms $J:\mathcal{G}\to\mathcal{G}$ are of the form $Jf=g+f\circ \phi$ for a translation $g:\mathcal{X}\to\mathbb{R}$ and a bijective reparametrization $\phi:\mathcal{X}\to \mathcal{X}$. We apply our theory to the sets of $c$-convex functions on compact Hausdorff spaces, to the set of lower semicontinuous (convex) functions on a Hausdorff topological vector space and to Lipschitz and 1-Lipschitz functions of complete metric spaces.</p><h4 id="udiff-generating-conditional-unsigned-distance-fields-with-optimal-wavelet-diffusion" tabindex="-1"><a class="header-anchor" href="#udiff-generating-conditional-unsigned-distance-fields-with-optimal-wavelet-diffusion"><span>UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion</span></a></h4><p><strong>Authors</strong>: Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu, Zhizhong Han</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06851v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06851v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Diffusion models have shown remarkable results for image generation, editing and inpainting. Recent works explore diffusion models for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function. However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces. In this work, we present UDiFF, a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally. Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation. Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs. We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used benchmarks. Page: https://weiqi-zhang.github.io/UDiFF.</p><h4 id="tuning-free-adaptive-style-incorporation-for-structure-consistent-text-driven-style-transfer" tabindex="-1"><a class="header-anchor" href="#tuning-free-adaptive-style-incorporation-for-structure-consistent-text-driven-style-transfer"><span>Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer</span></a></h4><p><strong>Authors</strong>: Yanqi Ge, Jiaqi Liu, Qingnan Fan, Xi Jiang, Ye Huang, Shuai Qin, Hong Gu, Wen Li, Lixin Duan</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06835v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06835v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this work, we target the task of text-driven style transfer in the context of text-to-image (T2I) diffusion models. The main challenge is consistent structure preservation while enabling effective style transfer effects. The past approaches in this field directly concatenate the content and style prompts for a prompt-level style injection, leading to unavoidable structure distortions. In this work, we propose a novel solution to the text-driven style transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve fine-grained feature-level style incorporation. It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and style features, and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and style information from a structure-consistent manner. Experimentally, our method exhibits much better performance in both structure preservation and stylized effects.</p><h4 id="zero-shot-point-cloud-completion-via-2d-priors" tabindex="-1"><a class="header-anchor" href="#zero-shot-point-cloud-completion-via-2d-priors"><span>Zero-shot Point Cloud Completion Via 2D Priors</span></a></h4><p><strong>Authors</strong>: Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06814v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06814v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: 3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training %, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a zero-shot framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion models to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data.</p><h4 id="modeling-of-antenna-coupled-si-mosfets-in-the-terahertz-frequency-range" tabindex="-1"><a class="header-anchor" href="#modeling-of-antenna-coupled-si-mosfets-in-the-terahertz-frequency-range"><span>Modeling of antenna-coupled Si MOSFETs in the Terahertz Frequency Range</span></a></h4><p><strong>Authors</strong>: Florian Ludwig, Jakob Holstein, Anastasiya Krysl, Alvydas Lisauskas, Hartmut G. Roskos</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06790v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06790v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We report on the modeling and experimental characterization of Si CMOS detectors of terahertz radiation based on antenna-coupled field-effect transistors (TeraFETs). The detectors are manufactured using TSMC&#39;s 65-nm technology. We apply two models -- the TSMC RF foundry model and our own ADS-HDM -- to simulate the Si CMOS TeraFET performance and compare their predictions with respective experimental data. Both models are implemented in the commercial circuit simulation software Keysight Advanced Design System (ADS). We find that the compact model TSMC RF is capable to predict the detector responsivity and its dependence on frequency and gate voltage with good accuracy up to the highest frequency of 1.2 THz covered in this study. This frequency is well beyond the tool&#39;s intended operation range for 5G communications and 110-GHz millimeter wave applications. We demonstrate that our self-developed physics-based ADS-HDM tool, which relies on an extended one-dimensional hydrodynamic transport model and can be adapted readily to other material technologies, has high predictive qualities comparable to those of the foundry model. We use the ADS-HDM to discuss the contribution of diffusive and plasmonic effects to the THz response of Si CMOS TeraFETs, finding that these effects, while becoming more significant with rising frequency, are never dominant. Finally, we estimate that the electrical NEP (perfect power coupling conditions) is on the order of 5 pW/$\sqrt{\rm{Hz}}$ at room-temperature.</p><h4 id="fluid-simulation-for-a-finite-size-plasma" tabindex="-1"><a class="header-anchor" href="#fluid-simulation-for-a-finite-size-plasma"><span>Fluid Simulation for a Finite Size Plasma</span></a></h4><p><strong>Authors</strong>: Subhasish Bag, Vikrant Saxena, Amita Das</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06786v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06786v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Studies on finite-size plasma have attracted a lot of attention lately. They can form by ionizing liquid droplets by lasers. The dynamical behavior of such plasma droplets is, therefore, a topic of significant interest. In particular, questions related to the linear and nonlinear characteristics (associated with the inhomogeneous density typically at the edge of the droplet), the behavior of plasma expansion, etc., are of interest. A one-dimensional fluid simulation study has been carried out to investigate this behavior. It is observed that a slight imbalance in the charge density leads to oscillations that are concentrated and keep acquiring higher amplitude and sharper profile at the inhomogeneous edge region. Such oscillations lead to the expansion of the droplet. Though the fluid description breaks when the sharpness of these structures becomes comparable to the grid size, it provides a reasonable estimate of wave-breaking time. The presence of dissipative effects like diffusion is shown to arrest the sharpness of these structures. The dynamics of these structures in the presence of an externally applied oscillating electric field corresponding to a long wavelength radiation has also been studied.</p><h4 id="urban-architect-steerable-3d-urban-scene-generation-with-layout-prior" tabindex="-1"><a class="header-anchor" href="#urban-architect-steerable-3d-urban-scene-generation-with-layout-prior"><span>Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior</span></a></h4><p><strong>Authors</strong>: Fan Lu, Kwan-Yee Lin, Yan Xu, Hongsheng Li, Guang Chen, Changjun Jiang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06780v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06780v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-3D generation has achieved remarkable success via large-scale text-to-image diffusion models. Nevertheless, there is no paradigm for scaling up the methodology to urban scale. Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization. In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior. It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation. Upon this, we propose two modifications -- (1) We introduce Layout-Guided Variational Score Distillation to address model optimization inadequacies. It conditions the score distillation sampling process with geometric and semantic constraints of 3D layouts. (2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes. Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time. We also present various scene editing demonstrations, showing the powers of steerable urban scene generation. Website: https://urbanarchitect.github.io.</p><h4 id="diffusiondialog-a-diffusion-model-for-diverse-dialog-generation-with-latent-space" tabindex="-1"><a class="header-anchor" href="#diffusiondialog-a-diffusion-model-for-diverse-dialog-generation-with-latent-space"><span>DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with Latent Space</span></a></h4><p><strong>Authors</strong>: Jianxiang Xiang, Zhenhua Liu, Haodong Liu, Yin Bai, Jia Cheng, Wenliang Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06760v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06760v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In real-life conversations, the content is diverse, and there exists the one-to-many problem that requires diverse generation. Previous studies attempted to introduce discrete or Gaussian-based continuous latent variables to address the one-to-many problem, but the diversity is limited. Recently, diffusion models have made breakthroughs in computer vision, and some attempts have been made in natural language processing. In this paper, we propose DiffusionDialog, a novel approach to enhance the diversity of dialogue generation with the help of diffusion model. In our approach, we introduce continuous latent variables into the diffusion model. The problem of using latent variables in the dialog task is how to build both an effective prior of the latent space and an inferring process to obtain the proper latent given the context. By combining the encoder and latent-based diffusion model, we encode the response&#39;s latent representation in a continuous space as the prior, instead of fixed Gaussian distribution or simply discrete ones. We then infer the latent by denoising step by step with the diffusion model. The experimental results show that our model greatly enhances the diversity of dialog responses while maintaining coherence. Furthermore, in further analysis, we find that our diffusion model achieves high inference efficiency, which is the main challenge of applying diffusion models in natural language processing.</p><h4 id="the-physics-of-antimicrobial-activity-of-ionic-liquids" tabindex="-1"><a class="header-anchor" href="#the-physics-of-antimicrobial-activity-of-ionic-liquids"><span>The Physics of Antimicrobial Activity of Ionic Liquids</span></a></h4><p><strong>Authors</strong>: V. K. Sharma, J. Gupta, J. Bhatt Mitra, H. Srinivasan, V. García Sakai, S. K. Ghosh, S. Mitra</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06739v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06739v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The bactericidal potency of ionic liquids (ILs) is well-established, yet their precise mechanism of action remains elusive. Here, we show evidence that the bactericidal action of ILs primarily involves permeabilizing the bacterial cell membrane. Our findings reveal that ILs exert their effects by directly interacting with the lipid bilayer and enhancing the membrane dynamics. Lateral lipid diffusion is accelerated which in turn augments membrane permeability, ultimately leading to bacterial death. Furthermore, our results establish a significant connection: an increase in the alkyl chain length of ILs correlates with a notable enhancement in both lipid lateral diffusion and antimicrobial potency. This underscores a compelling correlation between membrane dynamics and antimicrobial effectiveness, providing valuable insights for the rational design and optimization of IL-based antimicrobial agents in healthcare applications.</p><h4 id="disguised-copyright-infringement-of-latent-diffusion-models" tabindex="-1"><a class="header-anchor" href="#disguised-copyright-infringement-of-latent-diffusion-models"><span>Disguised Copyright Infringement of Latent Diffusion Models</span></a></h4><p><strong>Authors</strong>: Yiwei Lu, Matthew Y. R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06737v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06737v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access.</p><h4 id="covariance-regression-with-high-dimensional-predictors" tabindex="-1"><a class="header-anchor" href="#covariance-regression-with-high-dimensional-predictors"><span>Covariance Regression with High-Dimensional Predictors</span></a></h4><p><strong>Authors</strong>: Yuheng He, Changliang Zou, Yi Zhao</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06701v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06701v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In the high-dimensional landscape, addressing the challenges of covariance regression with high-dimensional covariates has posed difficulties for conventional methodologies. This paper addresses these hurdles by presenting a novel approach for high-dimensional inference with covariance matrix outcomes. The proposed methodology is illustrated through its application in elucidating brain coactivation patterns observed in functional magnetic resonance imaging (fMRI) experiments and unraveling complex associations within anatomical connections between brain regions identified through diffusion tensor imaging (DTI). In the pursuit of dependable statistical inference, we introduce an integrative approach based on penalized estimation. This approach combines data splitting, variable selection, aggregation of low-dimensional estimators, and robust variance estimation. It enables the construction of reliable confidence intervals for covariate coefficients, supported by theoretical confidence levels under specified conditions, where asymptotic distributions are provided. Through various types of simulation studies, the proposed approach performs well for covariance regression in the presence of high-dimensional covariates. This innovative approach is applied to the Lifespan Human Connectome Project (HCP) Aging Study, which aims to uncover a typical aging trajectory and variations in the brain connectome among mature and older adults. The proposed approach effectively identifies brain networks and associated predictors of white matter integrity, aligning with established knowledge of the human brain.</p><h4 id="voiceshop-a-unified-speech-to-speech-framework-for-identity-preserving-zero-shot-voice-editing" tabindex="-1"><a class="header-anchor" href="#voiceshop-a-unified-speech-to-speech-framework-for-identity-preserving-zero-shot-voice-editing"><span>VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing</span></a></h4><p><strong>Authors</strong>: Philip Anastassiou, Zhenyu Tang, Kainan Peng, Dongya Jia, Jiaxin Li, Ming Tu, Yuping Wang, Yuxuan Wang, Mingbo Ma</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06674v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06674v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We present VoiceShop, a novel speech-to-speech framework that can modify multiple attributes of speech, such as age, gender, accent, and speech style, in a single forward pass while preserving the input speaker&#39;s timbre. Previous works have been constrained to specialized models that can only edit these attributes individually and suffer from the following pitfalls: the magnitude of the conversion effect is weak, there is no zero-shot capability for out-of-distribution speakers, or the synthesized outputs exhibit undesirable timbre leakage. Our work proposes solutions for each of these issues in a simple modular framework based on a conditional diffusion backbone model with optional normalizing flow-based and sequence-to-sequence speaker attribute-editing modules, whose components can be combined or removed during inference to meet a wide array of tasks without additional model finetuning. Audio samples are available at \url{https://voiceshopai.github.io}.</p><h4 id="simple-arithmetic-operation-in-latent-space-can-generate-a-novel-three-dimensional-graph-metamaterials" tabindex="-1"><a class="header-anchor" href="#simple-arithmetic-operation-in-latent-space-can-generate-a-novel-three-dimensional-graph-metamaterials"><span>Simple arithmetic operation in latent space can generate a novel three dimensional graph metamaterials</span></a></h4><p><strong>Authors</strong>: Namjung Kim, Dongseok Lee, Chanyoung Kim, Dosung Lee, Youngjoon Hong</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06671v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06671v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recent advancements in artificial intelligence (AI)-based design strategies for metamaterials have revolutionized the creation of customizable architectures spanning nano- to macro-scale dimensions, achieving unprecedented mechanical behaviors that surpass the inherent properties of the constituent materials. However, the growing complexity of these methods poses challenges in generating diverse metamaterials without substantial human and computational resources, hindering widespread adoption. Addressing this, our study introduces an innovative design strategy capable of generating various three-dimensional graph metamaterials using simple arithmetic operations within the latent space. By seamlessly integrating hidden representations of disentangled latent space and latent diffusion processes, our approach provides a comprehensive understanding of complex design spaces, generating diverse graph metamaterials through arithmetic operations. This methodology stands as a versatile tool for creating structures ranging from repetitive lattice structures to functionally graded mechanical metamaterials. It also serves as an inverse design strategy for diverse lattice structures, including crystalline structures and those made of trabecular bone. We believe that this methodology represents a foundational step in advancing our comprehension of the intricate latent design space, offering the potential to establish a unified model for various traditional generative models in the realm of mechanical metamaterials.</p><h4 id="safegen-mitigating-unsafe-content-generation-in-text-to-image-models" tabindex="-1"><a class="header-anchor" href="#safegen-mitigating-unsafe-content-generation-in-text-to-image-models"><span>SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models</span></a></h4><p><strong>Authors</strong>: Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06666v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06666v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Text-to-image (T2I) models, such as Stable Diffusion, have exhibited remarkable performance in generating high-quality images from text descriptions in recent years. However, text-to-image models may be tricked into generating not-safe-for-work (NSFW) content, particularly in sexual scenarios. Existing countermeasures mostly focus on filtering inappropriate inputs and outputs, or suppressing improper text embeddings, which can block explicit NSFW-related content (e.g., naked or sexy) but may still be vulnerable to adversarial prompts inputs that appear innocent but are ill-intended. In this paper, we present SafeGen, a framework to mitigate unsafe content generation by text-to-image models in a text-agnostic manner. The key idea is to eliminate unsafe visual representations from the model regardless of the text input. In this way, the text-to-image model is resistant to adversarial prompts since unsafe visual representations are obstructed from within. Extensive experiments conducted on four datasets demonstrate SafeGen&#39;s effectiveness in mitigating unsafe content generation while preserving the high-fidelity of benign images. SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1% sexual content removal performance. Furthermore, our constructed benchmark of adversarial prompts provides a basis for future development and evaluation of anti-NSFW-generation methods.</p><h4 id="deep-generative-data-assimilation-in-multimodal-setting" tabindex="-1"><a class="header-anchor" href="#deep-generative-data-assimilation-in-multimodal-setting"><span>Deep Generative Data Assimilation in Multimodal Setting</span></a></h4><p><strong>Authors</strong>: Yongquan Qu, Juan Nathaniel, Shuolin Li, Pierre Gentine</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06665v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06665v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Robust integration of physical knowledge and data is key to improve computational simulations, such as Earth system models. Data assimilation is crucial for achieving this goal because it provides a systematic framework to calibrate model outputs with observations, which can include remote sensing imagery and ground station measurements, with uncertainty quantification. Conventional methods, including Kalman filters and variational approaches, inherently rely on simplifying linear and Gaussian assumptions, and can be computationally expensive. Nevertheless, with the rapid adoption of data-driven methods in many areas of computational sciences, we see the potential of emulating traditional data assimilation with deep learning, especially generative models. In particular, the diffusion-based probabilistic framework has large overlaps with data assimilation principles: both allows for conditional generation of samples with a Bayesian inverse framework. These models have shown remarkable success in text-conditioned image generation or image-controlled video synthesis. Likewise, one can frame data assimilation as observation-conditioned state calibration. In this work, we propose SLAMS: Score-based Latent Assimilation in Multimodal Setting. Specifically, we assimilate in-situ weather station data and ex-situ satellite imagery to calibrate the vertical temperature profiles, globally. Through extensive ablation, we demonstrate that SLAMS is robust even in low-resolution, noisy, and sparse data settings. To our knowledge, our work is the first to apply deep generative framework for multimodal data assimilation using real-world datasets; an important step for building robust computational simulators, including the next-generation Earth system models. Our code is available at: https://github.com/yongquan-qu/SLAMS</p><h4 id="efficient-denoising-using-score-embedding-in-score-based-diffusion-models" tabindex="-1"><a class="header-anchor" href="#efficient-denoising-using-score-embedding-in-score-based-diffusion-models"><span>Efficient Denoising using Score Embedding in Score-based Diffusion Models</span></a></h4><p><strong>Authors</strong>: Andrew S. Na, William Gao, Justin W. L. Wan</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06661v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06661v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: It is well known that training a denoising score-based diffusion models requires tens of thousands of epochs and a substantial number of image data to train the model. In this paper, we propose to increase the efficiency in training score-based diffusion models. Our method allows us to decrease the number of epochs needed to train the diffusion model. We accomplish this by solving the log-density Fokker-Planck (FP) Equation numerically to compute the score \textit{before} training. The pre-computed score is embedded into the image to encourage faster training under slice Wasserstein distance. Consequently, it also allows us to decrease the number of images we need to train the neural network to learn an accurate score. We demonstrate through our numerical experiments the improved performance of our proposed method compared to standard score-based diffusion models. Our proposed method achieves a similar quality to the standard method meaningfully faster.</p><h2 id="_2024-04-09" tabindex="-1"><a class="header-anchor" href="#_2024-04-09"><span>2024-04-09</span></a></h2><h4 id="an-energy-stable-high-order-cut-cell-discontinuous-galerkin-method-with-state-redistribution-for-wave-propagation" tabindex="-1"><a class="header-anchor" href="#an-energy-stable-high-order-cut-cell-discontinuous-galerkin-method-with-state-redistribution-for-wave-propagation"><span>An Energy Stable High-Order Cut Cell Discontinuous Galerkin Method with State Redistribution for Wave Propagation</span></a></h4><p><strong>Authors</strong>: Christina G. Taylor, Lucas Wilcox, Jesse Chan</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06630v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06630v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Cut meshes are a type of mesh that is formed by allowing embedded boundaries to &quot;cut&quot; a simple underlying mesh resulting in a hybrid mesh of cut and standard elements. While cut meshes can allow complex boundaries to be represented well regardless of the mesh resolution, their arbitrarily shaped and sized cut elements can present issues such as the small cell problem, where small cut elements can result in a severely restricted CFL condition. State redistribution, a technique developed by Berger and Giuliani [1], can be used to address the small cell problem. In this work, we pair state redistribution with a high-order discontinuous Galerkin scheme that is $L_2$ energy stable for arbitrary quadrature. We prove that state redistribution can be added to a provably $L_2$ energy stable discontinuous Galerkin method on a cut mesh without damaging the scheme&#39;s $L_2$ stability. We numerically verify the high order accuracy and stability of our scheme on two-dimensional wave propagation problems.</p><h4 id="inertial-active-matter-with-coulomb-friction" tabindex="-1"><a class="header-anchor" href="#inertial-active-matter-with-coulomb-friction"><span>Inertial active matter with Coulomb friction</span></a></h4><p><strong>Authors</strong>: Alexander P. Antonov, Lorenzo Caprini, Christian Scholz, Hartmut Löwen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06615v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06615v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Friction is central to the motion of active (self-propelled) objects, such as bacteria, animals or robots. While in a viscous fluid friction is typically described by Stokes&#39; law, objects in contact with other solid bodies are often governed by more complex empirical friction laws. We study active dry particles subject to dry Coulomb friction force numerically and analytically. The interplay of friction and activity forces induces a rich behavior resulting in three distinct dynamical regimes. While for low activity levels, active Brownian motion is recovered, for large activity we observe a dynamical Stop &amp; Go regime continuously switching from diffusion and accelerated motion. For further activity values, we observe a super-mobile dynamical regime characterized by a fully accelerated motion which is described by an anomalous scaling of the diffusion coefficient with the activity. These findings cannot be observed with Stokes viscous forces typical of active swimmers but are central in dry active objects.</p><h4 id="differences-between-stable-and-unstable-architectures-of-compact-planetary-systems" tabindex="-1"><a class="header-anchor" href="#differences-between-stable-and-unstable-architectures-of-compact-planetary-systems"><span>Differences between Stable and Unstable Architectures of Compact Planetary Systems</span></a></h4><p><strong>Authors</strong>: Kathryn Volk, Renu Malhotra</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06567v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06567v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We present a stability analysis of a large set of simulated planetary systems of three or more planets based on architectures of multiplanet systems discovered by \textit{Kepler} and \textit{K2}. We propagated 21,400 simulated planetary systems up to 5 billion orbits of the innermost planet; approximately 13% of these simulations ended in a planet-planet collision within that timespan. We examined trends in dynamical stability based on dynamical spacings, orbital period ratios, and mass ratios of nearest-neighbor planets as well as the system-wide planet mass distribution and the spectral fraction describing the system&#39;s short-term evolution. We find that instability is more likely in planetary systems with adjacent planet pairs that have period ratios less than two and in systems of greater variance of planet masses. Systems with planet pairs at very small dynamical spacings (less than $\sim10-12$ mutual Hill radius) are also prone to instabilities, but instabilities also occur at much larger planetary separations. We find that a large spectral fraction (calculated from short integrations) is a reasonable predictor of longer-term dynamical instability; systems that have a large number of Fourier components in their eccentricity vectors are prone to secular chaos and subsequent eccentricity growth and instabilities.</p><h4 id="training-free-open-vocabulary-segmentation-with-offline-diffusion-augmented-prototype-generation" tabindex="-1"><a class="header-anchor" href="#training-free-open-vocabulary-segmentation-with-offline-diffusion-augmented-prototype-generation"><span>Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation</span></a></h4><p><strong>Authors</strong>: Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06542v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06542v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form. Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal alignments. However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts. Further, training on large-scale datasets inevitably brings significant computational costs. In this paper, we propose FreeDA, a training-free diffusion-augmented method for open-vocabulary semantic segmentation, which leverages the ability of diffusion models to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts. At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities. Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training.</p><h4 id="first-search-for-high-energy-neutrino-emission-from-galaxy-mergers" tabindex="-1"><a class="header-anchor" href="#first-search-for-high-energy-neutrino-emission-from-galaxy-mergers"><span>First Search for High-Energy Neutrino Emission from Galaxy Mergers</span></a></h4><p><strong>Authors</strong>: Subhadip Bouri, Priyank Parashari, Mousumi Das, Ranjan Laha</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06539v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06539v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The exact sources of high-energy neutrinos detected by the IceCube neutrino observatory still remain a mystery. For the first time, this work explores the hypothesis that galaxy mergers may serve as sources for these high-energy neutrinos. Galaxy mergers can host very high-energy hadronic and photohadronic processes, which may produce very high-energy neutrinos. We perform an unbinned maximum-likelihood-ratio analysis utilizing the galaxy merger data from six catalogs and 10 years of public IceCube muon-track data to quantify any correlation between these mergers and neutrino events. First, we perform the single source search analysis, which reveals that none of the considered galaxy mergers exhibit a statistically significant correlation with high-energy neutrino events detected by IceCube. Furthermore, we conduct a stacking analysis with three different weighting schemes to understand if these galaxy mergers can contribute significantly to the diffuse flux of high-energy astrophysical neutrinos detected by IceCube. We find that upper limits (at $95%$ c.l.) of the all flavour high-energy neutrino flux, associated with galaxy mergers considered in this study, at $100$ TeV with spectral index $\Gamma=-2$ are $2.57\times 10^{-18}$, $8.51 \times 10^{-19}$ and $2.36 \times 10^{-18}$ $\rm GeV^{-1},cm^2,s^{-1},sr^{-1}$ for the three weighting schemes. This work shows that these selected galaxy mergers do not contribute significantly to the IceCube detected high energy neutrino flux. We hope that in the near future with more data, the search for neutrinos from galaxy mergers can either discover their neutrino production or impose more stringent constraints on the production mechanism of high-energy neutrinos within galaxy mergers.</p><h4 id="exploring-the-nature-of-dark-matter-with-the-extreme-galaxy-agc-114905" tabindex="-1"><a class="header-anchor" href="#exploring-the-nature-of-dark-matter-with-the-extreme-galaxy-agc-114905"><span>Exploring the nature of dark matter with the extreme galaxy AGC 114905</span></a></h4><p><strong>Authors</strong>: Pavel E. Mancera Piña, Giulia Golini, Ignacio Trujillo, Mireia Montes</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06537v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06537v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: AGC 114905 is a dwarf gas-rich ultra-diffuse galaxy seemingly in tension with the cold dark matter (CDM) model. Specifically, the galaxy appears to have an extremely low-density halo and a high baryon fraction, while CDM predicts dwarfs to have very dense and dominant dark haloes. The alleged tension relies on the galaxy&#39;s rotation curve decomposition, which depends heavily on its inclination. This inclination, estimated from the gas morphology, remains somewhat uncertain. We present unmatched ultra-deep optical imaging of AGC 114905 reaching surface brightness limits $\mu_{\rm r,lim} \approx 32$ mag/arcsec$^2$ ($3\sigma$; 10 arcsec $\times$ 10 arcsec) obtained with the 10.4-m Gran Telescopio Canarias. With the new imaging, we characterise the galaxy&#39;s morphology, surface brightness, colours, and stellar mass profiles in great detail. The stellar disc has a similar extent as the gas, presents spiral arms-like features, and shows a well-defined edge. Stars and gas share similar morphology, and crucially, we find an inclination of $31\pm2^\circ$, in agreement with the previous determinations. We revisit the rotation curve decomposition of the galaxy, and we explore different mass models in the context of CDM, self-interacting dark matter (SIDM), fuzzy dark matter (FDM) or Modified Newtonian Dynamics (MOND). We find that the latter does not fit the circular speed of the galaxy, while CDM only does so with dark halo parameters rarely seen in cosmological simulations. Within the uncertainties, SIDM and FDM remain feasible candidates to explain the observed kinematics of AGC 114905.</p><h4 id="convergence-analysis-of-novel-discontinuous-galerkin-methods-for-a-convection-dominated-problem" tabindex="-1"><a class="header-anchor" href="#convergence-analysis-of-novel-discontinuous-galerkin-methods-for-a-convection-dominated-problem"><span>Convergence analysis of novel discontinuous Galerkin methods for a convection dominated problem</span></a></h4><p><strong>Authors</strong>: Satyajith Bommana Boyana, Thomas Lewis, Sijing Liu, Yi Zhang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06490v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06490v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this paper, we propose and analyze a numerically stable and convergent scheme for a convection-diffusion-reaction equation in the convection-dominated regime. Discontinuous Galerkin (DG) methods are considered since standard finite element methods for the convection-dominated equation cause spurious oscillations. We choose to follow a novel DG finite element differential calculus framework introduced in Feng et al. (2016) and approximate the infinite-dimensional operators in the equation with the finite-dimensional DG differential operators. Specifically, we construct the numerical method by using the dual-wind discontinuous Galerkin (DWDG) formulation for the diffusive term and the average discrete gradient operator for the convective term along with standard DG stabilization. We prove that the method converges optimally in the convection-dominated regime. Numerical results are provided to support the theoretical findings.</p><h4 id="finding-stable-price-zones-in-european-electricity-markets-aiming-to-square-the-circle" tabindex="-1"><a class="header-anchor" href="#finding-stable-price-zones-in-european-electricity-markets-aiming-to-square-the-circle"><span>Finding Stable Price Zones in European Electricity Markets: Aiming to Square the Circle?</span></a></h4><p><strong>Authors</strong>: Teodora Dobos, Martin Bichler, Johannes Knörr</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06489v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06489v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The European day-ahead electricity market is split into multiple bidding zones. Within these zones, a uniform energy price is computed for each hour. Large bidding zones have been under scrutiny. The fact that zonal clearing ignores the transmission capacities within zones and the increase in renewables lead to a growing number of interventions in the generation of energy sources and large redispatch costs. The European Union Agency for the Cooperation of Energy Regulators (ACER) proposed alternative bidding zone configurations that should be analyzed as part of the Bidding Zone Review. For Germany, four alternative configurations were suggested. Bidding zones shall be stable and based on long-term, structural congestion in the grid. We analyzed the proposed configurations considering different clustering algorithms and periods. We found that the configurations do not reduce the price standard deviations within zones considerably, while the average prices across zones are similar. Other configurations identified based on clustering prices lead to lower price standard deviations but are not geographically coherent. Importantly, different configurations emerge depending on clustering features, algorithm, and period considered. Given the substantial changes in energy supply and demand that can be expected in the future, defining stable configurations appears to be a moving target.</p><h4 id="uncovering-tidal-treasures-automated-classification-of-faint-tidal-features-in-decals-data" tabindex="-1"><a class="header-anchor" href="#uncovering-tidal-treasures-automated-classification-of-faint-tidal-features-in-decals-data"><span>Uncovering Tidal Treasures: Automated Classification of Faint Tidal Features in DECaLS Data</span></a></h4><p><strong>Authors</strong>: Alexander J. Gordon, Annette M. N. Ferguson, Robert G. Mann</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06487v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06487v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Tidal features are a key observable prediction of the hierarchical model of galaxy formation and contain a wealth of information about the properties and history of a galaxy. Modern wide-field surveys such as LSST and Euclid will revolutionise the study of tidal features. However, the volume of data will far surpass the capacity to inspect each galaxy to identify the feature visually, thereby motivating an urgent need to develop automated detection methods. This paper presents a visual classification of $\sim$2,000 galaxies from the DECaLS survey into different tidal feature categories: arms, streams, shells, and diffuse. Using these labels, we trained a Convolutional Neural Network (CNN) to reproduce the assigned visual classifications. Overall our network performed well and retrieved a median $81.1^{+5.8}<em>{-6.5}$, $65.7^{+5.0}</em>{-8.4}$, $91.3^{+6.0}<em>{-5.9}$, and $82.3^{+1.4}</em>{-7.9}$ per cent of the actual instances of arm, stream, shell, and diffuse features respectively for just 20 per cent contamination. We verified that the network was classifying the images correctly by using a Gradient-weighted Class Activation Mapping analysis to highlight important regions on the images for a given classification. This is the first demonstration of using CNNs to classify tidal features into sub-categories, and it will pave the way for the identification of different categories of tidal features in the vast samples of galaxies that forthcoming wide-field surveys will deliver.</p><h4 id="geodirdock-guiding-docking-along-geodesic-paths" tabindex="-1"><a class="header-anchor" href="#geodirdock-guiding-docking-along-geodesic-paths"><span>GeoDirDock: Guiding Docking Along Geodesic Paths</span></a></h4><p><strong>Authors</strong>: Raúl Miñán, Javier Gallardo, Álvaro Ciudad, Alexis Molina</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06481v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06481v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This work introduces GeoDirDock (GDD), a novel approach to molecular docking that enhances the accuracy and physical plausibility of ligand docking predictions. GDD guides the denoising process of a diffusion model along geodesic paths within multiple spaces representing translational, rotational, and torsional degrees of freedom. Our method leverages expert knowledge to direct the generative modeling process, specifically targeting desired protein-ligand interaction regions. We demonstrate that GDD significantly outperforms existing blind docking methods in terms of RMSD accuracy and physicochemical pose realism. Our results indicate that incorporating domain expertise into the diffusion process leads to more biologically relevant docking predictions. Additionally, we explore the potential of GDD for lead optimization in drug discovery through angle transfer in maximal common substructure (MCS) docking, showcasing its capability to predict ligand orientations for chemically similar compounds accurately.</p><h4 id="magic-boost-boost-3d-generation-with-mutli-view-conditioned-diffusion" tabindex="-1"><a class="header-anchor" href="#magic-boost-boost-3d-generation-with-mutli-view-conditioned-diffusion"><span>Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion</span></a></h4><p><strong>Authors</strong>: Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Jiashi Feng, Guosheng Lin</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06429v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06429v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Benefiting from the rapid development of 2D diffusion models, 3D content creation has made significant progress recently. One promising solution involves the fine-tuning of pre-trained 2D diffusion models to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models. However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries. To solve this problem, we propose Magic-Boost, a multi-view conditioned diffusion model that significantly refines coarse generative results through a brief period of SDS optimization ($\sim15$min). Compared to the previous text or single image based diffusion models, Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images. It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both geometry and texture of the initial generative results. Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details. (Project Page: https://magic-research.github.io/magic-boost/)</p><h4 id="zest-zero-shot-material-transfer-from-a-single-image" tabindex="-1"><a class="header-anchor" href="#zest-zero-shot-material-transfer-from-a-single-image"><span>ZeST: Zero-Shot Material Transfer from a Single Image</span></a></h4><p><strong>Authors</strong>: Ta-Ying Cheng, Prafull Sharma, Andrew Markham, Niki Trigoni, Varun Jampani</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06425v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06425v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We propose ZeST, a method for zero-shot material transfer to an object in the input image given a material exemplar image. ZeST leverages existing diffusion adapters to extract implicit material representation from the exemplar image. This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues. The method works on real images without any training resulting a zero-shot approach. Both qualitative and quantitative results on real and synthetic datasets demonstrate that ZeST outputs photorealistic images with transferred materials. We also show the application of ZeST to perform multiple edits and robust material assignment under different illuminations. Project Page: https://ttchengab.github.io/zest</p><h4 id="policy-guided-diffusion" tabindex="-1"><a class="header-anchor" href="#policy-guided-diffusion"><span>Policy-Guided Diffusion</span></a></h4><p><strong>Authors</strong>: Matthew Thomas Jackson, Michael Tryfan Matthews, Cong Lu, Benjamin Ellis, Shimon Whiteson, Jakob Foerster</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06356v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06356v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy. Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided diffusion. Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline. Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments. Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.</p><h4 id="quantum-state-generation-with-structure-preserving-diffusion-model" tabindex="-1"><a class="header-anchor" href="#quantum-state-generation-with-structure-preserving-diffusion-model"><span>Quantum State Generation with Structure-Preserving Diffusion Model</span></a></h4><p><strong>Authors</strong>: Yuchen Zhu, Tianrong Chen, Evangelos A. Theodorou, Xie Chen, Molei Tao</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06336v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06336v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This article considers the generative modeling of the states of quantum systems, and an approach based on denoising diffusion model is proposed. The key contribution is an algorithmic innovation that respects the physical nature of quantum states. More precisely, the commonly used density matrix representation of mixed-state has to be complex-valued Hermitian, positive semi-definite, and trace one. Generic diffusion models, or other generative methods, may not be able to generate data that strictly satisfy these structural constraints, even if all training data do. To develop a machine learning algorithm that has physics hard-wired in, we leverage the recent development of Mirror Diffusion Model and design a previously unconsidered mirror map, to enable strict structure-preserving generation. Both unconditional generation and conditional generation via classifier-free guidance are experimentally demonstrated efficacious, the latter even enabling the design of new quantum states when generated on unseen labels.</p><h4 id="compensating-slice-emittance-growth-in-high-brightness-photoinjectors-using-sacrificial-charge" tabindex="-1"><a class="header-anchor" href="#compensating-slice-emittance-growth-in-high-brightness-photoinjectors-using-sacrificial-charge"><span>Compensating slice emittance growth in high brightness photoinjectors using sacrificial charge</span></a></h4><p><strong>Authors</strong>: W. H. Li, A. C. Bartnik, A. Fukasawa, M. Kaemingk, G. Lawler, N. Majernik, J. B. Rosenzweig, J. M. Maxson</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06312v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06312v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Achieving maximum electron beam brightness in photoinjectors requires detailed control of the 3D bunch shape and precise tuning of the beam focusing. Even in state-of-the-art designs, slice emittance growth due to nonlinear space charge forces and partial nonlaminarity often remains non-negligible. In this work we introduce a new means to linearize the transverse slice phase space: a sacrificial portion of the bunch&#39;s own charge distribution, formed into a wavebroken shock front by highly nonlinear space charge forces within the gun, whose downstream purpose is to dynamically linearize the desired bunch core. We show that linearization of an appropriately prepared bunch can be achieved via strongly nonlaminar focusing of the sacrificial shock front, while the inner core focuses laminarly. This leads to a natural spatial separation of the two distributions: a dense core surrounded by a diffuse halo of sacrificial charge that can be collimated. Multi-objective genetic algorithm optimizations of the ultra-compact x-ray free electron laser (UCXFEL) injector employ this concept, and we interpret it with an analytic model that agrees well with the simulations. In simulation we demonstrate a final bunch charge of 100 pC, peak current $\sim 30$ A, and a sacrificial charge of 150 pC (250 pC total emitted from cathode) with normalized emittance growth of $&lt;20$ nm-rad due to space charge. This implies a maximum achievable brightness approximately an order of magnitude greater than existing FEL injector designs.</p><h4 id="noisenca-noisy-seed-improves-spatio-temporal-continuity-of-neural-cellular-automata" tabindex="-1"><a class="header-anchor" href="#noisenca-noisy-seed-improves-spatio-temporal-continuity-of-neural-cellular-automata"><span>NoiseNCA: Noisy Seed Improves Spatio-Temporal Continuity of Neural Cellular Automata</span></a></h4><p><strong>Authors</strong>: Ehsan Pajouheshgar, Yitao Xu, Sabine Süsstrunk</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06279v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06279v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Neural Cellular Automata (NCA) is a class of Cellular Automata where the update rule is parameterized by a neural network that can be trained using gradient descent. In this paper, we focus on NCA models used for texture synthesis, where the update rule is inspired by partial differential equations (PDEs) describing reaction-diffusion systems. To train the NCA model, the spatio-termporal domain is discretized, and Euler integration is used to numerically simulate the PDE. However, whether a trained NCA truly learns the continuous dynamic described by the corresponding PDE or merely overfits the discretization used in training remains an open question. We study NCA models at the limit where space-time discretization approaches continuity. We find that existing NCA models tend to overfit the training discretization, especially in the proximity of the initial condition, also called &quot;seed&quot;. To address this, we propose a solution that utilizes uniform noise as the initial condition. We demonstrate the effectiveness of our approach in preserving the consistency of NCA dynamics across a wide range of spatio-temporal granularities. Our improved NCA model enables two new test-time interactions by allowing continuous control over the speed of pattern formation and the scale of the synthesized patterns. We demonstrate this new NCA feature in our interactive online demo. Our work reveals that NCA models can learn continuous dynamics and opens new venues for NCA research from a dynamical systems&#39; perspective.</p><h4 id="the-walled-brauer-category-and-stable-cohomology-of-mathrm-ia-n" tabindex="-1"><a class="header-anchor" href="#the-walled-brauer-category-and-stable-cohomology-of-mathrm-ia-n"><span>The walled Brauer category and stable cohomology of $\mathrm{IA}_n$</span></a></h4><p><strong>Authors</strong>: Erik Lindell</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06263v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06263v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The IA-automorphism group is the group of automorphisms of the free group $F_n$ that act trivially on the abelianization $F_n^{\mathrm{ab}}$. This group is in many ways analoguous to Torelli groups of surfaces and their higher dimensional analogues. In recent work, the stable rational cohomology of such groups was studied by Kupers and Randal-Williams, using the machinery of so-called Brauer categories. In this paper, we adapt their methods to study the stable rational cohomology of the IA-automorphism group. We obtain a conjectural description of the algebraic part of the stable rational cohomology and prove that it holds up to degree $Q+1$, given the assumption that the stable cohomology groups are stably finite dimensional in degrees up to $Q$. In particular, this allows us to compute the algebraic part of the stable cohomology in degree 2, which we show agrees with the part generated by the first cohomology group via the cup product map and which has previously been computed by Pettet. In the appendix, written by Mai Katada, it is shown how the results of the paper can be applied to compute the stable Albanese (co)homology of the IA-automorphism group.</p><h4 id="a-large-scale-simulation-method-for-neuromorphic-circuits" tabindex="-1"><a class="header-anchor" href="#a-large-scale-simulation-method-for-neuromorphic-circuits"><span>A Large-Scale Simulation Method for Neuromorphic Circuits</span></a></h4><p><strong>Authors</strong>: Amir Shahhosseini, Thomas Chaffey, Rodolphe Sepulchre</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06255v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06255v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Splitting algorithms are well-established in convex optimization and are designed to solve large-scale problems. Using such algorithms to simulate the behavior of nonlinear circuit networks provides scalable methods for the simulation and design of neuromorphic systems. For circuits made of linear capacitors and inductors with nonlinear resistive elements, we propose a splitting that breaks the network into its LTI lossless component and its static resistive component. This splitting has both physical and algorithmic advantages and allows for separate calculations in the time domain and in the frequency domain. To demonstrate the scalability of this approach, a network made from one hundred neurons modeled by the well-known FitzHugh-Nagumo circuit with all-to-all diffusive coupling is simulated.</p><h4 id="scrdit-generating-single-cell-rna-seq-data-by-diffusion-transformers-and-accelerating-sampling" tabindex="-1"><a class="header-anchor" href="#scrdit-generating-single-cell-rna-seq-data-by-diffusion-transformers-and-accelerating-sampling"><span>scRDiT: Generating single-cell RNA-seq data by diffusion transformers and accelerating sampling</span></a></h4><p><strong>Authors</strong>: Shengze Dong, Zhuorui Cui, Ding Liu, Jinzhi Lei</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06153v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06153v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Motivation: Single-cell RNA sequencing (scRNA-seq) is a groundbreaking technology extensively utilized in biological research, facilitating the examination of gene expression at the individual cell level within a given tissue sample. While numerous tools have been developed for scRNA-seq data analysis, the challenge persists in capturing the distinct features of such data and replicating virtual datasets that share analogous statistical properties. Results: Our study introduces a generative approach termed scRNA-seq Diffusion Transformer (scRDiT). This method generates virtual scRNA-seq data by leveraging a real dataset. The method is a neural network constructed based on Denoising Diffusion Probabilistic Models (DDPMs) and Diffusion Transformers (DiTs). This involves subjecting Gaussian noises to the real dataset through iterative noise-adding steps and ultimately restoring the noises to form scRNA-seq samples. This scheme allows us to learn data features from actual scRNA-seq samples during model training. Our experiments, conducted on two distinct scRNA-seq datasets, demonstrate superior performance. Additionally, the model sampling process is expedited by incorporating Denoising Diffusion Implicit Models (DDIM). scRDiT presents a unified methodology empowering users to train neural network models with their unique scRNA-seq datasets, enabling the generation of numerous high-quality scRNA-seq samples. Availability and implementation: https://github.com/DongShengze/scRDiT</p><h4 id="diffharmony-latent-diffusion-model-meets-image-harmonization" tabindex="-1"><a class="header-anchor" href="#diffharmony-latent-diffusion-model-meets-image-harmonization"><span>DiffHarmony: Latent Diffusion Model Meets Image Harmonization</span></a></h4><p><strong>Authors</strong>: Pengfei Zhou, Fangxiang Feng, Xiaojie Wang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06139v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06139v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Image harmonization, which involves adjusting the foreground of a composite image to attain a unified visual consistency with the background, can be conceptualized as an image-to-image translation task. Diffusion models have recently promoted the rapid development of image-to-image translation tasks . However, training diffusion models from scratch is computationally intensive. Fine-tuning pre-trained latent diffusion models entails dealing with the reconstruction error induced by the image compression autoencoder, making it unsuitable for image generation tasks that involve pixel-level evaluation metrics. To deal with these issues, in this paper, we first adapt a pre-trained latent diffusion model to the image harmonization task to generate the harmonious but potentially blurry initial images. Then we implement two strategies: utilizing higher-resolution images during inference and incorporating an additional refinement stage, to further enhance the clarity of the initially harmonized images. Extensive experiments on iHarmony4 datasets demonstrate the superiority of our proposed method. The code and model will be made publicly available at https://github.com/nicecv/DiffHarmony .</p><h4 id="a-multi-phase-thermo-mechanical-model-for-rock-ice-avalanche" tabindex="-1"><a class="header-anchor" href="#a-multi-phase-thermo-mechanical-model-for-rock-ice-avalanche"><span>A multi-phase thermo-mechanical model for rock-ice avalanche</span></a></h4><p><strong>Authors</strong>: Shiva P. Pudasaini</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06130v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06130v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We propose a novel physically-based multi-phase thermo-mechanical model for rock-ice avalanche. The model is built on a multi-phase mass flow model and extends a two-phase rock-ice avalanche model. It considers rock, ice and fluid; includes the mechanism of ice-melting and a rigorously derived dynamically changing general temperature equation for avalanching bulk mass, the first of its kind. It explains advection-diffusion of heat including the heat exchange across the rock-ice avalanche body, basal heat conduction, production and loss of heat due to frictional shearing and changing temperature, a general formulation of the ice melting rate and enhancement of temperature due to basal entrainment. The temperature equation includes a composite term containing coupled dynamics: rate of change of thermal conductivity and temperature. Ice melt intensity determines these rates as mixture conductivity evolves, characterizing distinctive thermo-mechanical processes. The model highlights essential aspects of rock-ice avalanches. Lateral heat productions play an important role in temperature evolution. Fast moving avalanches produce higher amount of heat. Fast ice melting results in substantial change in temperature. We formally derive the melting efficiency dependent general fluid production rate. The model includes internal mass and momentum exchanges between the phases and mass and momentum productions due to entrainment. The latter significantly changes the state of temperature; yet, the former exclusively characterizes rock-ice avalanche. Temperature changes are rapid when heat entrainment across the avalanche boundary is substantial. It also applies to basal heat conduction. A strong coupling exists between phase mass and momentum balances and the temperature equation. The new model offers the first-ever complete dynamical solution for simulating rock-ice avalanche with changing temperature.</p><h4 id="hash3d-training-free-acceleration-for-3d-generation" tabindex="-1"><a class="header-anchor" href="#hash3d-training-free-acceleration-for-3d-generation"><span>Hash3D: Training-free Acceleration for 3D Generation</span></a></h4><p><strong>Authors</strong>: Xingyi Yang, Xinchao Wang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06091v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06091v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model&#39;s inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D&#39;s versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D&#39;s integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at https://adamdad.github.io/hash3D/.</p><h4 id="greedy-dim-greedy-algorithms-for-unreasonably-effective-face-morphs" tabindex="-1"><a class="header-anchor" href="#greedy-dim-greedy-algorithms-for-unreasonably-effective-face-morphs"><span>Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs</span></a></h4><p><strong>Authors</strong>: Zander W. Blasingame, Chen Liu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06025v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06025v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Morphing attacks are an emerging threat to state-of-the-art Face Recognition (FR) systems, which aim to create a single image that contains the biometric information of multiple identities. Diffusion Morphs (DiM) are a recently proposed morphing attack that has achieved state-of-the-art performance for representation-based morphing attacks. However, none of the existing research on DiMs have leveraged the iterative nature of DiMs and left the DiM model as a black box, treating it no differently than one would a Generative Adversarial Network (GAN) or Varational AutoEncoder (VAE). We propose a greedy strategy on the iterative sampling process of DiM models which searches for an optimal step guided by an identity-based heuristic function. We compare our proposed algorithm against ten other state-of-the-art morphing algorithms using the open-source SYN-MAD 2022 competition dataset. We find that our proposed algorithm is unreasonably effective, fooling all of the tested FR systems with an MMPMR of 100%, outperforming all other morphing algorithms compared.</p><h4 id="diffusion-based-point-cloud-super-resolution-for-mmwave-radar-data" tabindex="-1"><a class="header-anchor" href="#diffusion-based-point-cloud-super-resolution-for-mmwave-radar-data"><span>Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data</span></a></h4><p><strong>Authors</strong>: Kai Luan, Chenghao Shi, Neng Wang, Yuwei Cheng, Huimin Lu, Xieyuanli Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.06012v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.06012v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The millimeter-wave radar sensor maintains stable performance under adverse environmental conditions, making it a promising solution for all-weather perception tasks, such as outdoor mobile robotics. However, the radar point clouds are relatively sparse and contain massive ghost points, which greatly limits the development of mmWave radar technology. In this paper, we propose a novel point cloud super-resolution approach for 3D mmWave radar data, named Radar-diffusion. Our approach employs the diffusion model defined by mean-reverting stochastic differential equations(SDE). Using our proposed new objective function with supervision from corresponding LiDAR point clouds, our approach efficiently handles radar ghost points and enhances the sparse mmWave radar point clouds to dense LiDAR-like point clouds. We evaluate our approach on two different datasets, and the experimental results show that our method outperforms the state-of-the-art baseline methods in 3D radar super-resolution tasks. Furthermore, we demonstrate that our enhanced radar point cloud is capable of downstream radar point-based registration tasks.</p><h4 id="tackling-structural-hallucination-in-image-translation-with-local-diffusion" tabindex="-1"><a class="header-anchor" href="#tackling-structural-hallucination-in-image-translation-with-local-diffusion"><span>Tackling Structural Hallucination in Image Translation with Local Diffusion</span></a></h4><p><strong>Authors</strong>: Seunghoi Kim, Chen Jin, Tom Diethe, Matteo Figini, Henry F. J. Tregidgo, Asher Mullokandov, Philip Teare, Daniel C. Alexander</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05980v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05980v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recent developments in diffusion models have advanced conditioned image generation, yet they struggle with reconstructing out-of-distribution (OOD) images, such as unseen tumors in medical images, causing <code>image hallucination&#39;&#39; and risking misdiagnosis. We hypothesize such hallucinations result from local OOD regions in the conditional images. We verify that partitioning the OOD region and conducting separate image generations alleviates hallucinations in several applications. From this, we propose a training-free diffusion framework that reduces hallucination with multiple Local Diffusion processes. Our approach involves OOD estimation followed by two modules: a </code>branching&#39;&#39; module generates locally both within and outside OOD regions, and a ``fusion&#39;&#39; module integrates these predictions into one. Our evaluation shows our method mitigates hallucination over baseline models quantitatively and qualitatively, reducing misdiagnosis by 40% and 25% in the real-world medical and natural image datasets, respectively. It also demonstrates compatibility with various pre-trained diffusion models.</p><h4 id="passive-non-line-of-sight-imaging-of-moving-targets-using-physical-embedding-and-event-based-vision" tabindex="-1"><a class="header-anchor" href="#passive-non-line-of-sight-imaging-of-moving-targets-using-physical-embedding-and-event-based-vision"><span>Passive Non-line-of-sight imaging of moving targets using Physical embedding and Event-based vision</span></a></h4><p><strong>Authors</strong>: Conghe Wang, Xia Wang, Yujie Fang, Changda Yan, Xin Zhang, Yifan Zuo</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05977v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05977v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Passive Non-line-of-sight (NLOS) imaging has shown promising applications in imaging occluded objects around corners. However, this inverse problem is highly ill-posed and results in poor reconstruction with traditional physical retrieval methods, particularly in moving target imaging. With the development of neural networks, data-driven methods have greatly improved accuracy, however, heavy reliance on data volume has put great pressure on data collection and dataset fabrication. We propose a physical embedded passive NLOS imaging prototype with event-based vision (PNPE), which induces an event camera for feature extraction of dynamic diffusion spot and leverages simulation dataset to pre-train the physical embedded model before fine-tuning with limited real-shot data. The proposed PNPE is verified by simulation and real-world experiments, and the comparisons of data paradigms also validate the superiority of event-based vision in passive NLOS imaging for moving targets.</p><h4 id="map-optical-properties-to-subwavelength-structures-directly-via-a-diffusion-model" tabindex="-1"><a class="header-anchor" href="#map-optical-properties-to-subwavelength-structures-directly-via-a-diffusion-model"><span>Map Optical Properties to Subwavelength Structures Directly via a Diffusion Model</span></a></h4><p><strong>Authors</strong>: Shijie Rao, Kaiyu Cui, Yidong Huang, Jiawei Yang, Yali Li, Shengjin Wang, Xue Feng, Fang Liu, Wei Zhang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05959v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05959v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Subwavelength photonic structures and metamaterials provide revolutionary approaches for controlling light. The inverse design methods proposed for these subwavelength structures are vital to the development of new photonic devices. However, most of the existing inverse design methods cannot realize direct mapping from optical properties to photonic structures but instead rely on forward simulation methods to perform iterative optimization. In this work, we exploit the powerful generative abilities of artificial intelligence (AI) and propose a practical inverse design method based on latent diffusion models. Our method maps directly the optical properties to structures without the requirement of forward simulation and iterative optimization. Here, the given optical properties can work as &quot;prompts&quot; and guide the constructed model to correctly &quot;draw&quot; the required photonic structures. Experiments show that our direct mapping-based inverse design method can generate subwavelength photonic structures at high fidelity while following the given optical properties. This may change the method used for optical design and greatly accelerate the research on new photonic devices.</p><h2 id="_2024-04-08" tabindex="-1"><a class="header-anchor" href="#_2024-04-08"><span>2024-04-08</span></a></h2><h4 id="numerical-validation-of-scaling-laws-for-stratified-turbulence" tabindex="-1"><a class="header-anchor" href="#numerical-validation-of-scaling-laws-for-stratified-turbulence"><span>Numerical validation of scaling laws for stratified turbulence</span></a></h4><p><strong>Authors</strong>: Pascale Garaud, Greg P. Chini, Laura Cope, Kasturi Shah, Colm-cille P. Caulfield</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05896v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05896v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recent theoretical progress using multiscale asymptotic analysis has revealed various possible regimes of stratified turbulence. Notably, buoyancy transport can either be dominated by advection or diffusion, depending on the effective P&#39;eclet number of the flow. Two types of asymptotic models have been proposed, which yield measurably different predictions for the characteristic vertical velocity and length scale of the turbulent eddies in both diffusive and non-diffusive regimes. The first, termed a <code>single-scale model&#39;, is designed to describe flow structures having large horizontal and small vertical scales, while the second, termed a </code>multiscale model&#39;, additionally incorporates flow features with small horizontal scales, and reduces to the single-scale model in their absence. By comparing predicted vertical velocity scaling laws with direct numerical simulation data, we show that the multiscale model correctly captures the properties of strongly stratified turbulence within spatiotemporally-intermittent turbulent patches. Meanwhile its single-scale reduction accurately describes the more orderly layer-like flow outside those patches.</p><h4 id="spin-free-exact-two-component-linear-response-coupled-cluster-theory-for-estimation-of-frequency-dependent-second-order-property" tabindex="-1"><a class="header-anchor" href="#spin-free-exact-two-component-linear-response-coupled-cluster-theory-for-estimation-of-frequency-dependent-second-order-property"><span>Spin-free exact two-component linear response coupled cluster theory for estimation of frequency-dependent second-order property</span></a></h4><p><strong>Authors</strong>: Sudipta Chakraborty, Tamoghna Mukhopadhyay, Achintya Kumar Dutta</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05869v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05869v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: We have presented the theory, implementation, and benchmark results for spin-free exact two-component (SFX2C) linear response coupled cluster (LRCCSD) theory for static and dynamic polarizabilities of atoms and molecules. The resolution of identity (RI) approximation for two-electron integrals has been used to reduce the computational cost of the calculation and has been shown to have a negligible effect on accuracy. The calculated static and dynamic polarizability values agree very well with the more expensive X2C-LR-CCSD results. Our calculated results show that accurate predictions of polarizabilities of atoms and molecules containing heavy atoms require the use of a large basis set containing an adequate number of diffuse functions, in addition to accounting for electron correlation and relativistic effects.</p><h4 id="the-increasing-fragmentation-of-global-science-limits-the-diffusion-of-ideas" tabindex="-1"><a class="header-anchor" href="#the-increasing-fragmentation-of-global-science-limits-the-diffusion-of-ideas"><span>The increasing fragmentation of global science limits the diffusion of ideas</span></a></h4><p><strong>Authors</strong>: Alexander J. Gates, Indraneel Mane, Jianjian Gao</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05861v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05861v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The global scientific landscape emerges from a complex interplay of collaboration and competition, where nations vie for dominance while simultaneously fostering the diffusion of knowledge on a global scale. This raises crucial questions: What underlying patterns govern international scientific recognition and influence? How does this structure impact knowledge dissemination? Traditional models view the global scientific ecosystem through a core-periphery lens, with Western nations dominating knowledge production. Here, we investigate the dynamics of international scientific recognition through the lens of national preferences, introducing a novel signed measure to characterize national citation preferences and enabling a network analysis of international scientific recognition. We find that scientific recognition is related to cultural and political factors in addition to economic strength and scientific quality. Our analysis challenges the conventional core-periphery narrative, uncovering instead several communities of international knowledge production that are rapidly fragmenting the scientific recognition ecosystem. Moreover, we provide compelling evidence that this network significantly constrains the diffusion of ideas across international borders. The resulting network framework for global scientific recognition sheds light on the barriers and opportunities for collaboration, innovation, and the equitable recognition of scientific advancements, with significant consequences for policymakers seeking to foster inclusive and impactful international scientific endeavors.</p><h4 id="electrical-control-of-superconducting-spin-valves-using-ferromagnetic-helices" tabindex="-1"><a class="header-anchor" href="#electrical-control-of-superconducting-spin-valves-using-ferromagnetic-helices"><span>Electrical control of superconducting spin valves using ferromagnetic helices</span></a></h4><p><strong>Authors</strong>: Tancredi Salamone, Henning G. Hugdal, Morten Amundsen, Sol H. Jacobsen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05798v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05798v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The geometrical properties of a helical ferromagnet are shown theoretically to control the critical temperature of a proximity-coupled superconductor. Using the Usadel equation for diffusive spin transport, we provide self-consistent analysis of how curvature and torsion modulate the proximity effect. When the helix is attached to a piezoelectric actuator, the pitch of the helix -- and hence the superconducting transition -- can be controlled electrically.</p><h4 id="the-neutrino-background-from-non-jetted-active-galactic-nuclei" tabindex="-1"><a class="header-anchor" href="#the-neutrino-background-from-non-jetted-active-galactic-nuclei"><span>The neutrino background from non-jetted active galactic nuclei</span></a></h4><p><strong>Authors</strong>: P. Padovani, R. Gilli, E. Resconi, C. Bellenghi, F. Henningsen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05690v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05690v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Aims. We calculate the contribution to the neutrino background from the non-jetted active galactic nuclei (AGN) population following the recent IceCube association of TeV neutrinos with NGC 1068. Methods. We exploit our robust knowledge of the AGN X-ray luminosity function and evolution and convert it to the neutrino band by using NGC 1068 as a benchmark and a theoretically motivated neutrino spectrum. Results. The resulting neutrino background up to redshift 5 does not violate either the IceCube diffuse flux or the upper bounds for non-jetted AGN, although barely so. This is consistent with a scenario where the latter class makes a substantial contribution mostly below 1 PeV, while jetted AGN, i.e. blazars, dominate above this energy, in intriguing agreement with the dip in the neutrino data at ~ 300 TeV. More and better IceCube data on Seyfert galaxies will allow us to constrain the fraction of neutrino emitters among non-jetted AGN.</p><h4 id="spherehead-stable-3d-full-head-synthesis-with-spherical-tri-plane-representation" tabindex="-1"><a class="header-anchor" href="#spherehead-stable-3d-full-head-synthesis-with-spherical-tri-plane-representation"><span>SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane Representation</span></a></h4><p><strong>Authors</strong>: Heyuan Li, Ce Chen, Tianhao Shi, Yuda Qiu, Sizhe An, Guanying Chen, Xiaoguang Han</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05680v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05680v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: While recent advances in 3D-aware Generative Adversarial Networks (GANs) have aided the development of near-frontal view human face synthesis, the challenge of comprehensively synthesizing a full 3D head viewable from all angles still persists. Although PanoHead proves the possibilities of using a large-scale dataset with images of both frontal and back views for full-head synthesis, it often causes artifacts for back views. Based on our in-depth analysis, we found the reasons are mainly twofold. First, from network architecture perspective, we found each plane in the utilized tri-plane/tri-grid representation space tends to confuse the features from both sides, causing &quot;mirroring&quot; artifacts (e.g., the glasses appear in the back). Second, from data supervision aspect, we found that existing discriminator training in 3D GANs mainly focuses on the quality of the rendered image itself, and does not care much about its plausibility with the perspective from which it was rendered. This makes it possible to generate &quot;face&quot; in non-frontal views, due to its easiness to fool the discriminator. In response, we propose SphereHead, a novel tri-plane representation in the spherical coordinate system that fits the human head&#39;s geometric characteristics and efficiently mitigates many of the generated artifacts. We further introduce a view-image consistency loss for the discriminator to emphasize the correspondence of the camera parameters and the images. The combination of these efforts results in visually superior outcomes with significantly fewer artifacts. Our code and dataset are publicly available at https://lhyfst.github.io/spherehead.</p><h4 id="moma-multimodal-llm-adapter-for-fast-personalized-image-generation" tabindex="-1"><a class="header-anchor" href="#moma-multimodal-llm-adapter-for-fast-personalized-image-generation"><span>MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</span></a></h4><p><strong>Authors</strong>: Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2404.05674v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2404.05674v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements.</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><!----><!----></div></footer><!----><!--[--><!--[--><div class="my-footer">Copyright © 2024-present OpenDesign Community</div><!--]--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-b0nPsb4c.js" defer></script>
  </body>
</html>
