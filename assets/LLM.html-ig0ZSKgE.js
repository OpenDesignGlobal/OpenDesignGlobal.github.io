import{_ as i,r as o,o as s,c as r,a as e,b as t,d as a,e as l}from"./app-DmGAXvh9.js";const h={},c=l('<h1 id="llm" tabindex="-1"><a class="header-anchor" href="#llm"><span>LLM</span></a></h1><h2 id="_2024-04-04" tabindex="-1"><a class="header-anchor" href="#_2024-04-04"><span>2024-04-04</span></a></h2><h4 id="training-llms-over-neurally-compressed-text" tabindex="-1"><a class="header-anchor" href="#training-llms-over-neurally-compressed-text"><span>Training LLMs over Neurally Compressed Text</span></a></h4><p><strong>Authors</strong>: Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant</p>',4),d=e("strong",null,"Link",-1),u={href:"http://arxiv.org/abs/2404.03626v1",target:"_blank",rel:"noopener noreferrer"},g=e("p",null,[e("strong",null,"Abstract"),t(': In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.')],-1),p=e("h4",{id:"unveiling-llms-the-evolution-of-latent-representations-in-a-temporal-knowledge-graph",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unveiling-llms-the-evolution-of-latent-representations-in-a-temporal-knowledge-graph"},[e("span",null,"Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph")])],-1),m=e("p",null,[e("strong",null,"Authors"),t(": Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini")],-1),f=e("strong",null,"Link",-1),v={href:"http://arxiv.org/abs/2404.03623v1",target:"_blank",rel:"noopener noreferrer"},b=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of common factual knowledge information. However, unravelling the underlying reasoning of LLMs and explaining their internal mechanisms of exploiting this factual knowledge remain active areas of investigation. Our work analyzes the factual knowledge encoded in the latent representation of LLMs when prompted to assess the truthfulness of factual claims. We propose an end-to-end framework that jointly decodes the factual knowledge embedded in the latent space of LLMs from a vector space to a set of ground predicates and represents its evolution across the layers using a temporal knowledge graph. Our framework relies on the technique of activation patching which intervenes in the inference computation of a model by dynamically altering its latent representations. Consequently, we neither rely on external models nor training processes. We showcase our framework with local and global interpretability analyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The local interpretability analysis exposes different latent errors from representation to multi-hop reasoning errors. On the other hand, the global analysis uncovered patterns in the underlying evolution of the model's factual knowledge (e.g., store-and-seek factual information). By enabling graph-based analyses of the latent representations, this work represents a step towards the mechanistic interpretability of LLMs.")],-1),y=e("h4",{id:"evaluating-llms-at-detecting-errors-in-llm-responses",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evaluating-llms-at-detecting-errors-in-llm-responses"},[e("span",null,"Evaluating LLMs at Detecting Errors in LLM Responses")])],-1),L=e("p",null,[e("strong",null,"Authors"),t(": Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang")],-1),w=e("strong",null,"Link",-1),_={href:"http://arxiv.org/abs/2404.03602v1",target:"_blank",rel:"noopener noreferrer"},k=e("p",null,[e("strong",null,"Abstract"),t(": With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial. However, little research has been conducted on error detection of LLM responses. Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization). This work introduces ReaLMistake, the first error detection benchmark consisting of objective, realistic, and diverse errors made by LLMs. ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories (reasoning correctness, instruction-following, context-faithfulness, and parameterized knowledge), eliciting naturally observed and diverse errors in responses of GPT-4 and Llama 2 70B annotated by experts. We use ReaLMistake to evaluate error detectors based on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect errors made by LLMs at very low recall, and all LLM-based error detectors perform much worse than humans. 2) Explanations by LLM-based error detectors lack reliability. 3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve. 4) Popular approaches to improving LLMs, including self-consistency and majority vote, do not improve the error detection performance. Our benchmark and code are provided at https://github.com/psunlpgroup/ReaLMistake.")],-1),x=e("h4",{id:"personalized-llm-response-generation-with-parameterized-memory-injection",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#personalized-llm-response-generation-with-parameterized-memory-injection"},[e("span",null,"Personalized LLM Response Generation with Parameterized Memory Injection")])],-1),M=e("p",null,[e("strong",null,"Authors"),t(": Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu")],-1),A=e("strong",null,"Link",-1),T={href:"http://arxiv.org/abs/2404.03565v1",target:"_blank",rel:"noopener noreferrer"},S=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \\textbf{L}LM \\textbf{P}ersonalization(\\textbf{MiLP}).")],-1),C=e("h4",{id:"minigpt4-video-advancing-multimodal-llms-for-video-understanding-with-interleaved-visual-textual-tokens",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#minigpt4-video-advancing-multimodal-llms-for-video-understanding-with-interleaved-visual-textual-tokens"},[e("span",null,"MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens")])],-1),I=e("p",null,[e("strong",null,"Authors"),t(": Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny")],-1),q=e("strong",null,"Link",-1),z={href:"http://arxiv.org/abs/2404.03413v1",target:"_blank",rel:"noopener noreferrer"},W=e("p",null,[e("strong",null,"Abstract"),t(": This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/")],-1),P=e("h4",{id:"do-large-language-models-rank-fairly-an-empirical-study-on-the-fairness-of-llms-as-rankers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#do-large-language-models-rank-fairly-an-empirical-study-on-the-fairness-of-llms-as-rankers"},[e("span",null,"Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers")])],-1),E=e("p",null,[e("strong",null,"Authors"),t(": Yuan Wang, Xuyang Wu, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang")],-1),G=e("strong",null,"Link",-1),R={href:"http://arxiv.org/abs/2404.03192v1",target:"_blank",rel:"noopener noreferrer"},H=e("p",null,[e("strong",null,"Abstract"),t(": The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works (e.g., RankGPT) have also demonstrated that the LLMs exhibit better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.")],-1),D=e("h4",{id:"robust-pronoun-use-fidelity-with-english-llms-are-they-reasoning-repeating-or-just-biased",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robust-pronoun-use-fidelity-with-english-llms-are-they-reasoning-repeating-or-just-biased"},[e("span",null,"Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?")])],-1),F=e("p",null,[e("strong",null,"Authors"),t(": Vagrant Gautam, Eileen Bingert, Dawei Zhu, Anne Lauscher, Dietrich Klakow")],-1),B=e("strong",null,"Link",-1),O={href:"http://arxiv.org/abs/2404.03134v1",target:"_blank",rel:"noopener noreferrer"},j=e("p",null,[e("strong",null,"Abstract"),t(": Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these components at a time. To measure progress towards the combined goal, we introduce the task of pronoun use fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later, independent of potential distractors. We present a carefully-designed dataset of over 5 million instances to evaluate pronoun use fidelity in English, and we use it to evaluate 37 popular large language models across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters). We find that while models can mostly faithfully reuse previously-specified pronouns in the presence of no distractors, they are significantly worse at processing she/her/her, singular they and neopronouns. Additionally, models are not robustly faithful to pronouns, as they are easily distracted. With even one additional sentence containing a distractor pronoun, accuracy drops on average by 34%. With 5 distractor sentences, accuracy drops by 52% for decoder-only models and 13% for encoder-only models. We show that widely-used large language models are still brittle, with large gaps in reasoning and in processing different pronouns in a setting that is very simple for humans, and we encourage researchers in bias and reasoning to bridge them.")],-1),Z=e("h4",{id:"towards-standards-compliant-assistive-technology-product-specifications-via-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-standards-compliant-assistive-technology-product-specifications-via-llms"},[e("span",null,"Towards Standards-Compliant Assistive Technology Product Specifications via LLMs")])],-1),N=e("p",null,[e("strong",null,"Authors"),t(": Chetan Arora, John Grundy, Louise Puli, Natasha Layton")],-1),Y=e("strong",null,"Link",-1),J={href:"http://arxiv.org/abs/2404.03122v1",target:"_blank",rel:"noopener noreferrer"},K=e("p",null,[e("strong",null,"Abstract"),t(": In the rapidly evolving field of assistive technology (AT), ensuring that products meet national and international standards is essential for user safety, efficacy, and accessibility. In this vision paper, we introduce CompliAT, a pioneering framework designed to streamline the compliance process of AT product specifications with these standards through the innovative use of Large Language Models (LLMs). CompliAT addresses three critical tasks: checking terminology consistency, classifying products according to standards, and tracing key product specifications to standard requirements. We tackle the challenge of terminology consistency to ensure that the language used in product specifications aligns with relevant standards, reducing misunderstandings and non-compliance risks. We propose a novel approach for product classification, leveraging a retrieval-augmented generation model to accurately categorize AT products aligning to international standards, despite the sparse availability of training data. Finally, CompliAT implements a traceability and compliance mechanism from key product specifications to standard requirements, ensuring all aspects of an AT product are thoroughly vetted against the corresponding standards. By semi-automating these processes, CompliAT aims to significantly reduce the time and effort required for AT product standards compliance and uphold quality and safety standards. We outline our planned implementation and evaluation plan for CompliAT.")],-1),V=e("h2",{id:"_2024-04-03",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-03"},[e("span",null,"2024-04-03")])],-1),U=e("h4",{id:"the-artificial-intelligence-ontology-llm-assisted-construction-of-ai-concept-hierarchies",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-artificial-intelligence-ontology-llm-assisted-construction-of-ai-concept-hierarchies"},[e("span",null,"The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies")])],-1),Q=e("p",null,[e("strong",null,"Authors"),t(": Marcin P. Joachimiak, Mark A. Miller, J. Harry Caufield, Ryan Ly, Nomi L. Harris, Andrew Tritt, Christopher J. Mungall, Kristofer E. Bouchard")],-1),X=e("strong",null,"Link",-1),$={href:"http://arxiv.org/abs/2404.03044v1",target:"_blank",rel:"noopener noreferrer"},ee=e("p",null,[e("strong",null,"Abstract"),t(": The Artificial Intelligence Ontology (AIO) is a systematization of artificial intelligence (AI) concepts, methodologies, and their interrelations. Developed via manual curation, with the additional assistance of large language models (LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a comprehensive framework that encompasses both technical and ethical aspects of AI technologies. The primary audience for AIO includes AI researchers, developers, and educators seeking standardized terminology and concepts within the AI domain. The ontology is structured around six top-level branches: Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to support the modular composition of AI methods and facilitate a deeper understanding of deep learning architectures and ethical considerations in AI. AIO's development utilized the Ontology Development Kit (ODK) for its creation and maintenance, with its content being dynamically updated through AI-driven curation support. This approach not only ensures the ontology's relevance amidst the fast-paced advancements in AI but also significantly enhances its utility for researchers, developers, and educators by simplifying the integration of new AI concepts and methodologies. The ontology's utility is demonstrated through the annotation of AI methods data in a catalog of AI research publications and the integration into the BioPortal ontology resource, highlighting its potential for cross-disciplinary research. The AIO ontology is open source and is available on GitHub (https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal (https://bioportal.bioontology.org/ontologies/AIO).")],-1),te=e("h4",{id:"i-design-personalized-llm-interior-designer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#i-design-personalized-llm-interior-designer"},[e("span",null,"I-Design: Personalized LLM Interior Designer")])],-1),ne=e("p",null,[e("strong",null,"Authors"),t(": Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, Xi Wang")],-1),ae=e("strong",null,"Link",-1),ie={href:"http://arxiv.org/abs/2404.02838v1",target:"_blank",rel:"noopener noreferrer"},oe=e("p",null,[e("strong",null,"Abstract"),t(": Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality. However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury. To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication. I-Design starts with a team of large language model agents that engage in dialogues and logical reasoning with one another, transforming textual user input into feasible scene graph designs with relative object relationships. Subsequently, an effective placement algorithm determines optimal locations for each object within the scene. The final design is then constructed in 3D by retrieving and integrating assets from an existing object database. Additionally, we propose a new evaluation protocol that utilizes a vision-language model and complements the design pipeline. Extensive quantitative and qualitative experiments show that I-Design outperforms existing methods in delivering high-quality 3D design solutions and aligning with abstract concepts that match user input, showcasing its advantages across detailed 3D arrangement and conceptual fidelity.")],-1),se=e("h4",{id:"aqua-combining-experts-and-non-experts-views-to-assess-deliberation-quality-in-online-discussions-using-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#aqua-combining-experts-and-non-experts-views-to-assess-deliberation-quality-in-online-discussions-using-llms"},[e("span",null,"AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs")])],-1),re=e("p",null,[e("strong",null,"Authors"),t(": Maike Behrendt, Stefan Sylvius Wagner, Marc Ziegele, Lena Wilms, Anke Stoll, Dominique Heinbach, Stefan Harmeling")],-1),le=e("strong",null,"Link",-1),he={href:"http://arxiv.org/abs/2404.02761v1",target:"_blank",rel:"noopener noreferrer"},ce=e("p",null,[e("strong",null,"Abstract"),t(": Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices into a single deliberative score. We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not be seen during training. The analysis of experts' vs. non-experts' annotations confirms theoretical findings in the social science literature.")],-1),de=e("h4",{id:"unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm"},[e("span",null,"Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM")])],-1),ue=e("p",null,[e("strong",null,"Authors"),t(": Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Yuekai Huang, Jun Hu, Qing Wang")],-1),ge=e("strong",null,"Link",-1),pe={href:"http://arxiv.org/abs/2404.02706v1",target:"_blank",rel:"noopener noreferrer"},me=e("p",null,[e("strong",null,"Abstract"),t(": Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text. These issues are mostly caused by developers' lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI.")],-1),fe=e("h4",{id:"improving-topic-relevance-model-by-mix-structured-summarization-and-llm-based-data-augmentation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#improving-topic-relevance-model-by-mix-structured-summarization-and-llm-based-data-augmentation"},[e("span",null,"Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation")])],-1),ve=e("p",null,[e("strong",null,"Authors"),t(": Yizhu Liu, Ran Tao, Shengyu Guo, Yifan Yang")],-1),be=e("strong",null,"Link",-1),ye={href:"http://arxiv.org/abs/2404.02616v1",target:"_blank",rel:"noopener noreferrer"},Le=e("p",null,[e("strong",null,"Abstract"),t(": Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user's requirement. In most social search scenarios such as Dianping, modeling search relevance always faces two challenges. One is that many documents in social search are very long and have much redundant information. The other is that the training data for search relevance model is difficult to get, especially for multi-classification relevance model. To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document. Then, we utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query from queries and documents in existing training data, which can construct new query-document pairs as training data. Extensive offline experiments and online A/B tests show that the proposed approaches effectively improve the performance of relevance modeling.")],-1),we=e("h4",{id:"learn-to-disguise-avoid-refusal-responses-in-llm-s-defense-via-a-multi-agent-attacker-disguiser-game",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learn-to-disguise-avoid-refusal-responses-in-llm-s-defense-via-a-multi-agent-attacker-disguiser-game"},[e("span",null,"Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game")])],-1),_e=e("p",null,[e("strong",null,"Authors"),t(": Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng Liu, Dongsheng Li")],-1),ke=e("strong",null,"Link",-1),xe={href:"http://arxiv.org/abs/2404.02532v1",target:"_blank",rel:"noopener noreferrer"},Me=e("p",null,[e("strong",null,"Abstract"),t(": With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks. After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents. The experiments verify that the method in this paper is more effective in strengthening the model's ability to disguise the defense intent compared with other methods. Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations.")],-1),Ae=e("h4",{id:"utebc-nlp-at-semeval-2024-task-9-can-llms-be-lateral-thinkers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#utebc-nlp-at-semeval-2024-task-9-can-llms-be-lateral-thinkers"},[e("span",null,"uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?")])],-1),Te=e("p",null,[e("strong",null,"Authors"),t(": Pouya Sadeghi, Amirhossein Abaskohi, Yadollah Yaghoobzadeh")],-1),Se=e("strong",null,"Link",-1),Ce={href:"http://arxiv.org/abs/2404.02474v1",target:"_blank",rel:"noopener noreferrer"},Ie=e("p",null,[e("strong",null,"Abstract"),t(": Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. Furthermore, fine-tuning Zephyr on our dataset enhances performance across other commonsense datasets, underscoring the value of innovative thinking.")],-1),qe=e("h4",{id:"enhancing-low-resource-llms-classification-with-peft-and-synthetic-data",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-low-resource-llms-classification-with-peft-and-synthetic-data"},[e("span",null,"Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data")])],-1),ze=e("p",null,[e("strong",null,"Authors"),t(": Parth Patwa, Simone Filice, Zhiyu Chen, Giuseppe Castellucci, Oleg Rokhlenko, Shervin Malmasi")],-1),We=e("strong",null,"Link",-1),Pe={href:"http://arxiv.org/abs/2404.02422v1",target:"_blank",rel:"noopener noreferrer"},Ee=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.")],-1),Ge=e("h2",{id:"_2024-04-02",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-02"},[e("span",null,"2024-04-02")])],-1),Re=e("h4",{id:"constrained-robotic-navigation-on-preferred-terrains-using-llms-and-speech-instruction-exploiting-the-power-of-adverbs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#constrained-robotic-navigation-on-preferred-terrains-using-llms-and-speech-instruction-exploiting-the-power-of-adverbs"},[e("span",null,"Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs")])],-1),He=e("p",null,[e("strong",null,"Authors"),t(": Faraz Lotfi, Farnoosh Faraji, Nikhil Kakodkar, Travis Manderson, David Meger, Gregory Dudek")],-1),De=e("strong",null,"Link",-1),Fe={href:"http://arxiv.org/abs/2404.02294v1",target:"_blank",rel:"noopener noreferrer"},Be=e("p",null,[e("strong",null,"Abstract"),t(": This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.")],-1),Oe=e("h4",{id:"llms-in-the-loop-leveraging-large-language-model-annotations-for-active-learning-in-low-resource-languages",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-in-the-loop-leveraging-large-language-model-annotations-for-active-learning-in-low-resource-languages"},[e("span",null,"LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages")])],-1),je=e("p",null,[e("strong",null,"Authors"),t(": Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah Gumus, Michael Granitzer")],-1),Ze=e("strong",null,"Link",-1),Ne={href:"http://arxiv.org/abs/2404.02261v1",target:"_blank",rel:"noopener noreferrer"},Ye=e("p",null,[e("strong",null,"Abstract"),t(": Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential cost savings of at least 42.45 times compared to human annotation. Our proposed solution shows promising potential to substantially reduce both the monetary and computational costs associated with automation in low-resource settings. By bridging the gap between low-resource languages and AI, this approach fosters broader inclusion and shows the potential to enable automation across diverse linguistic landscapes.")],-1),Je=e("h4",{id:"jailbreaking-leading-safety-aligned-llms-with-simple-adaptive-attacks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#jailbreaking-leading-safety-aligned-llms-with-simple-adaptive-attacks"},[e("span",null,"Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks")])],-1),Ke=e("p",null,[e("strong",null,"Authors"),t(": Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion")],-1),Ve=e("strong",null,"Link",-1),Ue={href:"http://arxiv.org/abs/2404.02151v1",target:"_blank",rel:"noopener noreferrer"},Qe=e("p",null,[e("strong",null,"Abstract"),t(`: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). We provide the code, prompts, and logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.`)],-1),Xe=e("h4",{id:"topic-based-watermarks-for-llm-generated-text",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#topic-based-watermarks-for-llm-generated-text"},[e("span",null,"Topic-based Watermarks for LLM-Generated Text")])],-1),$e=e("p",null,[e("strong",null,"Authors"),t(": Alexander Nemecek, Yuzhou Jiang, Erman Ayday")],-1),et=e("strong",null,"Link",-1),tt={href:"http://arxiv.org/abs/2404.02138v1",target:"_blank",rel:"noopener noreferrer"},nt=e("p",null,[e("strong",null,"Abstract"),t(': Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked LLM. Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the LLM. Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm. Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for LLMs and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss.')],-1),at=e("h4",{id:"advancing-llm-reasoning-generalists-with-preference-trees",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#advancing-llm-reasoning-generalists-with-preference-trees"},[e("span",null,"Advancing LLM Reasoning Generalists with Preference Trees")])],-1),it=e("p",null,[e("strong",null,"Authors"),t(": Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun")],-1),ot=e("strong",null,"Link",-1),st={href:"http://arxiv.org/abs/2404.02078v1",target:"_blank",rel:"noopener noreferrer"},rt=e("p",null,[e("strong",null,"Abstract"),t(": We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltraInteract allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.")],-1),lt=e("h4",{id:"long-context-llms-struggle-with-long-in-context-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#long-context-llms-struggle-with-long-in-context-learning"},[e("span",null,"Long-context LLMs Struggle with Long In-context Learning")])],-1),ht=e("p",null,[e("strong",null,"Authors"),t(": Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen")],-1),ct=e("strong",null,"Link",-1),dt={href:"http://arxiv.org/abs/2404.02060v1",target:"_blank",rel:"noopener noreferrer"},ut=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LIConBench could serve as a more realistic evaluation for the future long context LLMs.")],-1),gt=e("h4",{id:"multitask-based-evaluation-of-open-source-llm-on-software-vulnerability",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#multitask-based-evaluation-of-open-source-llm-on-software-vulnerability"},[e("span",null,"Multitask-based Evaluation of Open-Source LLM on Software Vulnerability")])],-1),pt=e("p",null,[e("strong",null,"Authors"),t(": Xin Yin, Chao Ni")],-1),mt=e("strong",null,"Link",-1),ft={href:"http://arxiv.org/abs/2404.02056v1",target:"_blank",rel:"noopener noreferrer"},vt=e("p",null,[e("strong",null,"Abstract"),t(": This paper proposes a pipeline for quantitatively evaluating interactive LLMs using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. We evaluate the multitask and multilingual aspects of LLMs based on this dataset. We find that the existing state-of-the-art methods are generally superior to LLMs in software vulnerability detection. Although LLMs improve accuracy when providing context information, they still have limitations in accurately predicting severity ratings for certain CWE types. In addition, LLMs demonstrate some ability to locate vulnerabilities for certain CWE types, but their performance varies among different CWE types. Finally, LLMs show uneven performance in generating CVE descriptions for various CWE types, with limited accuracy in a few-shot setting. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights for further enhancing LLMs' software vulnerability handling capabilities.")],-1),bt=e("h4",{id:"muxserve-flexible-multiplexing-for-efficient-multiple-llm-serving",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#muxserve-flexible-multiplexing-for-efficient-multiple-llm-serving"},[e("span",null,"MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving")])],-1),yt=e("p",null,[e("strong",null,"Authors"),t(": Jiangfei Duan, Runyu Lu, Haojie Duanmu, Xiuhong Li, Xingcheng Zhang, Dahua Lin, Ion Stoica, Hao Zhang")],-1),Lt=e("strong",null,"Link",-1),wt={href:"http://arxiv.org/abs/2404.02015v1",target:"_blank",rel:"noopener noreferrer"},_t=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have demon- strated remarkable performance, and organiza- tions are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search. However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs. In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving. The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources. MuxServe formally for- mulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal coloca- tions and maximize utilization. MuxServe de- signs a unified resource manager to enable flexi- ble and efficient multiplexing. Evaluation results show that MuxServe can achieves up to $1.8\\times$ higher throughput or processes $2.9\\times$ more requests within $99%$ SLO attainment.")],-1),kt=e("h4",{id:"self-organized-agents-a-llm-multi-agent-framework-toward-ultra-large-scale-code-generation-and-optimization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#self-organized-agents-a-llm-multi-agent-framework-toward-ultra-large-scale-code-generation-and-optimization"},[e("span",null,"Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization")])],-1),xt=e("p",null,[e("strong",null,"Authors"),t(": Yoichi Ishibashi, Yoshimasa Nishimura")],-1),Mt=e("strong",null,"Link",-1),At={href:"http://arxiv.org/abs/2404.02183v1",target:"_blank",rel:"noopener noreferrer"},Tt=e("p",null,[e("strong",null,"Abstract"),t(": Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.")],-1),St=e("h4",{id:"towards-better-understanding-of-cybercrime-the-role-of-fine-tuned-llms-in-translation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-better-understanding-of-cybercrime-the-role-of-fine-tuned-llms-in-translation"},[e("span",null,"Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation")])],-1),Ct=e("p",null,[e("strong",null,"Authors"),t(": Veronica Valeros, Anna Širokova, Carlos Catania, Sebastian Garcia")],-1),It=e("strong",null,"Link",-1),qt={href:"http://arxiv.org/abs/2404.01940v1",target:"_blank",rel:"noopener noreferrer"},zt=e("p",null,[e("strong",null,"Abstract"),t(": Understanding cybercrime communications is paramount for cybersecurity defence. This often involves translating communications into English for processing, interpreting, and generating timely intelligence. The problem is that translation is hard. Human translation is slow, expensive, and scarce. Machine translation is inaccurate and biased. We propose using fine-tuned Large Language Models (LLM) to generate translations that can accurately capture the nuances of cybercrime language. We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group. Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language. Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator.")],-1),Wt=e("h4",{id:"where-to-move-next-zero-shot-generalization-of-llms-for-next-poi-recommendation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#where-to-move-next-zero-shot-generalization-of-llms-for-next-poi-recommendation"},[e("span",null,"Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation")])],-1),Pt=e("p",null,[e("strong",null,"Authors"),t(": Shanshan Feng, Haoming Lyu, Caishun Chen, Yew-Soon Ong")],-1),Et=e("strong",null,"Link",-1),Gt={href:"http://arxiv.org/abs/2404.01855v1",target:"_blank",rel:"noopener noreferrer"},Rt=e("p",null,[e("strong",null,"Abstract"),t(": Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for predicting a user's next check-in. Specifically, we consider several essential factors in human movement behaviors, including user geographical preference, spatial distance, and sequential transitions, and formulate the recommendation task as a ranking problem. Through extensive experiments on two widely used real-world datasets, we derive several key findings. Empirical evaluations demonstrate that LLMs have promising zero-shot recommendation abilities and can provide accurate and reasonable predictions. We also reveal that LLMs cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of LLMs and necessitates further research on robust human mobility reasoning mechanisms.")],-1),Ht=e("h4",{id:"great-now-write-an-article-about-that-the-crescendo-multi-turn-llm-jailbreak-attack",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#great-now-write-an-article-about-that-the-crescendo-multi-turn-llm-jailbreak-attack"},[e("span",null,"Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack")])],-1),Dt=e("p",null,[e("strong",null,"Authors"),t(": Mark Russinovich, Ahmed Salem, Ronen Eldan")],-1),Ft=e("strong",null,"Link",-1),Bt={href:"http://arxiv.org/abs/2404.01833v1",target:"_blank",rel:"noopener noreferrer"},Ot=e("p",null,[e("strong",null,"Abstract"),t(`: Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as "jailbreaks", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models.`)],-1),jt=e("h4",{id:"transforming-llms-into-cross-modal-and-cross-lingual-retrieval-systems",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#transforming-llms-into-cross-modal-and-cross-lingual-retrieval-systems"},[e("span",null,"Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems")])],-1),Zt=e("p",null,[e("strong",null,"Authors"),t(": Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer, Siddharth Dalmia, Gustavo Hernandez Abrego")],-1),Nt=e("strong",null,"Link",-1),Yt={href:"http://arxiv.org/abs/2404.01616v2",target:"_blank",rel:"noopener noreferrer"},Jt=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.")],-1),Kt=e("h4",{id:"transforming-llms-into-cross-modal-and-cross-lingual-retrievalsystems",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#transforming-llms-into-cross-modal-and-cross-lingual-retrievalsystems"},[e("span",null,"Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems")])],-1),Vt=e("p",null,[e("strong",null,"Authors"),t(": Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer, Siddharth Dalmia, Gustavo Hernandez Abrego")],-1),Ut=e("strong",null,"Link",-1),Qt={href:"http://arxiv.org/abs/2404.01616v1",target:"_blank",rel:"noopener noreferrer"},Xt=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.")],-1),$t=e("h2",{id:"_2024-04-01",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-01"},[e("span",null,"2024-04-01")])],-1),en=e("h4",{id:"syntactic-robustness-for-llm-based-code-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#syntactic-robustness-for-llm-based-code-generation"},[e("span",null,"Syntactic Robustness for LLM-based Code Generation")])],-1),tn=e("p",null,[e("strong",null,"Authors"),t(": Laboni Sarker, Mara Downing, Achintya Desai, Tevfik Bultan")],-1),nn=e("strong",null,"Link",-1),an={href:"http://arxiv.org/abs/2404.01535v1",target:"_blank",rel:"noopener noreferrer"},on=e("p",null,[e("strong",null,"Abstract"),t(": Rapid advances in the field of Large Language Models (LLMs) have made LLM-based code generation an important area for investigation. An LLM-based code generator takes a prompt as input and produces code that implements the requirements specified in the prompt. Many software requirements include mathematical formulas that specify the expected behavior of the code to be generated. Given a code generation prompt that includes a mathematical formula, a reasonable expectation is that, if the formula is syntactically modified without changing its semantics, the generated code for the modified prompt should be semantically equivalent. We formalize this concept as syntactic robustness and investigate the syntactic robustness of GPT-3.5-Turbo and GPT-4 as code generators. To test syntactic robustness, we generate syntactically different but semantically equivalent versions of prompts using a set of mutators that only modify mathematical formulas in prompts. In this paper, we focus on prompts that ask for code that generates solutions to variables in an equation, when given coefficients of the equation as input. Our experimental evaluation demonstrates that GPT-3.5-Turbo and GPT-4 are not syntactically robust for this type of prompts. To improve syntactic robustness, we define a set of reductions that transform the formulas to a simplified form and use these reductions as a pre-processing step. Our experimental results indicate that the syntactic robustness of LLM-based code generation can be improved using our approach.")],-1),sn=e("h4",{id:"will-the-real-linda-please-stand-up-to-large-language-models-examining-the-representativeness-heuristic-in-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#will-the-real-linda-please-stand-up-to-large-language-models-examining-the-representativeness-heuristic-in-llms"},[e("span",null,"Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs")])],-1),rn=e("p",null,[e("strong",null,"Authors"),t(": Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald")],-1),ln=e("strong",null,"Link",-1),hn={href:"http://arxiv.org/abs/2404.01461v1",target:"_blank",rel:"noopener noreferrer"},cn=e("p",null,[e("strong",null,"Abstract"),t(": Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model of using its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while failing in a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.")],-1),dn=e("h4",{id:"unveiling-divergent-inductive-biases-of-llms-on-temporal-data",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unveiling-divergent-inductive-biases-of-llms-on-temporal-data"},[e("span",null,"Unveiling Divergent Inductive Biases of LLMs on Temporal Data")])],-1),un=e("p",null,[e("strong",null,"Authors"),t(": Sindhu Kishore, Hangfeng He")],-1),gn=e("strong",null,"Link",-1),pn={href:"http://arxiv.org/abs/2404.01453v1",target:"_blank",rel:"noopener noreferrer"},mn=e("p",null,[e("strong",null,"Abstract"),t(`: Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data. Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for "AFTER'' in the QA format for both implicit and explicit events, while GPT-4 leans towards "BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends towards "TRUE'', and GPT-4 exhibits a preference for "FALSE'' in the TE format for both implicit and explicit events. This persistent discrepancy between GPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of inductive bias in LLMs, suggesting that the evolution of these models may not merely mitigate bias but may introduce new layers of complexity.`)],-1),fn=e("h4",{id:"position-aware-parameter-efficient-fine-tuning-approach-for-reducing-positional-bias-in-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#position-aware-parameter-efficient-fine-tuning-approach-for-reducing-positional-bias-in-llms"},[e("span",null,"Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs")])],-1),vn=e("p",null,[e("strong",null,"Authors"),t(": Zheng Zhang, Fan Yang, Ziyan Jiang, Zheng Chen, Zhengyang Zhao, Chengyuan Ma, Liang Zhao, Yang Liu")],-1),bn=e("strong",null,"Link",-1),yn={href:"http://arxiv.org/abs/2404.01430v1",target:"_blank",rel:"noopener noreferrer"},Ln=e("p",null,[e("strong",null,"Abstract"),t(": Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augmentation technique and a parameter efficient adapter, enhancing a uniform attention distribution across the input context. Our experiments demonstrate that the proposed approach effectively reduces positional bias, improving LLMs' effectiveness in handling long context sequences for various tasks that require externally retrieved knowledge.")],-1),wn=e("h4",{id:"a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs"},[e("span",null,"A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs")])],-1),_n=e("p",null,[e("strong",null,"Authors"),t(": Harry Li, Gabriel Appleby, Ashley Suh")],-1),kn=e("strong",null,"Link",-1),xn={href:"http://arxiv.org/abs/2404.01425v1",target:"_blank",rel:"noopener noreferrer"},Mn=e("p",null,[e("strong",null,"Abstract"),t(": We present a mixed-methods study to explore how large language models (LLMs) can assist users in the visual exploration and analysis of knowledge graphs (KGs). We surveyed and interviewed 20 professionals from industry, government laboratories, and academia who regularly work with KGs and LLMs, either collaboratively or concurrently. Our findings show that participants overwhelmingly want an LLM to facilitate data retrieval from KGs through joint query construction, to identify interesting relationships in the KG through multi-turn conversation, and to create on-demand visualizations from the KG that enhance their trust in the LLM's outputs. To interact with an LLM, participants strongly prefer a chat-based 'widget,' built on top of their regular analysis workflows, with the ability to guide the LLM using their interactions with a visualization. When viewing an LLM's outputs, participants similarly prefer a combination of annotated visuals (e.g., subgraphs or tables extracted from the KG) alongside summarizing text. However, participants also expressed concerns about an LLM's ability to maintain semantic intent when translating natural language questions into KG queries, the risk of an LLM 'hallucinating' false data from the KG, and the difficulties of engineering a 'perfect prompt.' From the analysis of our interviews, we contribute a preliminary roadmap for the design of LLM-driven knowledge graph exploration systems and outline future opportunities in this emergent design space.")],-1),An=e("h4",{id:"prompt-prompted-mixture-of-experts-for-efficient-llm-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#prompt-prompted-mixture-of-experts-for-efficient-llm-generation"},[e("span",null,"Prompt-prompted Mixture of Experts for Efficient LLM Generation")])],-1),Tn=e("p",null,[e("strong",null,"Authors"),t(": Harry Dong, Beidi Chen, Yuejie Chi")],-1),Sn=e("strong",null,"Link",-1),Cn={href:"http://arxiv.org/abs/2404.01365v1",target:"_blank",rel:"noopener noreferrer"},In=e("p",null,[e("strong",null,"Abstract"),t(": With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.25$\\times$ speed-up in Llama 2 13B on an NVIDIA L40). Code will be available at https://github.com/hdong920/GRIFFIN.")],-1),qn=e("h4",{id:"mapping-the-increasing-use-of-llms-in-scientific-papers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#mapping-the-increasing-use-of-llms-in-scientific-papers"},[e("span",null,"Mapping the Increasing Use of LLMs in Scientific Papers")])],-1),zn=e("p",null,[e("strong",null,"Authors"),t(": Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D Manning, James Y. Zou")],-1),Wn=e("strong",null,"Link",-1),Pn={href:"http://arxiv.org/abs/2404.01268v1",target:"_blank",rel:"noopener noreferrer"},En=e("p",null,[e("strong",null,"Abstract"),t(": Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.")],-1),Gn=e("h4",{id:"llm-as-a-mastermind-a-survey-of-strategic-reasoning-with-large-language-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-as-a-mastermind-a-survey-of-strategic-reasoning-with-large-language-models"},[e("span",null,"LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models")])],-1),Rn=e("p",null,[e("strong",null,"Authors"),t(": Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei")],-1),Hn=e("strong",null,"Link",-1),Dn={href:"http://arxiv.org/abs/2404.01230v1",target:"_blank",rel:"noopener noreferrer"},Fn=e("p",null,[e("strong",null,"Abstract"),t(": This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.")],-1),Bn=e("h4",{id:"detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms"},[e("span",null,"Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs")])],-1),On=e("p",null,[e("strong",null,"Authors"),t(": Jialou Wang, Manli Zhu, Yulei Li, Honglei Li, Longzhi Yang, Wai Lok Woo")],-1),jn=e("strong",null,"Link",-1),Zn={href:"http://arxiv.org/abs/2404.01151v1",target:"_blank",rel:"noopener noreferrer"},Nn=e("p",null,[e("strong",null,"Abstract"),t(`: Localization plays a crucial role in enhancing the practicality and precision of VQA systems. By enabling fine-grained identification and interaction with specific parts of an object, it significantly improves the system's ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality. However, traditional systems face challenges in accurately mapping objects within images to generate nuanced and spatially aware responses. In this work, we introduce "Detect2Interact", which addresses these challenges by introducing an advanced approach for fine-grained object visual key field detection. First, we use the segment anything model (SAM) to generate detailed spatial maps of objects in images. Next, we use Vision Studio to extract semantic object descriptions. Third, we employ GPT-4's common sense knowledge, bridging the gap between an object's semantics and its spatial map. As a result, Detect2Interact achieves consistent qualitative results on object key field detection across extensive test cases and outperforms the existing VQA system with object detection by providing a more reasonable and finer visual representation.`)],-1),Yn=e("h4",{id:"do-llms-find-human-answers-to-fact-driven-questions-perplexing-a-case-study-on-reddit",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#do-llms-find-human-answers-to-fact-driven-questions-perplexing-a-case-study-on-reddit"},[e("span",null,"Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit")])],-1),Jn=e("p",null,[e("strong",null,"Authors"),t(": Parker Seegmiller, Joseph Gatto, Omar Sharif, Madhusudan Basak, Sarah Masud Preum")],-1),Kn=e("strong",null,"Link",-1),Vn={href:"http://arxiv.org/abs/2404.01147v1",target:"_blank",rel:"noopener noreferrer"},Un=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.")],-1),Qn=e("h4",{id:"structured-information-matters-incorporating-abstract-meaning-representation-into-llms-for-improved-open-domain-dialogue-evaluation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#structured-information-matters-incorporating-abstract-meaning-representation-into-llms-for-improved-open-domain-dialogue-evaluation"},[e("span",null,"Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation")])],-1),Xn=e("p",null,[e("strong",null,"Authors"),t(": Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, Chenghua Lin")],-1),$n=e("strong",null,"Link",-1),ea={href:"http://arxiv.org/abs/2404.01129v1",target:"_blank",rel:"noopener noreferrer"},ta=e("p",null,[e("strong",null,"Abstract"),t(": Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs) with LLMs. The SLMs can explicitly incorporate Abstract Meaning Representation (AMR) graph information of the dialogue through a gating mechanism for enhanced semantic representation learning. The evaluation result of SLMs and AMR graph information are plugged into the prompt of LLM, for the enhanced in-context learning performance. Experimental results on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to a wide range of state-of-the-art baselines, especially in discriminating adversarial negative responses. Our code is available at https://github.com/Bernard-Yang/SIMAMR.")],-1),na=e("h4",{id:"llm-attributor-interactive-visual-attribution-for-llm-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-attributor-interactive-visual-attribution-for-llm-generation"},[e("span",null,"LLM Attributor: Interactive Visual Attribution for LLM Generation")])],-1),aa=e("p",null,[e("strong",null,"Authors"),t(": Seongmin Lee, Zijie J. Wang, Aishwarya Chakravarthy, Alec Helbling, ShengYun Peng, Mansi Phute, Duen Horng Chau, Minsuk Kahng")],-1),ia=e("strong",null,"Link",-1),oa={href:"http://arxiv.org/abs/2404.01361v1",target:"_blank",rel:"noopener noreferrer"},sa=e("p",null,[e("strong",null,"Abstract"),t(": While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.")],-1),ra=e("h4",{id:"enabling-memory-safety-of-c-programs-using-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enabling-memory-safety-of-c-programs-using-llms"},[e("span",null,"Enabling Memory Safety of C Programs using LLMs")])],-1),la=e("p",null,[e("strong",null,"Authors"),t(": Nausheen Mohammed, Akash Lal, Aseem Rastogi, Subhajit Roy, Rahul Sharma")],-1),ha=e("strong",null,"Link",-1),ca={href:"http://arxiv.org/abs/2404.01096v1",target:"_blank",rel:"noopener noreferrer"},da=e("p",null,[e("strong",null,"Abstract"),t(": Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique. The task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use Large Language Models (LLMs) towards addressing both these concerns. We show how to harness LLM capabilities to do complex code reasoning as well as rewriting of large codebases. We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM. We implement our ideas in a tool called MSA that targets the CheckedC dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla LLM baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.")],-1),ua=e("h4",{id:"can-llms-get-help-from-other-llms-without-revealing-private-information",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-get-help-from-other-llms-without-revealing-private-information"},[e("span",null,"Can LLMs get help from other LLMs without revealing private information?")])],-1),ga=e("p",null,[e("strong",null,"Authors"),t(": Florian Hartmann, Duc-Hieu Tran, Peter Kairouz, Victor Cărbune, Blaise Aguera y Arcas")],-1),pa=e("strong",null,"Link",-1),ma={href:"http://arxiv.org/abs/2404.01041v2",target:"_blank",rel:"noopener noreferrer"},fa=e("p",null,[e("strong",null,"Abstract"),t(": Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language. Using this paradigm, we demonstrate on several datasets that our methods minimize the privacy loss while at the same time improving task performance compared to a non-cascade baseline.")],-1),va=e("h4",{id:"efficiently-distilling-llms-for-edge-applications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#efficiently-distilling-llms-for-edge-applications"},[e("span",null,"Efficiently Distilling LLMs for Edge Applications")])],-1),ba=e("p",null,[e("strong",null,"Authors"),t(": Achintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong, Rhui Dih Lee")],-1),ya=e("strong",null,"Link",-1),La={href:"http://arxiv.org/abs/2404.01353v1",target:"_blank",rel:"noopener noreferrer"},wa=e("p",null,[e("strong",null,"Abstract"),t(": Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.")],-1),_a=e("h4",{id:"exploring-and-evaluating-hallucinations-in-llm-powered-code-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-and-evaluating-hallucinations-in-llm-powered-code-generation"},[e("span",null,"Exploring and Evaluating Hallucinations in LLM-Powered Code Generation")])],-1),ka=e("p",null,[e("strong",null,"Authors"),t(": Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, Li Zhang")],-1),xa=e("strong",null,"Link",-1),Ma={href:"http://arxiv.org/abs/2404.00971v1",target:"_blank",rel:"noopener noreferrer"},Aa=e("p",null,[e("strong",null,"Abstract"),t(": The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.")],-1),Ta=e("h4",{id:"llms-are-good-sign-language-translators",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-are-good-sign-language-translators"},[e("span",null,"LLMs are Good Sign Language Translators")])],-1),Sa=e("p",null,[e("strong",null,"Authors"),t(": Jia Gong, Lin Geng Foo, Yixuan He, Hossein Rahmani, Jun Liu")],-1),Ca=e("strong",null,"Link",-1),Ia={href:"http://arxiv.org/abs/2404.00925v1",target:"_blank",rel:"noopener noreferrer"},qa=e("p",null,[e("strong",null,"Abstract"),t(": Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.")],-1),za=e("h4",{id:"tm-trek-at-semeval-2024-task-8-towards-llm-based-automatic-boundary-detection-for-human-machine-mixed-text",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tm-trek-at-semeval-2024-task-8-towards-llm-based-automatic-boundary-detection-for-human-machine-mixed-text"},[e("span",null,"TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text")])],-1),Wa=e("p",null,[e("strong",null,"Authors"),t(": Xiaoyan Qu, Xiangfeng Meng")],-1),Pa=e("strong",null,"Link",-1),Ea={href:"http://arxiv.org/abs/2404.00899v1",target:"_blank",rel:"noopener noreferrer"},Ga=e("p",null,[e("strong",null,"Abstract"),t(": With the increasing prevalence of text generated by large language models (LLMs), there is a growing concern about distinguishing between LLM-generated and human-written texts in order to prevent the misuse of LLMs, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content. This paper explores LLMs' ability to identify boundaries in human-written and machine-generated mixed texts. We approach this task by transforming it into a token classification problem and regard the label turning point as the boundary. Notably, our ensemble model of LLMs achieved first place in the 'Human-Machine Mixed Text Detection' sub-task of the SemEval'24 Competition Task 8. Additionally, we investigate factors that influence the capability of LLMs in detecting boundaries within mixed texts, including the incorporation of extra layers on top of LLMs, combination of segmentation loss, and the impact of pretraining. Our findings aim to provide valuable insights for future research in this area.")],-1),Ra=e("h2",{id:"_2024-03-31",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-31"},[e("span",null,"2024-03-31")])],-1),Ha=e("h4",{id:"the-larger-the-better-improved-llm-code-generation-via-budget-reallocation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-larger-the-better-improved-llm-code-generation-via-budget-reallocation"},[e("span",null,"The Larger the Better? Improved LLM Code-Generation via Budget Reallocation")])],-1),Da=e("p",null,[e("strong",null,"Authors"),t(": Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, Yossi Adi")],-1),Fa=e("strong",null,"Link",-1),Ba={href:"http://arxiv.org/abs/2404.00725v1",target:"_blank",rel:"noopener noreferrer"},Oa=e("p",null,[e("strong",null,"Abstract"),t(": It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.")],-1),ja=e("h4",{id:"training-free-semantic-segmentation-via-llm-supervision",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#training-free-semantic-segmentation-via-llm-supervision"},[e("span",null,"Training-Free Semantic Segmentation via LLM-Supervision")])],-1),Za=e("p",null,[e("strong",null,"Authors"),t(": Wenfang Sun, Yingjun Du, Gaowen Liu, Ramana Kompella, Cees G. M. Snoek")],-1),Na=e("strong",null,"Link",-1),Ya={href:"http://arxiv.org/abs/2404.00701v1",target:"_blank",rel:"noopener noreferrer"},Ja=e("p",null,[e("strong",null,"Abstract"),t(": Recent advancements in open vocabulary models, like CLIP, have notably advanced zero-shot classification and segmentation by utilizing natural language for class-specific embeddings. However, most research has focused on improving model accuracy through prompt engineering, prompt learning, or fine-tuning with limited labeled data, thereby overlooking the importance of refining the class descriptors. This paper introduces a new approach to text-supervised semantic segmentation using supervision by a large language model (LLM) that does not require extra training. Our method starts from an LLM, like GPT-3, to generate a detailed set of subclasses for more accurate class representation. We then employ an advanced text-supervised semantic segmentation model to apply the generated subclasses as target labels, resulting in diverse segmentation results tailored to each subclass's unique characteristics. Additionally, we propose an assembly that merges the segmentation maps from the various subclass descriptors to ensure a more comprehensive representation of the different aspects in the test images. Through comprehensive experiments on three standard benchmarks, our method outperforms traditional text-supervised semantic segmentation methods by a marked margin.")],-1),Ka=e("h4",{id:"how-much-are-llms-contaminated-a-comprehensive-survey-and-the-llmsanitize-library",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#how-much-are-llms-contaminated-a-comprehensive-survey-and-the-llmsanitize-library"},[e("span",null,"How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library")])],-1),Va=e("p",null,[e("strong",null,"Authors"),t(": Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq Joty")],-1),Ua=e("strong",null,"Link",-1),Qa={href:"http://arxiv.org/abs/2404.00699v1",target:"_blank",rel:"noopener noreferrer"},Xa=e("p",null,[e("strong",null,"Abstract"),t(": With the rise of Large Language Models (LLMs) in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a critical issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address contamination, or a clear consensus on prevention, mitigation and classification of contamination. In this paper, we survey all recent work on contamination with LLMs, and help the community track contamination levels of LLMs by releasing an open-source Python library named LLMSanitize implementing major contamination detection algorithms, which link is: https://github.com/ntunlp/LLMSanitize.")],-1),$a=e("h4",{id:"llm-meets-vision-language-models-for-zero-shot-one-class-classification",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-meets-vision-language-models-for-zero-shot-one-class-classification"},[e("span",null,"LLM meets Vision-Language Models for Zero-Shot One-Class Classification")])],-1),ei=e("p",null,[e("strong",null,"Authors"),t(": Yassir Bendou, Giulia Lioi, Bastien Pasdeloup, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux, Vincent Gripon")],-1),ti=e("strong",null,"Link",-1),ni={href:"http://arxiv.org/abs/2404.00675v2",target:"_blank",rel:"noopener noreferrer"},ai=e("p",null,[e("strong",null,"Abstract"),t(": We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to discriminate between a single category and other semantically related ones using only its label")],-1),ii=e("h4",{id:"face-it-yourselves-an-llm-based-two-stage-strategy-to-localize-configuration-errors-via-logs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#face-it-yourselves-an-llm-based-two-stage-strategy-to-localize-configuration-errors-via-logs"},[e("span",null,"Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs")])],-1),oi=e("p",null,[e("strong",null,"Authors"),t(": Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li, Zibin Zheng")],-1),si=e("strong",null,"Link",-1),ri={href:"http://arxiv.org/abs/2404.00640v2",target:"_blank",rel:"noopener noreferrer"},li=e("p",null,[e("strong",null,"Abstract"),t(": Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis. To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models~(LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.")],-1),hi=e("h4",{id:"ai-act-and-large-language-models-llms-when-critical-issues-and-privacy-impact-require-human-and-ethical-oversight",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ai-act-and-large-language-models-llms-when-critical-issues-and-privacy-impact-require-human-and-ethical-oversight"},[e("span",null,"AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight")])],-1),ci=e("p",null,[e("strong",null,"Authors"),t(": Nicola Fabiano")],-1),di=e("strong",null,"Link",-1),ui={href:"http://arxiv.org/abs/2404.00600v2",target:"_blank",rel:"noopener noreferrer"},gi=e("p",null,[e("strong",null,"Abstract"),t(": The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.")],-1),pi=e("h4",{id:"chops-chat-with-customer-profile-systems-for-customer-service-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#chops-chat-with-customer-profile-systems-for-customer-service-with-llms"},[e("span",null,"CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs")])],-1),mi=e("p",null,[e("strong",null,"Authors"),t(": Jingzhe Shi, Jialuo Li, Qinwei Ma, Zaiwen Yang, Huan Ma, Lei Li")],-1),fi=e("strong",null,"Link",-1),vi={href:"http://arxiv.org/abs/2404.01343v1",target:"_blank",rel:"noopener noreferrer"},bi=e("p",null,[e("strong",null,"Abstract"),t(": Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverage a combination of small and large LLMs to achieve satisfying performance at a reasonable inference cost. We introduce a practical dataset, the CPHOS-dataset, which includes a database, guiding files, and QA pairs collected from CPHOS, an online platform that facilitates the organization of simulated Physics Olympiads for high school teachers and students. We have conducted extensive experiments to validate the performance of our proposed CHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how LLMs can enhance or serve as alternatives to human customer service. Our code and dataset will be open-sourced soon.")],-1),yi=e("h4",{id:"my-agent-understands-me-better-integrating-dynamic-human-like-memory-recall-and-consolidation-in-llm-based-agents",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#my-agent-understands-me-better-integrating-dynamic-human-like-memory-recall-and-consolidation-in-llm-based-agents"},[e("span",null,'"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents')])],-1),Li=e("p",null,[e("strong",null,"Authors"),t(": Yuki Hou, Haruki Tamoto, Homei Miyashita")],-1),wi=e("strong",null,"Link",-1),_i={href:"http://arxiv.org/abs/2404.00573v1",target:"_blank",rel:"noopener noreferrer"},ki=e("p",null,[e("strong",null,"Abstract"),t(": In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of large language model based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of LLMs. We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user's interaction history in a database that encapsulates each memory's content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences.")],-1),xi=e("h4",{id:"divtod-unleashing-the-power-of-llms-for-diversifying-task-oriented-dialogue-representations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#divtod-unleashing-the-power-of-llms-for-diversifying-task-oriented-dialogue-representations"},[e("span",null,"DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations")])],-1),Mi=e("p",null,[e("strong",null,"Authors"),t(": Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang, Yukai Xu, Weiran Xu")],-1),Ai=e("strong",null,"Link",-1),Ti={href:"http://arxiv.org/abs/2404.00557v1",target:"_blank",rel:"noopener noreferrer"},Si=e("p",null,[e("strong",null,"Abstract"),t(": Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.")],-1),Ci=e("h4",{id:"llms-are-good-action-recognizers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-are-good-action-recognizers"},[e("span",null,"LLMs are Good Action Recognizers")])],-1),Ii=e("p",null,[e("strong",null,"Authors"),t(": Haoxuan Qu, Yujun Cai, Jun Liu")],-1),qi=e("strong",null,"Link",-1),zi={href:"http://arxiv.org/abs/2404.00532v1",target:"_blank",rel:"noopener noreferrer"},Wi=e("p",null,[e("strong",null,"Abstract"),t(": Skeleton-based action recognition has attracted lots of research attention. Recently, to build an accurate skeleton-based action recognizer, a variety of works have been proposed. Among them, some works use large model architectures as backbones of their recognizers to boost the skeleton data representation capability, while some other works pre-train their recognizers on external data to enrich the knowledge. In this work, we observe that large language models which have been extensively used in various natural language processing tasks generally hold both large model architectures and rich implicit knowledge. Motivated by this, we propose a novel LLM-AR framework, in which we investigate treating the Large Language Model as an Action Recognizer. In our framework, we propose a linguistic projection process to project each input action signal (i.e., each skeleton sequence) into its "),e("code",null,"sentence format'' (i.e., an "),t("action sentence''). Moreover, we also incorporate our framework with several designs to further facilitate this linguistic projection process. Extensive experiments demonstrate the efficacy of our proposed framework.")],-1),Pi=e("h2",{id:"_2024-03-30",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-30"},[e("span",null,"2024-03-30")])],-1),Ei=e("h4",{id:"contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app"},[e("span",null,"Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App")])],-1),Gi=e("p",null,[e("strong",null,"Authors"),t(": Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Eunsol Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Colin Depp, Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell")],-1),Ri=e("strong",null,"Link",-1),Hi={href:"http://arxiv.org/abs/2404.00487v1",target:"_blank",rel:"noopener noreferrer"},Di=e("p",null,[e("strong",null,"Abstract"),t(": MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.")],-1),Fi=e("h4",{id:"dialectical-alignment-resolving-the-tension-of-3h-and-security-threats-of-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#dialectical-alignment-resolving-the-tension-of-3h-and-security-threats-of-llms"},[e("span",null,"Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs")])],-1),Bi=e("p",null,[e("strong",null,"Authors"),t(": Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, Di Wang")],-1),Oi=e("strong",null,"Link",-1),ji={href:"http://arxiv.org/abs/2404.00486v1",target:"_blank",rel:"noopener noreferrer"},Zi=e("p",null,[e("strong",null,"Abstract"),t(": With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for LLM alignment to defense poisoned context attack while preserving the effectiveness of in-context knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20 and does not require any additional prompt engineering or prior declaration of "),e("code",null,"you may be attacked"),t(" to the LLMs' context window.")],-1),Ni=e("h4",{id:"numerologic-number-encoding-for-enhanced-llms-numerical-reasoning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#numerologic-number-encoding-for-enhanced-llms-numerical-reasoning"},[e("span",null,"NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning")])],-1),Yi=e("p",null,[e("strong",null,"Authors"),t(": Eli Schwartz, Leshem Choshen, Joseph Shtok, Sivan Doveh, Leonid Karlinsky, Assaf Arbelle")],-1),Ji=e("strong",null,"Link",-1),Ki={href:"http://arxiv.org/abs/2404.00459v1",target:"_blank",rel:"noopener noreferrer"},Vi=e("p",null,[e("strong",null,"Abstract"),t(': Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of "42", we suggest using "{2:42}" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.')],-1),Ui=e("h4",{id:"metaie-distilling-a-meta-model-from-llm-for-all-kinds-of-information-extraction-tasks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#metaie-distilling-a-meta-model-from-llm-for-all-kinds-of-information-extraction-tasks"},[e("span",null,"MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks")])],-1),Qi=e("p",null,[e("strong",null,"Authors"),t(": Letian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo Shang")],-1),Xi=e("strong",null,"Link",-1),$i={href:"http://arxiv.org/abs/2404.00457v1",target:"_blank",rel:"noopener noreferrer"},eo=e("p",null,[e("strong",null,"Abstract"),t(': Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract "important information", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to identify the typed spans of "important information". We evaluate the meta-model under the few-shot adaptation setting. Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer a better starting point for few-shot tuning on IE datasets and outperform other meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic distillation from LLM. Moreover, we provide comprehensive analyses of MetaIE, such as the size of the distillation dataset, the meta-model architecture, and the size of the meta-model.')],-1),to=e("h4",{id:"quarot-outlier-free-4-bit-inference-in-rotated-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#quarot-outlier-free-4-bit-inference-in-rotated-llms"},[e("span",null,"QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs")])],-1),no=e("p",null,[e("strong",null,"Authors"),t(": Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James Hensman")],-1),ao=e("strong",null,"Link",-1),io={href:"http://arxiv.org/abs/2404.00456v1",target:"_blank",rel:"noopener noreferrer"},oo=e("p",null,[e("strong",null,"Abstract"),t(": We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.")],-1),so=e("h4",{id:"a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration"},[e("span",null,"A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration")])],-1),ro=e("p",null,[e("strong",null,"Authors"),t(": Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, Thomas W. Malone")],-1),lo=e("strong",null,"Link",-1),ho={href:"http://arxiv.org/abs/2404.00405v1",target:"_blank",rel:"noopener noreferrer"},co=e("p",null,[e("strong",null,"Abstract"),t(`: With ChatGPT's release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the "5W1H" guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.`)],-1),uo=e("h4",{id:"can-llms-master-math-investigating-large-language-models-on-math-stack-exchange",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-master-math-investigating-large-language-models-on-math-stack-exchange"},[e("span",null,"Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange")])],-1),go=e("p",null,[e("strong",null,"Authors"),t(": Ankit Satpute, Noah Giessing, Andre Greiner-Petter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp")],-1),po=e("strong",null,"Link",-1),mo={href:"http://arxiv.org/abs/2404.00344v1",target:"_blank",rel:"noopener noreferrer"},fo=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10. Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving. Through case analysis, we shed light on the gaps in LLM capabilities within mathematics, thereby setting the stage for future research and advancements in AI-driven mathematical reasoning. We make our code and findings publicly available for research: \\url{https://github.com/gipplab/LLM-Investig-MathStackExchange}")],-1),vo=e("h4",{id:"augmenting-ner-datasets-with-llms-towards-automated-and-refined-annotation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#augmenting-ner-datasets-with-llms-towards-automated-and-refined-annotation"},[e("span",null,"Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation")])],-1),bo=e("p",null,[e("strong",null,"Authors"),t(": Yuji Naraki, Ryosuke Yamaki, Yoshikazu Ikeda, Takafumi Horie, Hiroki Naganuma")],-1),yo=e("strong",null,"Link",-1),Lo={href:"http://arxiv.org/abs/2404.01334v1",target:"_blank",rel:"noopener noreferrer"},wo=e("p",null,[e("strong",null,"Abstract"),t(": In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under constrained budget conditions. This study illuminates the potential of leveraging LLMs to improve dataset quality, introduces a novel technique to mitigate class imbalances, and demonstrates the feasibility of achieving high-performance NER in a cost-effective way.")],-1),_o=e("h4",{id:"a-comprehensive-study-on-nlp-data-augmentation-for-hate-speech-detection-legacy-methods-bert-and-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-comprehensive-study-on-nlp-data-augmentation-for-hate-speech-detection-legacy-methods-bert-and-llms"},[e("span",null,"A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs")])],-1),ko=e("p",null,[e("strong",null,"Authors"),t(": Md Saroar Jahan, Mourad Oussalah, Djamila Romaissa Beddia, Jhuma kabir Mim, Nabil Arhab")],-1),xo=e("strong",null,"Link",-1),Mo={href:"http://arxiv.org/abs/2404.00303v1",target:"_blank",rel:"noopener noreferrer"},Ao=e("p",null,[e("strong",null,"Abstract"),t(": The surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, BERT-mask contextual augmentation, and LLM. Our analysis across five benchmarked datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed BERT-based contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting data with GPT-3 not only avoided overfitting with up to sevenfold data increase but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.")],-1),To=e("h4",{id:"secret-keepers-the-impact-of-llms-on-linguistic-markers-of-personal-traits",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#secret-keepers-the-impact-of-llms-on-linguistic-markers-of-personal-traits"},[e("span",null,"Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits")])],-1),So=e("p",null,[e("strong",null,"Authors"),t(": Zhivar Sourati, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Nuan Wen, Ala Tak, Fred Morstatter, Morteza Dehghani")],-1),Co=e("strong",null,"Link",-1),Io={href:"http://arxiv.org/abs/2404.00267v1",target:"_blank",rel:"noopener noreferrer"},qo=e("p",null,[e("strong",null,"Abstract"),t(": Prior research has established associations between individuals' language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of Large Language Models (LLMs) as writing assistants in everyday writing, a critical question emerges: are authors' linguistic patterns still predictive of their personal traits when LLMs are involved in the writing process? We investigate the impact of LLMs on the linguistic markers of demographic and psychological traits, specifically examining three LLMs - GPT3.5, Llama 2, and Gemini - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of LLMs slightly reduces the predictive power of linguistic patterns over authors' personal traits, the significant changes are infrequent, and the use of LLMs does not fully diminish the predictive power of authors' linguistic patterns over their personal traits. We also note that some theoretically established lexical-based linguistic markers lose their reliability as predictors when LLMs are used in the writing process. Our findings have important implications for the study of linguistic markers of personal traits in the age of LLMs.")],-1),zo=e("h4",{id:"deft-flash-tree-attention-with-io-awareness-for-efficient-tree-search-based-llm-inference",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#deft-flash-tree-attention-with-io-awareness-for-efficient-tree-search-based-llm-inference"},[e("span",null,"DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference")])],-1),Wo=e("p",null,[e("strong",null,"Authors"),t(": Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin")],-1),Po=e("strong",null,"Link",-1),Eo={href:"http://arxiv.org/abs/2404.00242v1",target:"_blank",rel:"noopener noreferrer"},Go=e("p",null,[e("strong",null,"Abstract"),t(": Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculation: we calculate partial attention of each QKV groups in a fused kernel then apply a Tree-topology-aware Global Reduction strategy to get final attention. Thanks to a reduction in KV cache IO by 3.6-4.5$\\times$, along with an additional reduction in IO for $\\mathbf{Q} \\mathbf{K}^\\top$ and Softmax equivalent to 25% of the total KV cache IO, DeFT can achieve a speedup of 1.7-2.4$\\times$ in end-to-end latency across two practical reasoning tasks over the SOTA attention algorithms.")],-1),Ro=e("h4",{id:"is-factuality-decoding-a-free-lunch-for-llms-evaluation-on-knowledge-editing-benchmark",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#is-factuality-decoding-a-free-lunch-for-llms-evaluation-on-knowledge-editing-benchmark"},[e("span",null,"Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark")])],-1),Ho=e("p",null,[e("strong",null,"Authors"),t(": Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng")],-1),Do=e("strong",null,"Link",-1),Fo={href:"http://arxiv.org/abs/2404.00216v1",target:"_blank",rel:"noopener noreferrer"},Bo=e("p",null,[e("strong",null,"Abstract"),t(": The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.")],-1),Oo=e("h2",{id:"_2024-03-29",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-29"},[e("span",null,"2024-03-29")])],-1),jo=e("h4",{id:"towards-greener-llms-bringing-energy-efficiency-to-the-forefront-of-llm-inference",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-greener-llms-bringing-energy-efficiency-to-the-forefront-of-llm-inference"},[e("span",null,"Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference")])],-1),Zo=e("p",null,[e("strong",null,"Authors"),t(": Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, Josep Torrellas")],-1),No=e("strong",null,"Link",-1),Yo={href:"http://arxiv.org/abs/2403.20306v1",target:"_blank",rel:"noopener noreferrer"},Jo=e("p",null,[e("strong",null,"Abstract"),t(": With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-the-line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving the way for sustainable and cost-effective LLM deployment in data center environments.")],-1),Ko=e("h4",{id:"can-llms-correct-physicians-yet-investigating-effective-interaction-methods-in-the-medical-domain",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-correct-physicians-yet-investigating-effective-interaction-methods-in-the-medical-domain"},[e("span",null,"Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain")])],-1),Vo=e("p",null,[e("strong",null,"Authors"),t(": Burcu Sayin, Pasquale Minervini, Jacopo Staiano, Andrea Passerini")],-1),Uo=e("strong",null,"Link",-1),Qo={href:"http://arxiv.org/abs/2403.20288v1",target:"_blank",rel:"noopener noreferrer"},Xo=e("p",null,[e("strong",null,"Abstract"),t(": We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models exhibit greater sensitivity to prompt choice. Our analysis also uncovers the challenges of ensuring that LLM-generated suggestions are pertinent and useful, emphasizing the need for further research in this area.")],-1),$o=e("h4",{id:"luq-long-text-uncertainty-quantification-for-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#luq-long-text-uncertainty-quantification-for-llms"},[e("span",null,"LUQ: Long-text Uncertainty Quantification for LLMs")])],-1),es=e("p",null,[e("strong",null,"Authors"),t(": Caiqi Zhang, Fangyu Liu, Marco Basaldella, Nigel Collier")],-1),ts=e("strong",null,"Link",-1),ns={href:"http://arxiv.org/abs/2403.20279v1",target:"_blank",rel:"noopener noreferrer"},as=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. Despite their effectiveness, these models are prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce \\textsc{Luq}, a novel sampling-based UQ approach specifically designed for long text. Our findings reveal that \\textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). With \\textsc{Luq} as the tool for UQ, we investigate behavior patterns of several popular LLMs' response confidence spectrum and how that interplays with the response' factuality. We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about. To further improve the factual accuracy of LLM responses, we propose a method called \\textsc{Luq-Ensemble} that ensembles responses from multiple models and selects the response with the least uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM.")],-1),is=e("h4",{id:"using-llms-to-model-the-beliefs-and-preferences-of-targeted-populations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#using-llms-to-model-the-beliefs-and-preferences-of-targeted-populations"},[e("span",null,"Using LLMs to Model the Beliefs and Preferences of Targeted Populations")])],-1),os=e("p",null,[e("strong",null,"Authors"),t(": Keiichi Namikoshi, Alex Filipowicz, David A. Shamma, Rumen Iliev, Candice L. Hogan, Nikos Arechiga")],-1),ss=e("strong",null,"Link",-1),rs={href:"http://arxiv.org/abs/2403.20252v1",target:"_blank",rel:"noopener noreferrer"},ls=e("p",null,[e("strong",null,"Abstract"),t(": We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.")],-1),hs=e("h4",{id:"accurate-block-quantization-in-llms-with-outliers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#accurate-block-quantization-in-llms-with-outliers"},[e("span",null,"Accurate Block Quantization in LLMs with Outliers")])],-1),cs=e("p",null,[e("strong",null,"Authors"),t(": Nikita Trukhanov, Ilya Soloveychik")],-1),ds=e("strong",null,"Link",-1),us={href:"http://arxiv.org/abs/2403.20137v1",target:"_blank",rel:"noopener noreferrer"},gs=e("p",null,[e("strong",null,"Abstract"),t(": The demand for inference on extremely large scale LLMs has seen enormous growth in the recent months. It made evident the colossal shortage of dedicated hardware capable of efficient and fast processing of the involved compute and memory movement. The problem is aggravated by the exploding raise in the lengths of the sequences being processed, since those require efficient on-chip storage of the KV-cache of size proportional to the sequence length. To make the required compute feasible and fit the involved data into available memory, numerous quantization techniques have been proposed that allow accurate quantization for both weights and activations. One of the main recent breakthroughs in this direction was introduction of the family of Block Floating Point (BFP) formats characterized by a block of mantissas with a shared scale factor. These enable memory- power-, and compute- efficient hardware support of the tensor operations and provide extremely good quantization accuracy. The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block. In this paper, we focus on the most critical problem of limited KV-cache storage. We propose a novel approach enabling usage of low precision BFP formats without compromising the resulting model accuracy. We exploit the common channel-wise patterns exhibited by the outliers to rearrange them in such a way, that their quantization quality is significantly improved. The methodology yields 2x savings in the memory footprint without significant degradation of the model's accuracy. Importantly, the rearrangement of channels happens at the compile time and thus has no impact on the inference latency.")],-1),ps=e("h4",{id:"can-llms-learn-from-previous-mistakes-investigating-llms-errors-to-boost-for-reasoning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-learn-from-previous-mistakes-investigating-llms-errors-to-boost-for-reasoning"},[e("span",null,"Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning")])],-1),ms=e("p",null,[e("strong",null,"Authors"),t(": Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang")],-1),fs=e("strong",null,"Link",-1),vs={href:"http://arxiv.org/abs/2403.20046v1",target:"_blank",rel:"noopener noreferrer"},bs=e("p",null,[e("strong",null,"Abstract"),t(": Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \\textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \\textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome. \\textsc{CoTErrorSet} will be published soon on \\texttt{Anonymity Link}.")],-1),ys=e("h4",{id:"enhancing-the-general-agent-capabilities-of-low-parameter-llms-through-tuning-and-multi-branch-reasoning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-the-general-agent-capabilities-of-low-parameter-llms-through-tuning-and-multi-branch-reasoning"},[e("span",null,"Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning")])],-1),Ls=e("p",null,[e("strong",null,"Authors"),t(": Qinhao Zhou, Zihan Zhang, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li")],-1),ws=e("strong",null,"Link",-1),_s={href:"http://arxiv.org/abs/2403.19962v1",target:"_blank",rel:"noopener noreferrer"},ks=e("p",null,[e("strong",null,"Abstract"),t(": Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.")],-1),xs=e("h4",{id:"are-llms-effective-backbones-for-fine-tuning-an-experimental-investigation-of-supervised-llms-on-chinese-short-text-matching",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#are-llms-effective-backbones-for-fine-tuning-an-experimental-investigation-of-supervised-llms-on-chinese-short-text-matching"},[e("span",null,"Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching")])],-1),Ms=e("p",null,[e("strong",null,"Authors"),t(": Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang")],-1),As=e("strong",null,"Link",-1),Ts={href:"http://arxiv.org/abs/2403.19930v1",target:"_blank",rel:"noopener noreferrer"},Ss=e("p",null,[e("strong",null,"Abstract"),t(": The recent success of Large Language Models (LLMs) has garnered significant attention in both academia and industry. Prior research on LLMs has primarily focused on enhancing or leveraging their generalization capabilities in zero- and few-shot settings. However, there has been limited investigation into effectively fine-tuning LLMs for a specific natural language understanding task in supervised settings. In this study, we conduct an experimental analysis by fine-tuning LLMs for the task of Chinese short text matching. We explore various factors that influence performance when fine-tuning LLMs, including task modeling methods, prompt formats, and output formats.")],-1),Cs=e("h2",{id:"_2024-03-28",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-28"},[e("span",null,"2024-03-28")])],-1),Is=e("h4",{id:"i-m-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#i-m-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices"},[e("span",null,`"I'm categorizing LLM as a productivity tool": Examining ethics of LLM use in HCI research practices`)])],-1),qs=e("p",null,[e("strong",null,"Authors"),t(": Shivani Kapania, Ruiyi Wang, Toby Jia-Jun Li, Tianshi Li, Hong Shen")],-1),zs=e("strong",null,"Link",-1),Ws={href:"http://arxiv.org/abs/2403.19876v1",target:"_blank",rel:"noopener noreferrer"},Ps=e("p",null,[e("strong",null,"Abstract"),t(": Large language models are increasingly applied in real-world scenarios, including research and education. These models, however, come with well-known ethical issues, which may manifest in unexpected ways in human-computer interaction research due to the extensive engagement with human subjects. This paper reports on research practices related to LLM use, drawing on 16 semi-structured interviews and a survey conducted with 50 HCI researchers. We discuss the ways in which LLMs are already being utilized throughout the entire HCI research pipeline, from ideation to system development and paper writing. While researchers described nuanced understandings of ethical issues, they were rarely or only partially able to identify and address those ethical concerns in their own projects. This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the LLM supply chain, the conditional nature of engaging with ethics, and competing priorities. Finally, we reflect on the implications of our findings and present opportunities to shape emerging norms of engaging with large language models in HCI research.")],-1),Es=e("h4",{id:"llmsense-harnessing-llms-for-high-level-reasoning-over-spatiotemporal-sensor-traces",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llmsense-harnessing-llms-for-high-level-reasoning-over-spatiotemporal-sensor-traces"},[e("span",null,"LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces")])],-1),Gs=e("p",null,[e("strong",null,"Authors"),t(": Xiaomin Ouyang, Mani Srivastava")],-1),Rs=e("strong",null,"Link",-1),Hs={href:"http://arxiv.org/abs/2403.19857v1",target:"_blank",rel:"noopener noreferrer"},Ds=e("p",null,[e("strong",null,"Abstract"),t(": Most studies on machine learning in sensing systems focus on low-level perception tasks that process raw sensory data within a short time window. However, many practical applications, such as human routine modeling and occupancy tracking, require high-level reasoning abilities to comprehend concepts and make inferences based on long-term sensor traces. Existing machine learning-based approaches for handling such complex tasks struggle to generalize due to the limited training samples and the high dimensionality of sensor traces, necessitating the integration of human knowledge for designing first-principle models or logic reasoning methods. We pose a fundamental question: Can we harness the reasoning capabilities and world knowledge of Large Language Models (LLMs) to recognize complex events from long-term spatiotemporal sensor traces? To answer this question, we design an effective prompting framework for LLMs on high-level reasoning tasks, which can handle traces from the raw sensor data as well as the low-level perception results. We also design two strategies to enhance performance with long sensor traces, including summarization before reasoning and selective inclusion of historical traces. Our framework can be implemented in an edge-cloud setup, running small LLMs on the edge for data summarization and performing high-level reasoning on the cloud for privacy preservation. The results show that LLMSense can achieve over 80% accuracy on two high-level reasoning tasks such as dementia diagnosis with behavior traces and occupancy tracking with environmental sensor traces. This paper provides a few insights and guidelines for leveraging LLM for high-level reasoning on sensor traces and highlights several directions for future work.")],-1),Fs=e("h4",{id:"llms-as-academic-reading-companions-extending-hci-through-synthetic-personae",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-as-academic-reading-companions-extending-hci-through-synthetic-personae"},[e("span",null,"LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae")])],-1),Bs=e("p",null,[e("strong",null,"Authors"),t(": Celia Chen, Alex Leitch")],-1),Os=e("strong",null,"Link",-1),js={href:"http://arxiv.org/abs/2403.19506v1",target:"_blank",rel:"noopener noreferrer"},Zs=e("p",null,[e("strong",null,"Abstract"),t(": This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning. We detail an exploratory study examining Claude.ai from Anthropic, an LLM-based interactive assistant that helps students comprehend complex qualitative literature content. The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging Claude.ai over a semester across two graduate courses. Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study. However, there is potential for overreliance and ethical considerations that warrant continued investigation. By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing.")],-1),Ns=e("h4",{id:"enhancing-anomaly-detection-in-financial-markets-with-an-llm-based-multi-agent-framework",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-anomaly-detection-in-financial-markets-with-an-llm-based-multi-agent-framework"},[e("span",null,"Enhancing Anomaly Detection in Financial Markets with an LLM-based Multi-Agent Framework")])],-1),Ys=e("p",null,[e("strong",null,"Authors"),t(": Taejin Park")],-1),Js=e("strong",null,"Link",-1),Ks={href:"http://arxiv.org/abs/2403.19735v1",target:"_blank",rel:"noopener noreferrer"},Vs=e("p",null,[e("strong",null,"Abstract"),t(": This paper introduces a Large Language Model (LLM)-based multi-agent framework designed to enhance anomaly detection within financial market data, tackling the longstanding challenge of manually verifying system-generated anomaly alerts. The framework harnesses a collaborative network of AI agents, each specialised in distinct functions including data conversion, expert analysis via web research, institutional knowledge utilization or cross-checking and report consolidation and management roles. By coordinating these agents towards a common objective, the framework provides a comprehensive and automated approach for validating and interpreting financial data anomalies. I analyse the S&P 500 index to demonstrate the framework's proficiency in enhancing the efficiency, accuracy and reduction of human intervention in financial market monitoring. The integration of AI's autonomous functionalities with established analytical methods not only underscores the framework's effectiveness in anomaly detection but also signals its broader applicability in supporting financial market monitoring.")],-1),Us=e("h4",{id:"checkpoint-merging-via-bayesian-optimization-in-llm-pretraining",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#checkpoint-merging-via-bayesian-optimization-in-llm-pretraining"},[e("span",null,"Checkpoint Merging via Bayesian Optimization in LLM Pretraining")])],-1),Qs=e("p",null,[e("strong",null,"Authors"),t(": Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui")],-1),Xs=e("strong",null,"Link",-1),$s={href:"http://arxiv.org/abs/2403.19390v1",target:"_blank",rel:"noopener noreferrer"},er=e("p",null,[e("strong",null,"Abstract"),t(": The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.")],-1),tr=e("h4",{id:"breaking-the-length-barrier-llm-enhanced-ctr-prediction-in-long-textual-user-behaviors",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#breaking-the-length-barrier-llm-enhanced-ctr-prediction-in-long-textual-user-behaviors"},[e("span",null,"Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors")])],-1),nr=e("p",null,[e("strong",null,"Authors"),t(": Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, Linjian Mo")],-1),ar=e("strong",null,"Link",-1),ir={href:"http://arxiv.org/abs/2403.19347v1",target:"_blank",rel:"noopener noreferrer"},or=e("p",null,[e("strong",null,"Abstract"),t(": With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from extensive user sequences and stores them in the offline database. Subsequently, the deeper, trainable layers of the LLM facilitate intricate inter-behavior interactions, thereby generating comprehensive user embeddings. This separation allows the learning of high-level user representations to be independent of low-level behavior encoding, significantly reducing computational complexity. Finally, these refined user embeddings, in conjunction with correspondingly processed item embeddings, are incorporated into the CTR model to compute the CTR scores. Extensive experimental results show that BAHE reduces training time and memory by five times for CTR models using LLMs, especially with longer user sequences. BAHE has been deployed in a real-world system, allowing for daily updates of 50 million CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR prediction.")],-1),sr=e("h4",{id:"tablellm-enabling-tabular-data-manipulation-by-llms-in-real-office-usage-scenarios",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tablellm-enabling-tabular-data-manipulation-by-llms-in-real-office-usage-scenarios"},[e("span",null,"TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios")])],-1),rr=e("p",null,[e("strong",null,"Authors"),t(": Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao, Juanzi Li, Jie Tang")],-1),lr=e("strong",null,"Link",-1),hr={href:"http://arxiv.org/abs/2403.19318v1",target:"_blank",rel:"noopener noreferrer"},cr=e("p",null,[e("strong",null,"Abstract"),t(": We introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction.")],-1),dr=e("h4",{id:"generate-then-retrieve-conversational-response-retrieval-using-llms-as-answer-and-query-generators",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#generate-then-retrieve-conversational-response-retrieval-using-llms-as-answer-and-query-generators"},[e("span",null,"Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators")])],-1),ur=e("p",null,[e("strong",null,"Authors"),t(": Zahra Abbasiantaeb, Mohammad Aliannejadi")],-1),gr=e("strong",null,"Link",-1),pr={href:"http://arxiv.org/abs/2403.19302v1",target:"_blank",rel:"noopener noreferrer"},mr=e("p",null,[e("strong",null,"Abstract"),t(": CIS is a prominent area in IR that focuses on developing interactive knowledge assistants. These systems must adeptly comprehend the user's information requirements within the conversational context and retrieve the relevant information. To this aim, the existing approaches model the user's information needs with one query called rewritten query and use this query for passage retrieval. In this paper, we propose three different methods for generating multiple queries to enhance the retrieval. In these methods, we leverage the capabilities of large language models (LLMs) in understanding the user's information need and generating an appropriate response, to generate multiple queries. We implement and evaluate the proposed models utilizing various LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot settings. In addition, we propose a new benchmark for TREC iKAT based on gpt 3.5 judgments. Our experiments reveal the effectiveness of our proposed models on the TREC iKAT dataset.")],-1),fr=e("h4",{id:"top-leaderboard-ranking-top-coding-proficiency-always-evoeval-evolving-coding-benchmarks-via-llm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#top-leaderboard-ranking-top-coding-proficiency-always-evoeval-evolving-coding-benchmarks-via-llm"},[e("span",null,"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM")])],-1),vr=e("p",null,[e("strong",null,"Authors"),t(": Chunqiu Steven Xia, Yinlin Deng, Lingming Zhang")],-1),br=e("strong",null,"Link",-1),yr={href:"http://arxiv.org/abs/2403.19114v1",target:"_blank",rel:"noopener noreferrer"},Lr=e("p",null,[e("strong",null,"Abstract"),t(": LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows that compared to the high performance obtained on standard benchmarks like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval. Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential overfitting of existing benchmarks. Furthermore, we showcase various insights, including the brittleness of instruction-following models when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition. EvoEval not only provides comprehensive benchmarks, but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of LLMs for code. We have open-sourced our benchmarks, tools, and complete LLM generations at https://github.com/evo-eval/evoeval")],-1),wr=e("h4",{id:"learning-from-correctness-without-prompting-makes-llm-efficient-reasoner",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learning-from-correctness-without-prompting-makes-llm-efficient-reasoner"},[e("span",null,"Learning From Correctness Without Prompting Makes LLM Efficient Reasoner")])],-1),_r=e("p",null,[e("strong",null,"Authors"),t(": Yuxuan Yao, Han Wu, Zhijiang Guo, Biyan Zhou, Jiahui Gao, Sichun Luo, Hanxu Hou, Xiaojin Fu, Linqi Song")],-1),kr=e("strong",null,"Link",-1),xr={href:"http://arxiv.org/abs/2403.19094v1",target:"_blank",rel:"noopener noreferrer"},Mr=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning performance with reduced token consumption.")],-1),Ar=e("h2",{id:"_2024-03-27",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-27"},[e("span",null,"2024-03-27")])],-1),Tr=e("h4",{id:"towards-llm-recsys-alignment-with-textual-id-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-llm-recsys-alignment-with-textual-id-learning"},[e("span",null,"Towards LLM-RecSys Alignment with Textual ID Learning")])],-1),Sr=e("p",null,[e("strong",null,"Authors"),t(": Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, Yongfeng Zhang")],-1),Cr=e("strong",null,"Link",-1),Ir={href:"http://arxiv.org/abs/2403.19021v1",target:"_blank",rel:"noopener noreferrer"},qr=e("p",null,[e("strong",null,"Abstract"),t(": Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potential for a foundational generative recommendation model. Experiments show that our framework consistently surpasses existing models in sequential recommendation under standard experimental setting. Then, we explore the possibility of training a foundation recommendation model with the proposed method on data collected from 19 different datasets and tested its recommendation performance on 6 unseen datasets across different platforms under a completely zero-shot setting. The results show that the zero-shot performance of the pre-trained foundation model is comparable to or even better than some traditional recommendation models based on supervised training, showing the potential of the IDGen paradigm serving as the foundation model for generative recommendation. Code and data are open-sourced at https://github.com/agiresearch/IDGenRec.")],-1),zr=e("h4",{id:"physicsassistant-an-llm-powered-interactive-learning-robot-for-physics-lab-investigations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#physicsassistant-an-llm-powered-interactive-learning-robot-for-physics-lab-investigations"},[e("span",null,"PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics Lab Investigations")])],-1),Wr=e("p",null,[e("strong",null,"Authors"),t(": Ehsan Latif, Ramviyas Parasuraman, Xiaoming Zhai")],-1),Pr=e("strong",null,"Link",-1),Er={href:"http://arxiv.org/abs/2403.18721v1",target:"_blank",rel:"noopener noreferrer"},Gr=e("p",null,[e("strong",null,"Abstract"),t(": Robot systems in education can leverage Large language models' (LLMs) natural language understanding capabilities to provide assistance and facilitate learning. This paper proposes a multimodal interactive robot (PhysicsAssistant) built on YOLOv8 object detection, cameras, speech recognition, and chatbot using LLM to provide assistance to students' physics labs. We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert. The Expert rates the assistants' responses to student queries on a 0-4 scale based on Bloom's taxonomy to provide educational support. We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human expert rating of both systems for factual understanding is the same. However, the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p < 0.05). However, the response time of GPT-4 is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively lower response quality of PhysicsAssistant than GPT-4, it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers' labor to assist with repetitive tasks. To the best of our knowledge, this is the first attempt to build such an interactive multimodal robotic assistant for K-12 science (physics) education.")],-1),Rr=e("h4",{id:"sdsat-accelerating-llm-inference-through-speculative-decoding-with-semantic-adaptive-tokens",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sdsat-accelerating-llm-inference-through-speculative-decoding-with-semantic-adaptive-tokens"},[e("span",null,"SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens")])],-1),Hr=e("p",null,[e("strong",null,"Authors"),t(": Chengbo Liu, Yong Zhu")],-1),Dr=e("strong",null,"Link",-1),Fr={href:"http://arxiv.org/abs/2403.18647v1",target:"_blank",rel:"noopener noreferrer"},Br=e("p",null,[e("strong",null,"Abstract"),t(`: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the "two-step-draft-then-verify" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to https://github.com/hasuoshenyun/SDSAT.`)],-1),Or=e("h4",{id:"foc-figure-out-the-cryptographic-functions-in-stripped-binaries-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#foc-figure-out-the-cryptographic-functions-in-stripped-binaries-with-llms"},[e("span",null,"FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs")])],-1),jr=e("p",null,[e("strong",null,"Authors"),t(": Guoqiang Chen, Xiuwei Shang, Shaoyin Cheng, Yanming Zhang, Weiming Zhang, Nenghai Yu")],-1),Zr=e("strong",null,"Link",-1),Nr={href:"http://arxiv.org/abs/2403.18403v1",target:"_blank",rel:"noopener noreferrer"},Yr=e("p",null,[e("strong",null,"Abstract"),t(": Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task. Cryptographic algorithms exhibit greater logical complexity compared to typical code, yet their analysis is unavoidable in areas such as virus analysis and legacy code inspection. Existing methods often rely on data or structural pattern matching, leading to suboptimal generalizability and suffering from manual work. In this paper, we propose a novel framework called FoC to Figure out the Cryptographic functions in stripped binaries. In FoC, we first build a binary large language model (FoCBinLLM) to summarize the semantics of cryptographic functions in natural language. The prediction of FoC-BinLLM is insensitive to minor changes, such as vulnerability patches. To mitigate it, we further build a binary code similarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive representations and use it to retrieve similar implementations of unknown cryptographic functions in a database. In addition, we construct a cryptographic binary dataset for evaluation and to facilitate further research in this domain. And an automated method is devised to create semantic labels for extensive binary functions. Evaluation results demonstrate that FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the previous best methods with a 52% higher Recall@1. Furthermore, our method also shows practical ability in virus analysis and 1-day vulnerability detection.")],-1),Jr=e("h4",{id:"rejection-improves-reliability-training-llms-to-refuse-unknown-questions-using-rl-from-knowledge-feedback",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#rejection-improves-reliability-training-llms-to-refuse-unknown-questions-using-rl-from-knowledge-feedback"},[e("span",null,"Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback")])],-1),Kr=e("p",null,[e("strong",null,"Authors"),t(": Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai Yu")],-1),Vr=e("strong",null,"Link",-1),Ur={href:"http://arxiv.org/abs/2403.18349v1",target:"_blank",rel:"noopener noreferrer"},Qr=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions. Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability.")],-1),Xr=e("h4",{id:"can-llms-converse-formally-automatically-assessing-llms-in-translating-and-interpreting-formal-specifications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-converse-formally-automatically-assessing-llms-in-translating-and-interpreting-formal-specifications"},[e("span",null,"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications")])],-1),$r=e("p",null,[e("strong",null,"Authors"),t(": Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava")],-1),el=e("strong",null,"Link",-1),tl={href:"http://arxiv.org/abs/2403.18327v1",target:"_blank",rel:"noopener noreferrer"},nl=e("p",null,[e("strong",null,"Abstract"),t(": Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems.")],-1),al=e("h4",{id:"exploring-the-deceptive-power-of-llm-generated-fake-news-a-study-of-real-world-detection-challenges",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-the-deceptive-power-of-llm-generated-fake-news-a-study-of-real-world-detection-challenges"},[e("span",null,"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges")])],-1),il=e("p",null,[e("strong",null,"Authors"),t(": Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu")],-1),ol=e("strong",null,"Link",-1),sl={href:"http://arxiv.org/abs/2403.18249v1",target:"_blank",rel:"noopener noreferrer"},rl=e("p",null,[e("strong",null,"Abstract"),t(": Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.")],-1),ll=e("h4",{id:"llms-in-hci-data-work-bridging-the-gap-between-information-retrieval-and-responsible-research-practices",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-in-hci-data-work-bridging-the-gap-between-information-retrieval-and-responsible-research-practices"},[e("span",null,"LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices")])],-1),hl=e("p",null,[e("strong",null,"Authors"),t(": Neda Taghizadeh Serajeh, Iman Mohammadi, Vittorio Fuccella, Mattia De Rosa")],-1),cl=e("strong",null,"Link",-1),dl={href:"http://arxiv.org/abs/2403.18173v1",target:"_blank",rel:"noopener noreferrer"},ul=e("p",null,[e("strong",null,"Abstract"),t(": Efficient and accurate information extraction from scientific papers is significant in the rapidly developing human-computer interaction research in the literature review process. Our paper introduces and analyses a new information retrieval system using state-of-the-art Large Language Models (LLMs) in combination with structured text analysis techniques to extract experimental data from HCI literature, emphasizing key elements. Then We analyze the challenges and risks of using LLMs in the world of research. We performed a comprehensive analysis on our conducted dataset, which contained the specified information of 300 CHI 2020-2022 papers, to evaluate the performance of the two large language models, GPT-3.5 (text-davinci-003) and Llama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model gains an accuracy of 58% and a mean absolute error of 7.00. In contrast, the Llama2 model indicates an accuracy of 56% with a mean absolute error of 7.63. The ability to answer questions was also included in the system in order to work with streamlined data. By evaluating the risks and opportunities presented by LLMs, our work contributes to the ongoing dialogue on establishing methodological validity and ethical guidelines for LLM use in HCI data work.")],-1),gl=e("h2",{id:"_2024-03-26",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-26"},[e("span",null,"2024-03-26")])],-1),pl=e("h4",{id:"don-t-trust-verify-grounding-llm-quantitative-reasoning-with-autoformalization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#don-t-trust-verify-grounding-llm-quantitative-reasoning-with-autoformalization"},[e("span",null,"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization")])],-1),ml=e("p",null,[e("strong",null,"Authors"),t(": Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu")],-1),fl=e("strong",null,"Link",-1),vl={href:"http://arxiv.org/abs/2403.18120v1",target:"_blank",rel:"noopener noreferrer"},bl=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.")],-1),yl=e("h4",{id:"magis-llm-based-multi-agent-framework-for-github-issue-resolution",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#magis-llm-based-multi-agent-framework-for-github-issue-resolution"},[e("span",null,"MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution")])],-1),Ll=e("p",null,[e("strong",null,"Authors"),t(": Wei Tao, Yucheng Zhou, Wenqiang Zhang, Yu Cheng")],-1),wl=e("strong",null,"Link",-1),_l={href:"http://arxiv.org/abs/2403.17927v1",target:"_blank",rel:"noopener noreferrer"},kl=e("p",null,[e("strong",null,"Abstract"),t(": In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method. We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc.")],-1),xl=e("h4",{id:"exploring-llms-as-a-source-of-targeted-synthetic-textual-data-to-minimize-high-confidence-misclassifications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-llms-as-a-source-of-targeted-synthetic-textual-data-to-minimize-high-confidence-misclassifications"},[e("span",null,"Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications")])],-1),Ml=e("p",null,[e("strong",null,"Authors"),t(": Philip Lippmann, Matthijs Spaan, Jie Yang")],-1),Al=e("strong",null,"Link",-1),Tl={href:"http://arxiv.org/abs/2403.17860v1",target:"_blank",rel:"noopener noreferrer"},Sl=e("p",null,[e("strong",null,"Abstract"),t(": Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable.")],-1),Cl=e("h4",{id:"verbing-weirds-language-models-evaluation-of-english-zero-derivation-in-five-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#verbing-weirds-language-models-evaluation-of-english-zero-derivation-in-five-llms"},[e("span",null,"Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs")])],-1),Il=e("p",null,[e("strong",null,"Authors"),t(": David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich Schütze, Leonie Weissweiler")],-1),ql=e("strong",null,"Link",-1),zl={href:"http://arxiv.org/abs/2403.17856v1",target:"_blank",rel:"noopener noreferrer"},Wl=e("p",null,[e("strong",null,"Abstract"),t(": Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of large language models with reference to conversion. We design a task for testing lexical-syntactic flexibility -- the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a natural language inference paradigm. We test the abilities of five language models -- two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral 7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task, followed by GPT-3.5, but that the open source language models are also able to perform it and that the 7B parameter Mistral displays as little difference between its baseline performance on the natural language inference task and the non-prototypical syntactic category task, as the massive GPT-4.")],-1),Pl=e("h4",{id:"accelerating-radio-spectrum-regulation-workflows-with-large-language-models-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#accelerating-radio-spectrum-regulation-workflows-with-large-language-models-llms"},[e("span",null,"Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)")])],-1),El=e("p",null,[e("strong",null,"Authors"),t(": Amir Ghasemi, Paul Guinand")],-1),Gl=e("strong",null,"Link",-1),Rl={href:"http://arxiv.org/abs/2403.17819v1",target:"_blank",rel:"noopener noreferrer"},Hl=e("p",null,[e("strong",null,"Abstract"),t(": Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications. To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks. In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes. We explore various roles that LLMs can play in this context while identifying some of the challenges to address. The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management.")],-1);function Dl(Fl,Bl){const n=o("ExternalLinkIcon");return s(),r("div",null,[c,e("p",null,[d,t(": "),e("a",u,[t("http://arxiv.org/abs/2404.03626v1"),a(n)])]),g,p,m,e("p",null,[f,t(": "),e("a",v,[t("http://arxiv.org/abs/2404.03623v1"),a(n)])]),b,y,L,e("p",null,[w,t(": "),e("a",_,[t("http://arxiv.org/abs/2404.03602v1"),a(n)])]),k,x,M,e("p",null,[A,t(": "),e("a",T,[t("http://arxiv.org/abs/2404.03565v1"),a(n)])]),S,C,I,e("p",null,[q,t(": "),e("a",z,[t("http://arxiv.org/abs/2404.03413v1"),a(n)])]),W,P,E,e("p",null,[G,t(": "),e("a",R,[t("http://arxiv.org/abs/2404.03192v1"),a(n)])]),H,D,F,e("p",null,[B,t(": "),e("a",O,[t("http://arxiv.org/abs/2404.03134v1"),a(n)])]),j,Z,N,e("p",null,[Y,t(": "),e("a",J,[t("http://arxiv.org/abs/2404.03122v1"),a(n)])]),K,V,U,Q,e("p",null,[X,t(": "),e("a",$,[t("http://arxiv.org/abs/2404.03044v1"),a(n)])]),ee,te,ne,e("p",null,[ae,t(": "),e("a",ie,[t("http://arxiv.org/abs/2404.02838v1"),a(n)])]),oe,se,re,e("p",null,[le,t(": "),e("a",he,[t("http://arxiv.org/abs/2404.02761v1"),a(n)])]),ce,de,ue,e("p",null,[ge,t(": "),e("a",pe,[t("http://arxiv.org/abs/2404.02706v1"),a(n)])]),me,fe,ve,e("p",null,[be,t(": "),e("a",ye,[t("http://arxiv.org/abs/2404.02616v1"),a(n)])]),Le,we,_e,e("p",null,[ke,t(": "),e("a",xe,[t("http://arxiv.org/abs/2404.02532v1"),a(n)])]),Me,Ae,Te,e("p",null,[Se,t(": "),e("a",Ce,[t("http://arxiv.org/abs/2404.02474v1"),a(n)])]),Ie,qe,ze,e("p",null,[We,t(": "),e("a",Pe,[t("http://arxiv.org/abs/2404.02422v1"),a(n)])]),Ee,Ge,Re,He,e("p",null,[De,t(": "),e("a",Fe,[t("http://arxiv.org/abs/2404.02294v1"),a(n)])]),Be,Oe,je,e("p",null,[Ze,t(": "),e("a",Ne,[t("http://arxiv.org/abs/2404.02261v1"),a(n)])]),Ye,Je,Ke,e("p",null,[Ve,t(": "),e("a",Ue,[t("http://arxiv.org/abs/2404.02151v1"),a(n)])]),Qe,Xe,$e,e("p",null,[et,t(": "),e("a",tt,[t("http://arxiv.org/abs/2404.02138v1"),a(n)])]),nt,at,it,e("p",null,[ot,t(": "),e("a",st,[t("http://arxiv.org/abs/2404.02078v1"),a(n)])]),rt,lt,ht,e("p",null,[ct,t(": "),e("a",dt,[t("http://arxiv.org/abs/2404.02060v1"),a(n)])]),ut,gt,pt,e("p",null,[mt,t(": "),e("a",ft,[t("http://arxiv.org/abs/2404.02056v1"),a(n)])]),vt,bt,yt,e("p",null,[Lt,t(": "),e("a",wt,[t("http://arxiv.org/abs/2404.02015v1"),a(n)])]),_t,kt,xt,e("p",null,[Mt,t(": "),e("a",At,[t("http://arxiv.org/abs/2404.02183v1"),a(n)])]),Tt,St,Ct,e("p",null,[It,t(": "),e("a",qt,[t("http://arxiv.org/abs/2404.01940v1"),a(n)])]),zt,Wt,Pt,e("p",null,[Et,t(": "),e("a",Gt,[t("http://arxiv.org/abs/2404.01855v1"),a(n)])]),Rt,Ht,Dt,e("p",null,[Ft,t(": "),e("a",Bt,[t("http://arxiv.org/abs/2404.01833v1"),a(n)])]),Ot,jt,Zt,e("p",null,[Nt,t(": "),e("a",Yt,[t("http://arxiv.org/abs/2404.01616v2"),a(n)])]),Jt,Kt,Vt,e("p",null,[Ut,t(": "),e("a",Qt,[t("http://arxiv.org/abs/2404.01616v1"),a(n)])]),Xt,$t,en,tn,e("p",null,[nn,t(": "),e("a",an,[t("http://arxiv.org/abs/2404.01535v1"),a(n)])]),on,sn,rn,e("p",null,[ln,t(": "),e("a",hn,[t("http://arxiv.org/abs/2404.01461v1"),a(n)])]),cn,dn,un,e("p",null,[gn,t(": "),e("a",pn,[t("http://arxiv.org/abs/2404.01453v1"),a(n)])]),mn,fn,vn,e("p",null,[bn,t(": "),e("a",yn,[t("http://arxiv.org/abs/2404.01430v1"),a(n)])]),Ln,wn,_n,e("p",null,[kn,t(": "),e("a",xn,[t("http://arxiv.org/abs/2404.01425v1"),a(n)])]),Mn,An,Tn,e("p",null,[Sn,t(": "),e("a",Cn,[t("http://arxiv.org/abs/2404.01365v1"),a(n)])]),In,qn,zn,e("p",null,[Wn,t(": "),e("a",Pn,[t("http://arxiv.org/abs/2404.01268v1"),a(n)])]),En,Gn,Rn,e("p",null,[Hn,t(": "),e("a",Dn,[t("http://arxiv.org/abs/2404.01230v1"),a(n)])]),Fn,Bn,On,e("p",null,[jn,t(": "),e("a",Zn,[t("http://arxiv.org/abs/2404.01151v1"),a(n)])]),Nn,Yn,Jn,e("p",null,[Kn,t(": "),e("a",Vn,[t("http://arxiv.org/abs/2404.01147v1"),a(n)])]),Un,Qn,Xn,e("p",null,[$n,t(": "),e("a",ea,[t("http://arxiv.org/abs/2404.01129v1"),a(n)])]),ta,na,aa,e("p",null,[ia,t(": "),e("a",oa,[t("http://arxiv.org/abs/2404.01361v1"),a(n)])]),sa,ra,la,e("p",null,[ha,t(": "),e("a",ca,[t("http://arxiv.org/abs/2404.01096v1"),a(n)])]),da,ua,ga,e("p",null,[pa,t(": "),e("a",ma,[t("http://arxiv.org/abs/2404.01041v2"),a(n)])]),fa,va,ba,e("p",null,[ya,t(": "),e("a",La,[t("http://arxiv.org/abs/2404.01353v1"),a(n)])]),wa,_a,ka,e("p",null,[xa,t(": "),e("a",Ma,[t("http://arxiv.org/abs/2404.00971v1"),a(n)])]),Aa,Ta,Sa,e("p",null,[Ca,t(": "),e("a",Ia,[t("http://arxiv.org/abs/2404.00925v1"),a(n)])]),qa,za,Wa,e("p",null,[Pa,t(": "),e("a",Ea,[t("http://arxiv.org/abs/2404.00899v1"),a(n)])]),Ga,Ra,Ha,Da,e("p",null,[Fa,t(": "),e("a",Ba,[t("http://arxiv.org/abs/2404.00725v1"),a(n)])]),Oa,ja,Za,e("p",null,[Na,t(": "),e("a",Ya,[t("http://arxiv.org/abs/2404.00701v1"),a(n)])]),Ja,Ka,Va,e("p",null,[Ua,t(": "),e("a",Qa,[t("http://arxiv.org/abs/2404.00699v1"),a(n)])]),Xa,$a,ei,e("p",null,[ti,t(": "),e("a",ni,[t("http://arxiv.org/abs/2404.00675v2"),a(n)])]),ai,ii,oi,e("p",null,[si,t(": "),e("a",ri,[t("http://arxiv.org/abs/2404.00640v2"),a(n)])]),li,hi,ci,e("p",null,[di,t(": "),e("a",ui,[t("http://arxiv.org/abs/2404.00600v2"),a(n)])]),gi,pi,mi,e("p",null,[fi,t(": "),e("a",vi,[t("http://arxiv.org/abs/2404.01343v1"),a(n)])]),bi,yi,Li,e("p",null,[wi,t(": "),e("a",_i,[t("http://arxiv.org/abs/2404.00573v1"),a(n)])]),ki,xi,Mi,e("p",null,[Ai,t(": "),e("a",Ti,[t("http://arxiv.org/abs/2404.00557v1"),a(n)])]),Si,Ci,Ii,e("p",null,[qi,t(": "),e("a",zi,[t("http://arxiv.org/abs/2404.00532v1"),a(n)])]),Wi,Pi,Ei,Gi,e("p",null,[Ri,t(": "),e("a",Hi,[t("http://arxiv.org/abs/2404.00487v1"),a(n)])]),Di,Fi,Bi,e("p",null,[Oi,t(": "),e("a",ji,[t("http://arxiv.org/abs/2404.00486v1"),a(n)])]),Zi,Ni,Yi,e("p",null,[Ji,t(": "),e("a",Ki,[t("http://arxiv.org/abs/2404.00459v1"),a(n)])]),Vi,Ui,Qi,e("p",null,[Xi,t(": "),e("a",$i,[t("http://arxiv.org/abs/2404.00457v1"),a(n)])]),eo,to,no,e("p",null,[ao,t(": "),e("a",io,[t("http://arxiv.org/abs/2404.00456v1"),a(n)])]),oo,so,ro,e("p",null,[lo,t(": "),e("a",ho,[t("http://arxiv.org/abs/2404.00405v1"),a(n)])]),co,uo,go,e("p",null,[po,t(": "),e("a",mo,[t("http://arxiv.org/abs/2404.00344v1"),a(n)])]),fo,vo,bo,e("p",null,[yo,t(": "),e("a",Lo,[t("http://arxiv.org/abs/2404.01334v1"),a(n)])]),wo,_o,ko,e("p",null,[xo,t(": "),e("a",Mo,[t("http://arxiv.org/abs/2404.00303v1"),a(n)])]),Ao,To,So,e("p",null,[Co,t(": "),e("a",Io,[t("http://arxiv.org/abs/2404.00267v1"),a(n)])]),qo,zo,Wo,e("p",null,[Po,t(": "),e("a",Eo,[t("http://arxiv.org/abs/2404.00242v1"),a(n)])]),Go,Ro,Ho,e("p",null,[Do,t(": "),e("a",Fo,[t("http://arxiv.org/abs/2404.00216v1"),a(n)])]),Bo,Oo,jo,Zo,e("p",null,[No,t(": "),e("a",Yo,[t("http://arxiv.org/abs/2403.20306v1"),a(n)])]),Jo,Ko,Vo,e("p",null,[Uo,t(": "),e("a",Qo,[t("http://arxiv.org/abs/2403.20288v1"),a(n)])]),Xo,$o,es,e("p",null,[ts,t(": "),e("a",ns,[t("http://arxiv.org/abs/2403.20279v1"),a(n)])]),as,is,os,e("p",null,[ss,t(": "),e("a",rs,[t("http://arxiv.org/abs/2403.20252v1"),a(n)])]),ls,hs,cs,e("p",null,[ds,t(": "),e("a",us,[t("http://arxiv.org/abs/2403.20137v1"),a(n)])]),gs,ps,ms,e("p",null,[fs,t(": "),e("a",vs,[t("http://arxiv.org/abs/2403.20046v1"),a(n)])]),bs,ys,Ls,e("p",null,[ws,t(": "),e("a",_s,[t("http://arxiv.org/abs/2403.19962v1"),a(n)])]),ks,xs,Ms,e("p",null,[As,t(": "),e("a",Ts,[t("http://arxiv.org/abs/2403.19930v1"),a(n)])]),Ss,Cs,Is,qs,e("p",null,[zs,t(": "),e("a",Ws,[t("http://arxiv.org/abs/2403.19876v1"),a(n)])]),Ps,Es,Gs,e("p",null,[Rs,t(": "),e("a",Hs,[t("http://arxiv.org/abs/2403.19857v1"),a(n)])]),Ds,Fs,Bs,e("p",null,[Os,t(": "),e("a",js,[t("http://arxiv.org/abs/2403.19506v1"),a(n)])]),Zs,Ns,Ys,e("p",null,[Js,t(": "),e("a",Ks,[t("http://arxiv.org/abs/2403.19735v1"),a(n)])]),Vs,Us,Qs,e("p",null,[Xs,t(": "),e("a",$s,[t("http://arxiv.org/abs/2403.19390v1"),a(n)])]),er,tr,nr,e("p",null,[ar,t(": "),e("a",ir,[t("http://arxiv.org/abs/2403.19347v1"),a(n)])]),or,sr,rr,e("p",null,[lr,t(": "),e("a",hr,[t("http://arxiv.org/abs/2403.19318v1"),a(n)])]),cr,dr,ur,e("p",null,[gr,t(": "),e("a",pr,[t("http://arxiv.org/abs/2403.19302v1"),a(n)])]),mr,fr,vr,e("p",null,[br,t(": "),e("a",yr,[t("http://arxiv.org/abs/2403.19114v1"),a(n)])]),Lr,wr,_r,e("p",null,[kr,t(": "),e("a",xr,[t("http://arxiv.org/abs/2403.19094v1"),a(n)])]),Mr,Ar,Tr,Sr,e("p",null,[Cr,t(": "),e("a",Ir,[t("http://arxiv.org/abs/2403.19021v1"),a(n)])]),qr,zr,Wr,e("p",null,[Pr,t(": "),e("a",Er,[t("http://arxiv.org/abs/2403.18721v1"),a(n)])]),Gr,Rr,Hr,e("p",null,[Dr,t(": "),e("a",Fr,[t("http://arxiv.org/abs/2403.18647v1"),a(n)])]),Br,Or,jr,e("p",null,[Zr,t(": "),e("a",Nr,[t("http://arxiv.org/abs/2403.18403v1"),a(n)])]),Yr,Jr,Kr,e("p",null,[Vr,t(": "),e("a",Ur,[t("http://arxiv.org/abs/2403.18349v1"),a(n)])]),Qr,Xr,$r,e("p",null,[el,t(": "),e("a",tl,[t("http://arxiv.org/abs/2403.18327v1"),a(n)])]),nl,al,il,e("p",null,[ol,t(": "),e("a",sl,[t("http://arxiv.org/abs/2403.18249v1"),a(n)])]),rl,ll,hl,e("p",null,[cl,t(": "),e("a",dl,[t("http://arxiv.org/abs/2403.18173v1"),a(n)])]),ul,gl,pl,ml,e("p",null,[fl,t(": "),e("a",vl,[t("http://arxiv.org/abs/2403.18120v1"),a(n)])]),bl,yl,Ll,e("p",null,[wl,t(": "),e("a",_l,[t("http://arxiv.org/abs/2403.17927v1"),a(n)])]),kl,xl,Ml,e("p",null,[Al,t(": "),e("a",Tl,[t("http://arxiv.org/abs/2403.17860v1"),a(n)])]),Sl,Cl,Il,e("p",null,[ql,t(": "),e("a",zl,[t("http://arxiv.org/abs/2403.17856v1"),a(n)])]),Wl,Pl,El,e("p",null,[Gl,t(": "),e("a",Rl,[t("http://arxiv.org/abs/2403.17819v1"),a(n)])]),Hl])}const jl=i(h,[["render",Dl],["__file","LLM.html.vue"]]),Zl=JSON.parse('{"path":"/posts/paper/LLM.html","title":"LLM","lang":"en-US","frontmatter":{"description":"LLM 2024-04-04 Training LLMs over Neurally Compressed Text Authors: Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant...","head":[["meta",{"property":"og:url","content":"https://opendesign.world/posts/paper/LLM.html"}],["meta",{"property":"og:site_name","content":"OpenDesign"}],["meta",{"property":"og:title","content":"LLM"}],["meta",{"property":"og:description","content":"LLM 2024-04-04 Training LLMs over Neurally Compressed Text Authors: Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-04-07T10:05:54.000Z"}],["meta",{"property":"article:modified_time","content":"2024-04-07T10:05:54.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-04-07T10:05:54.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":2,"title":"2024-04-04","slug":"_2024-04-04","link":"#_2024-04-04","children":[]},{"level":2,"title":"2024-04-03","slug":"_2024-04-03","link":"#_2024-04-03","children":[]},{"level":2,"title":"2024-04-02","slug":"_2024-04-02","link":"#_2024-04-02","children":[]},{"level":2,"title":"2024-04-01","slug":"_2024-04-01","link":"#_2024-04-01","children":[]},{"level":2,"title":"2024-03-31","slug":"_2024-03-31","link":"#_2024-03-31","children":[]},{"level":2,"title":"2024-03-30","slug":"_2024-03-30","link":"#_2024-03-30","children":[]},{"level":2,"title":"2024-03-29","slug":"_2024-03-29","link":"#_2024-03-29","children":[]},{"level":2,"title":"2024-03-28","slug":"_2024-03-28","link":"#_2024-03-28","children":[]},{"level":2,"title":"2024-03-27","slug":"_2024-03-27","link":"#_2024-03-27","children":[]},{"level":2,"title":"2024-03-26","slug":"_2024-03-26","link":"#_2024-03-26","children":[]}],"git":{"updatedTime":1712484354000,"contributors":[{"name":"xue160709","email":"xue160709@gmail.com","commits":1}]},"filePathRelative":"posts/paper/LLM.md","autoDesc":true,"excerpt":"\\n<h2>2024-04-04</h2>\\n<h4>Training LLMs over Neurally Compressed Text</h4>\\n<p><strong>Authors</strong>: Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant</p>\\n<p><strong>Link</strong>: <a href=\\"http://arxiv.org/abs/2404.03626v1\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">http://arxiv.org/abs/2404.03626v1</a></p>"}');export{jl as comp,Zl as data};
