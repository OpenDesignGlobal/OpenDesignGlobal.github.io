import{_ as i,r as o,o as r,c as s,a as e,b as t,d as a,e as l}from"./app-b0nPsb4c.js";const c={},d=l('<h1 id="xr" tabindex="-1"><a class="header-anchor" href="#xr"><span>XR</span></a></h1><h2 id="_2024-04-11" tabindex="-1"><a class="header-anchor" href="#_2024-04-11"><span>2024-04-11</span></a></h2><h4 id="toward-ultra-efficient-high-fidelity-predictions-of-wind-turbine-wakes-augmenting-the-accuracy-of-engineering-models-via-les-trained-machine-learning" tabindex="-1"><a class="header-anchor" href="#toward-ultra-efficient-high-fidelity-predictions-of-wind-turbine-wakes-augmenting-the-accuracy-of-engineering-models-via-les-trained-machine-learning"><span>Toward ultra-efficient high fidelity predictions of wind turbine wakes: Augmenting the accuracy of engineering models via LES-trained machine learning</span></a></h4><p><strong>Authors</strong>: Christian Santoni, Dichang Zhang, Zexia Zhang, Dimitris Samaras, Fotis Sotiropoulos, Ali Khosronejad</p>',4),h=e("strong",null,"Link",-1),u={href:"http://arxiv.org/abs/2404.07938v1",target:"_blank",rel:"noopener noreferrer"},g=e("p",null,[e("strong",null,"Abstract"),t(": This study proposes a novel machine learning (ML) methodology for the efficient and cost-effective prediction of high-fidelity three-dimensional velocity fields in the wake of utility-scale turbines. The model consists of an auto-encoder convolutional neural network with U-Net skipped connections, fine-tuned using high-fidelity data from large-eddy simulations (LES). The trained model takes the low-fidelity velocity field cost-effectively generated from the analytical engineering wake model as input and produces the high-fidelity velocity fields. The accuracy of the proposed ML model is demonstrated in a utility-scale wind farm for which datasets of wake flow fields were previously generated using LES under various wind speeds, wind directions, and yaw angles. Comparing the ML model results with those of LES, the ML model was shown to reduce the error in the prediction from 20% obtained from the GCH model to less than 5%. In addition, the ML model captured the non-symmetric wake deflection observed for opposing yaw angles for wake steering cases, demonstrating a greater accuracy than the GCH model. The computational cost of the ML model is on par with that of the analytical wake model while generating numerical outcomes nearly as accurate as those of the high-fidelity LES.")],-1),p=e("h4",{id:"nostra-domina-at-evalatin-2024-improving-latin-polarity-detection-through-data-augmentation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#nostra-domina-at-evalatin-2024-improving-latin-polarity-detection-through-data-augmentation"},[e("span",null,"Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection through Data Augmentation")])],-1),m=e("p",null,[e("strong",null,"Authors"),t(": Stephen Bothwell, Abigail Swenor, David Chiang")],-1),f=e("strong",null,"Link",-1),v={href:"http://arxiv.org/abs/2404.07792v1",target:"_blank",rel:"noopener noreferrer"},b=e("p",null,[e("strong",null,"Abstract"),t(": This paper describes submissions from the team Nostra Domina to the EvaLatin 2024 shared task of emotion polarity detection. Given the low-resource environment of Latin and the complexity of sentiment in rhetorical genres like poetry, we augmented the available data through automatic polarity annotation. We present two methods for doing so on the basis of the $k$-means algorithm, and we employ a variety of Latin large language models (LLMs) in a neural architecture to better capture the underlying contextual sentiment representations. Our best approach achieved the second highest macro-averaged Macro-$F_1$ score on the shared task's test set.")],-1),y=e("h4",{id:"simba-mamba-augmented-u-shiftgcn-for-skeletal-action-recognition-in-videos",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#simba-mamba-augmented-u-shiftgcn-for-skeletal-action-recognition-in-videos"},[e("span",null,"Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in Videos")])],-1),w=e("p",null,[e("strong",null,"Authors"),t(": Soumyabrata Chaudhuri, Saumik Bhattacharya")],-1),_=e("strong",null,"Link",-1),x={href:"http://arxiv.org/abs/2404.07645v1",target:"_blank",rel:"noopener noreferrer"},k=e("p",null,[e("strong",null,"Abstract"),t(": Skeleton Action Recognition (SAR) involves identifying human actions using skeletal joint coordinates and their interconnections. While plain Transformers have been attempted for this task, they still fall short compared to the current leading methods, which are rooted in Graph Convolutional Networks (GCNs) due to the absence of structural priors. Recently, a novel selective state space model, Mamba, has surfaced as a compelling alternative to the attention mechanism in Transformers, offering efficient modeling of long sequences. In this work, to the utmost extent of our awareness, we present the first SAR framework incorporating Mamba. Each fundamental block of our model adopts a novel U-ShiftGCN architecture with Mamba as its core component. The encoder segment of the U-ShiftGCN is devised to extract spatial features from the skeletal data using downsampling vanilla Shift S-GCN blocks. These spatial features then undergo intermediate temporal modeling facilitated by the Mamba block before progressing to the encoder section, which comprises vanilla upsampling Shift S-GCN blocks. Additionally, a Shift T-GCN (ShiftTCN) temporal modeling unit is employed before the exit of each fundamental block to refine temporal representations. This particular integration of downsampling spatial, intermediate temporal, upsampling spatial, and ultimate temporal subunits yields promising results for skeleton action recognition. We dub the resulting model \\textbf{Simba}, which attains state-of-the-art performance across three well-known benchmark skeleton action recognition datasets: NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA. Interestingly, U-ShiftGCN (Simba without Intermediate Mamba Block) by itself is capable of performing reasonably well and surpasses our baseline.")],-1),A=e("h4",{id:"promoting-collective-cooperation-through-temporal-interactions",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#promoting-collective-cooperation-through-temporal-interactions"},[e("span",null,"Promoting collective cooperation through temporal interactions")])],-1),L=e("p",null,[e("strong",null,"Authors"),t(": Yao Meng, Alex McAvoy, Aming Li")],-1),T=e("strong",null,"Link",-1),R={href:"http://arxiv.org/abs/2404.07530v1",target:"_blank",rel:"noopener noreferrer"},M=e("p",null,[e("strong",null,"Abstract"),t(": Collective cooperation drives the dynamics of many natural, social, and economic phenomena, making understanding the evolution of cooperation with evolutionary game theory a central question of modern science. Although human interactions are best described as complex networks, current explorations are limited to static networks where interactions represented by network links are permanent and do not change over time. In reality, human activities often involve temporal interactions, where links are impermanent, and understanding the evolution of cooperation on such ubiquitous temporal networks is an open question. Here, we present a general framework for systematically analyzing how collective cooperation evolves on any temporal network, which unifies the study of evolutionary game dynamics with dynamic and static interactions. We show that the emergence of cooperation is facilitated by a simple rule of thumb: hubs (individuals with many social ties) should be temporally deprioritized in interactions. We further provide a quantitative metric capturing the priority of hubs, which we utilize to orchestrate the ordering of interactions to best promote cooperation on empirical temporal networks. Our findings unveil the fundamental advantages conferred by temporal interactions for promoting collective cooperation, which transcends the specific insights gleaned from studying traditional static snapshots.")],-1),C=e("h4",{id:"generalization-gap-in-data-augmentation-insights-from-illumination",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#generalization-gap-in-data-augmentation-insights-from-illumination"},[e("span",null,"Generalization Gap in Data Augmentation: Insights from Illumination")])],-1),S=e("p",null,[e("strong",null,"Authors"),t(": Jianqiang Xiao, Weiwen Guo, Junfeng Liu, Mengze Li")],-1),I=e("strong",null,"Link",-1),D={href:"http://arxiv.org/abs/2404.07514v1",target:"_blank",rel:"noopener noreferrer"},z=e("p",null,[e("strong",null,"Abstract"),t(": In the field of computer vision, data augmentation is widely used to enrich the feature complexity of training datasets with deep learning techniques. However, regarding the generalization capabilities of models, the difference in artificial features generated by data augmentation and natural visual features has not been fully revealed. This study focuses on the visual representation variable 'illumination', by simulating its distribution degradation and examining how data augmentation techniques enhance model performance on a classification task. Our goal is to investigate the differences in generalization between models trained with augmented data and those trained under real-world illumination conditions. Results indicate that after undergoing various data augmentation methods, model performance has been significantly improved. Yet, a noticeable generalization gap still exists after utilizing various data augmentation methods, emphasizing the critical role of feature diversity in the training set for enhancing model generalization.")],-1),E=e("h4",{id:"learning-to-classify-new-foods-incrementally-via-compressed-exemplars",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learning-to-classify-new-foods-incrementally-via-compressed-exemplars"},[e("span",null,"Learning to Classify New Foods Incrementally Via Compressed Exemplars")])],-1),q=e("p",null,[e("strong",null,"Authors"),t(": Justin Yang, Zhihao Duan, Jiangpeng He, Fengqing Zhu")],-1),W=e("strong",null,"Link",-1),H={href:"http://arxiv.org/abs/2404.07507v1",target:"_blank",rel:"noopener noreferrer"},P=e("p",null,[e("strong",null,"Abstract"),t(": Food image classification systems play a crucial role in health monitoring and diet tracking through image-based dietary assessment techniques. However, existing food recognition systems rely on static datasets characterized by a pre-defined fixed number of food classes. This contrasts drastically with the reality of food consumption, which features constantly changing data. Therefore, food image classification systems should adapt to and manage data that continuously evolves. This is where continual learning plays an important role. A challenge in continual learning is catastrophic forgetting, where ML models tend to discard old knowledge upon learning new information. While memory-replay algorithms have shown promise in mitigating this problem by storing old data as exemplars, they are hampered by the limited capacity of memory buffers, leading to an imbalance between new and previously learned data. To address this, our work explores the use of neural image compression to extend buffer size and enhance data diversity. We introduced the concept of continuously learning a neural compression model to adaptively improve the quality of compressed data and optimize the bitrates per pixel (bpp) to store more exemplars. Our extensive experiments, including evaluations on food-specific datasets including Food-101 and VFN-74, as well as the general dataset ImageNet-100, demonstrate improvements in classification accuracy. This progress is pivotal in advancing more realistic food recognition systems that are capable of adapting to continually evolving data. Moreover, the principles and methodologies we've developed hold promise for broader applications, extending their benefits to other domains of continual machine learning systems.")],-1),G=e("h4",{id:"leveraging-data-augmentation-for-process-information-extraction",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#leveraging-data-augmentation-for-process-information-extraction"},[e("span",null,"Leveraging Data Augmentation for Process Information Extraction")])],-1),F=e("p",null,[e("strong",null,"Authors"),t(": Julian Neuberger, Leonie Doll, Benedict Engelmann, Lars Ackermann, Stefan Jablonski")],-1),O=e("strong",null,"Link",-1),V={href:"http://arxiv.org/abs/2404.07501v1",target:"_blank",rel:"noopener noreferrer"},N=e("p",null,[e("strong",null,"Abstract"),t(": Business Process Modeling projects often require formal process models as a central component. High costs associated with the creation of such formal process models motivated many different fields of research aimed at automated generation of process models from readily available data. These include process mining on event logs, and generating business process models from natural language texts. Research in the latter field is regularly faced with the problem of limited data availability, hindering both evaluation and development of new techniques, especially learning-based ones. To overcome this data scarcity issue, in this paper we investigate the application of data augmentation for natural language text data. Data augmentation methods are well established in machine learning for creating new, synthetic data without human assistance. We find that many of these methods are applicable to the task of business process information extraction, improving the accuracy of extraction. Our study shows, that data augmentation is an important component in enabling machine learning methods for the task of business process model generation from natural language text, where currently mostly rule-based systems are still state of the art. Simple data augmentation techniques improved the $F_1$ score of mention extraction by 2.9 percentage points, and the $F_1$ of relation extraction by $4.5$. To better understand how data augmentation alters human annotated texts, we analyze the resulting text, visualizing and discussing the properties of augmented textual data. We make all code and experiments results publicly available.")],-1),B=e("h4",{id:"rassar-room-accessibility-and-safety-scanning-in-augmented-reality",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#rassar-room-accessibility-and-safety-scanning-in-augmented-reality"},[e("span",null,"RASSAR: Room Accessibility and Safety Scanning in Augmented Reality")])],-1),j=e("p",null,[e("strong",null,"Authors"),t(": Xia Su, Han Zhang, Kaiming Cheng, Jaewook Lee, Qiaochu Liu, Wyatt Olson, Jon Froehlich")],-1),J=e("strong",null,"Link",-1),K={href:"http://arxiv.org/abs/2404.07479v1",target:"_blank",rel:"noopener noreferrer"},U=e("p",null,[e("strong",null,"Abstract"),t(": The safety and accessibility of our homes is critical to quality of life and evolves as we age, become ill, host guests, or experience life events such as having children. Researchers and health professionals have created assessment instruments such as checklists that enable homeowners and trained experts to identify and mitigate safety and access issues. With advances in computer vision, augmented reality (AR), and mobile sensors, new approaches are now possible. We introduce RASSAR, a mobile AR application for semi-automatically identifying, localizing, and visualizing indoor accessibility and safety issues such as an inaccessible table height or unsafe loose rugs using LiDAR and real-time computer vision. We present findings from three studies: a formative study with 18 participants across five stakeholder groups to inform the design of RASSAR, a technical performance evaluation across ten homes demonstrating state-of-the-art performance, and a user study with six stakeholders. We close with a discussion of future AI-based indoor accessibility assessment tools, RASSAR's extensibility, and key application scenarios.")],-1),Y=e("h4",{id:"acronym-augmented-degree-corrected-community-reticulated-organized-network-yielding-model",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#acronym-augmented-degree-corrected-community-reticulated-organized-network-yielding-model"},[e("span",null,"ACRONYM: Augmented degree corrected, Community Reticulated Organized Network Yielding Model")])],-1),X=e("p",null,[e("strong",null,"Authors"),t(": Benjamin Leinwand, Vince Lyzinski")],-1),Z=e("strong",null,"Link",-1),Q={href:"http://arxiv.org/abs/2404.07462v1",target:"_blank",rel:"noopener noreferrer"},$=e("p",null,[e("strong",null,"Abstract"),t(": Modeling networks can serve as a means of summarizing high-dimensional complex systems. Adapting an approach devised for dense, weighted networks, we propose a new method for generating and estimating unweighted networks. This approach can describe a broader class of potential networks than existing models, including those where nodes in different subnetworks connect to one another via various attachment mechanisms, inducing flexible and varied community structures. While unweighted edges provide less resolution than continuous weights, restricting to the binary case permits the use of likelihood-based estimation techniques, which can improve estimation of nodal features. The extra flexibility may contribute a different understanding of network generating structures, particularly for networks with heterogeneous densities in different regions.")],-1),ee=e("h2",{id:"_2024-04-10",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-10"},[e("span",null,"2024-04-10")])],-1),te=e("h4",{id:"mixed-reality-heritage-performance-as-a-decolonising-tool-for-heritage-sites",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#mixed-reality-heritage-performance-as-a-decolonising-tool-for-heritage-sites"},[e("span",null,"Mixed Reality Heritage Performance As a Decolonising Tool for Heritage Sites")])],-1),ne=e("p",null,[e("strong",null,"Authors"),t(": Mariza Dima, Damon Daylamani-Zad, Vangelis Lympouridis")],-1),ae=e("strong",null,"Link",-1),ie={href:"http://arxiv.org/abs/2404.07348v1",target:"_blank",rel:"noopener noreferrer"},oe=e("p",null,[e("strong",null,"Abstract"),t(": In this paper we introduce two world-first Mixed Reality (MR) experiences that fuse smart AR glasses and live theatre and take place in a heritage site with the purpose to reveal the site's hidden and difficult histories about slavery. We term these unique general audience experiences Mixed Reality Heritage Performances (MRHP). Along with the development of our initial two performances we designed and developed a tool and guidelines that can help heritage organisations with their decolonising process by critically engaging the public with under-represented voices and viewpoints of troubled European and colonial narratives. The evaluations showed the embodied and affective potential of MRHP to attract and educate heritage audiences visitors. Insights of the design process are being formulated into an extensive design toolkit that aims to support experience design, theatre and heritage professionals to collaboratively carry out similar projects.")],-1),re=e("h4",{id:"evaluating-navigation-and-comparison-performance-of-computational-notebooks-on-desktop-and-in-virtual-reality",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evaluating-navigation-and-comparison-performance-of-computational-notebooks-on-desktop-and-in-virtual-reality"},[e("span",null,"Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality")])],-1),se=e("p",null,[e("strong",null,"Authors"),t(": Sungwon In, Erick Krokos, Kirsten Whitley, Chris North, Yalong Yang")],-1),le=e("strong",null,"Link",-1),ce={href:"http://arxiv.org/abs/2404.07161v1",target:"_blank",rel:"noopener noreferrer"},de=e("p",null,[e("strong",null,"Abstract"),t(": The computational notebook serves as a versatile tool for data analysis. However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches. With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows. Virtual reality, in particular, has demonstrated its potential in interactive data visualizations. In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring. We focus on the navigation and comparison aspects as they are primitive components in analysts' workflow. To further improve comparison, we have designed and implemented a Branching&Merging functionality. We tested computational notebooks on the desktop and in VR, both with and without the added Branching&Merging capability. We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison.")],-1),he=e("h4",{id:"exploring-physiological-responses-in-virtual-reality-based-interventions-for-autism-spectrum-disorder-a-data-driven-investigation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-physiological-responses-in-virtual-reality-based-interventions-for-autism-spectrum-disorder-a-data-driven-investigation"},[e("span",null,"Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorder: A Data-Driven Investigation")])],-1),ue=e("p",null,[e("strong",null,"Authors"),t(": Gianpaolo Alvari, Ersilia Vallefuoco, Melanie Cristofolini, Elio Salvadori, Marco Dianti, Alessia Moltani, Davide Dal Castello, Paola Venuti, Cesare Furlanello")],-1),ge=e("strong",null,"Link",-1),pe={href:"http://arxiv.org/abs/2404.07159v1",target:"_blank",rel:"noopener noreferrer"},me=e("p",null,[e("strong",null,"Abstract"),t(": Virtual Reality (VR) has emerged as a promising tool for enhancing social skills and emotional well-being in individuals with Autism Spectrum Disorder (ASD). Through a technical exploration, this study employs a multiplayer serious gaming environment within VR, engaging 34 individuals diagnosed with ASD and employing high-precision biosensors for a comprehensive view of the participants' arousal and responses during the VR sessions. Participants were subjected to a series of 3 virtual scenarios designed in collaboration with stakeholders and clinical experts to promote socio-cognitive skills and emotional regulation in a controlled and structured virtual environment. We combined the framework with wearable non-invasive sensors for bio-signal acquisition, focusing on the collection of heart rate variability, and respiratory patterns to monitor participants behaviors. Further, behavioral assessments were conducted using observation and semi-structured interviews, with the data analyzed in conjunction with physiological measures to identify correlations and explore digital-intervention efficacy. Preliminary analysis revealed significant correlations between physiological responses and behavioral outcomes, indicating the potential of physiological feedback to enhance VR-based interventions for ASD. The study demonstrated the feasibility of using real-time data to adapt virtual scenarios, suggesting a promising avenue to support personalized therapy. The integration of quantitative physiological feedback into digital platforms represents a forward step in the personalized intervention for ASD. By leveraging real-time data to adjust therapeutic content, this approach promises to enhance the efficacy and engagement of digital-based therapies.")],-1),fe=e("h4",{id:"graph-chain-of-thought-augmenting-large-language-models-by-reasoning-on-graphs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#graph-chain-of-thought-augmenting-large-language-models-by-reasoning-on-graphs"},[e("span",null,"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs")])],-1),ve=e("p",null,[e("strong",null,"Authors"),t(": Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Suhang Wang, Yu Meng, Jiawei Han")],-1),be=e("strong",null,"Link",-1),ye={href:"http://arxiv.org/abs/2404.07103v1",target:"_blank",rel:"noopener noreferrer"},we=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBench, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRBench, where Graph-CoT outperforms the baselines consistently. The code is available at https://github.com/PeterGriffinJin/Graph-CoT.")],-1),_e=e("h4",{id:"groundedness-in-retrieval-augmented-long-form-generation-an-empirical-study",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#groundedness-in-retrieval-augmented-long-form-generation-an-empirical-study"},[e("span",null,"Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study")])],-1),xe=e("p",null,[e("strong",null,"Authors"),t(": Alessandro Stolfo")],-1),ke=e("strong",null,"Link",-1),Ae={href:"http://arxiv.org/abs/2404.07060v1",target:"_blank",rel:"noopener noreferrer"},Le=e("p",null,[e("strong",null,"Abstract"),t(": We present an empirical study of groundedness in long-form question answering (LFQA) by retrieval-augmented large language models (LLMs). In particular, we evaluate whether every generated sentence is grounded in the retrieved documents or the model's pre-training data. Across 3 datasets and 4 model families, our findings reveal that a significant fraction of generated sentences are consistently ungrounded, even when those sentences contain correct ground-truth answers. Additionally, we examine the impacts of factors such as model size, decoding strategy, and instruction tuning on groundedness. Our results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations. This study provides novel insights into the groundedness challenges in LFQA and underscores the necessity for more robust mechanisms in LLMs to mitigate the generation of ungrounded content.")],-1),Te=e("h4",{id:"superposition-prompting-improving-and-accelerating-retrieval-augmented-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#superposition-prompting-improving-and-accelerating-retrieval-augmented-generation"},[e("span",null,"Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation")])],-1),Re=e("p",null,[e("strong",null,"Authors"),t(": Thomas Merth, Qichen Fu, Mohammad Rastegari, Mahyar Najibi")],-1),Me=e("strong",null,"Link",-1),Ce={href:"http://arxiv.org/abs/2404.06910v1",target:"_blank",rel:"noopener noreferrer"},Se=e("p",null,[e("strong",null,"Abstract"),t(': Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the "distraction phenomenon," where irrelevant context in the prompt degrades output quality. To address these drawbacks, we propose a novel RAG prompting methodology, superposition prompting, which can be directly applied to pre-trained transformer-based LLMs without the need for fine-tuning. At a high level, superposition prompting allows the LLM to process input documents in parallel prompt paths, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on. For example, our approach facilitates an 93x reduction in compute time while improving accuracy by 43% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.')],-1),Ie=e("h4",{id:"sara-smart-ai-reading-assistant-for-reading-comprehension",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sara-smart-ai-reading-assistant-for-reading-comprehension"},[e("span",null,"SARA: Smart AI Reading Assistant for Reading Comprehension")])],-1),De=e("p",null,[e("strong",null,"Authors"),t(": Enkeleda Thaqi, Mohamed Mantawy, Enkelejda Kasneci")],-1),ze=e("strong",null,"Link",-1),Ee={href:"http://arxiv.org/abs/2404.06906v1",target:"_blank",rel:"noopener noreferrer"},qe=e("p",null,[e("strong",null,"Abstract"),t(": SARA integrates Eye Tracking and state-of-the-art large language models in a mixed reality framework to enhance the reading experience by providing personalized assistance in real-time. By tracking eye movements, SARA identifies the text segments that attract the user's attention the most and potentially indicate uncertain areas and comprehension issues. The process involves these key steps: text detection and extraction, gaze tracking and alignment, and assessment of detected reading difficulty. The results are customized solutions presented directly within the user's field of view as virtual overlays on identified difficult text areas. This support enables users to overcome challenges like unfamiliar vocabulary and complex sentences by offering additional context, rephrased solutions, and multilingual help. SARA's innovative approach demonstrates it has the potential to transform the reading experience and improve reading proficiency.")],-1),We=e("h4",{id:"dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#dreamscene360-unconstrained-text-to-3d-scene-generation-with-panoramic-gaussian-splatting"},[e("span",null,"DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting")])],-1),He=e("p",null,[e("strong",null,"Authors"),t(": Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi")],-1),Pe=e("strong",null,"Link",-1),Ge={href:"http://arxiv.org/abs/2404.06903v1",target:"_blank",rel:"noopener noreferrer"},Fe=e("p",null,[e("strong",null,"Abstract"),t(': The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary "flat" (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: http://dreamscene360.github.io/')],-1),Oe=e("h4",{id:"harnessing-the-power-of-ai-generated-content-for-semantic-communication",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#harnessing-the-power-of-ai-generated-content-for-semantic-communication"},[e("span",null,"Harnessing the Power of AI-Generated Content for Semantic Communication")])],-1),Ve=e("p",null,[e("strong",null,"Authors"),t(": Yiru Wang, Wanting Yang, Zehui Xiong, Yuping Zhao, Tony Q. S. Quek, Zhu Han")],-1),Ne=e("strong",null,"Link",-1),Be={href:"http://arxiv.org/abs/2404.06765v1",target:"_blank",rel:"noopener noreferrer"},je=e("p",null,[e("strong",null,"Abstract"),t(": Semantic Communication (SemCom) is envisaged as the next-generation paradigm to address challenges stemming from the conflicts between the increasing volume of transmission data and the scarcity of spectrum resources. However, existing SemCom systems face drawbacks, such as low explainability, modality rigidity, and inadequate reconstruction functionality. Recognizing the transformative capabilities of AI-generated content (AIGC) technologies in content generation, this paper explores a pioneering approach by integrating them into SemCom to address the aforementioned challenges. We employ a three-layer model to illustrate the proposed AIGC-assisted SemCom (AIGC-SCM) architecture, emphasizing its clear deviation from existing SemCom. Grounded in this model, we investigate various AIGC technologies with the potential to augment SemCom's performance. In alignment with SemCom's goal of conveying semantic meanings, we also introduce the new evaluation methods for our AIGC-SCM system. Subsequently, we explore communication scenarios where our proposed AIGC-SCM can realize its potential. For practical implementation, we construct a detailed integration workflow and conduct a case study in a virtual reality image transmission scenario. The results demonstrate our ability to maintain a high degree of alignment between the reconstructed content and the original source information, while substantially minimizing the data volume required for transmission. These findings pave the way for further enhancements in communication efficiency and the improvement of Quality of Service. At last, we present future directions for AIGC-SCM studies.")],-1),Je=e("h4",{id:"an-animation-based-augmentation-approach-for-action-recognition-from-discontinuous-video",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#an-animation-based-augmentation-approach-for-action-recognition-from-discontinuous-video"},[e("span",null,"An Animation-based Augmentation Approach for Action Recognition from Discontinuous Video")])],-1),Ke=e("p",null,[e("strong",null,"Authors"),t(": Xingyu Song, Zhan Li, Shi Chen, Xin-Qiang Cai, Kazuyuki Demachi")],-1),Ue=e("strong",null,"Link",-1),Ye={href:"http://arxiv.org/abs/2404.06741v1",target:"_blank",rel:"noopener noreferrer"},Xe=e("p",null,[e("strong",null,"Abstract"),t(": The study of action recognition has attracted considerable attention recently due to its broad applications in multiple areas. However, with the issue of discontinuous training video, which not only decreases the performance of action recognition model, but complicates the data augmentation process as well, still remains under-exploration. In this study, we introduce the 4A (Action Animation-based Augmentation Approach), an innovative pipeline for data augmentation to address the problem. The main contributions remain in our work includes: (1) we investigate the problem of severe decrease on performance of action recognition task training by discontinuous video, and the limitation of existing augmentation methods on solving this problem. (2) we propose a novel augmentation pipeline, 4A, to address the problem of discontinuous video for training, while achieving a smoother and natural-looking action representation than the latest data augmentation methodology. (3) We achieve the same performance with only 10% of the original data for training as with all of the original data from the real-world dataset, and a better performance on In-the-wild videos, by employing our data augmentation techniques.")],-1),Ze=e("h4",{id:"sparse-points-to-dense-clouds-enhancing-3d-detection-with-limited-lidar-data",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sparse-points-to-dense-clouds-enhancing-3d-detection-with-limited-lidar-data"},[e("span",null,"Sparse Points to Dense Clouds: Enhancing 3D Detection with Limited LiDAR Data")])],-1),Qe=e("p",null,[e("strong",null,"Authors"),t(": Aakash Kumar, Chen Chen, Ajmal Mian, Neils Lobo, Mubarak Shah")],-1),$e=e("strong",null,"Link",-1),et={href:"http://arxiv.org/abs/2404.06715v1",target:"_blank",rel:"noopener noreferrer"},tt=e("p",null,[e("strong",null,"Abstract"),t(": 3D detection is a critical task that enables machines to identify and locate objects in three-dimensional space. It has a broad range of applications in several fields, including autonomous driving, robotics and augmented reality. Monocular 3D detection is attractive as it requires only a single camera, however, it lacks the accuracy and robustness required for real world applications. High resolution LiDAR on the other hand, can be expensive and lead to interference problems in heavy traffic given their active transmissions. We propose a balanced approach that combines the advantages of monocular and point cloud-based 3D detection. Our method requires only a small number of 3D points, that can be obtained from a low-cost, low-resolution sensor. Specifically, we use only 512 points, which is just 1% of a full LiDAR frame in the KITTI dataset. Our method reconstructs a complete 3D point cloud from this limited 3D information combined with a single image. The reconstructed 3D point cloud and corresponding image can be used by any multi-modal off-the-shelf detector for 3D object detection. By using the proposed network architecture with an off-the-shelf multi-modal 3D detector, the accuracy of 3D detection improves by 20% compared to the state-of-the-art monocular detection methods and 6% to 9% compare to the baseline multi-modal methods on KITTI and JackRabbot datasets.")],-1),nt=e("h2",{id:"_2024-04-09",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-09"},[e("span",null,"2024-04-09")])],-1),at=e("h4",{id:"federated-learning-model-for-predicting-major-postoperative-complications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#federated-learning-model-for-predicting-major-postoperative-complications"},[e("span",null,"Federated learning model for predicting major postoperative complications")])],-1),it=e("p",null,[e("strong",null,"Authors"),t(": Yonggi Park, Yuanfang Ren, Benjamin Shickel, Ziyuan Guan, Ayush Patela, Yingbo Ma, Zhenhong Hu, Tyler J. Loftus, Parisa Rashidi, Tezcan Ozrazgat-Baslanti, Azra Bihorac")],-1),ot=e("strong",null,"Link",-1),rt={href:"http://arxiv.org/abs/2404.06641v1",target:"_blank",rel:"noopener noreferrer"},st=e("p",null,[e("strong",null,"Abstract"),t(": Background: The accurate prediction of postoperative complication risk using Electronic Health Records (EHR) and artificial intelligence shows great potential. Training a robust artificial intelligence model typically requires large-scale and diverse datasets. In reality, collecting medical data often encounters challenges surrounding privacy protection. Methods: This retrospective cohort study includes adult patients who were admitted to UFH Gainesville (GNV) (n = 79,850) and Jacksonville (JAX) (n = 28,636) for any type of inpatient surgical procedure. Using perioperative and intraoperative features, we developed federated learning models to predict nine major postoperative complications (i.e., prolonged intensive care unit stay and mechanical ventilation). We compared federated learning models with local learning models trained on a single site and central learning models trained on pooled dataset from two centers. Results: Our federated learning models achieved the area under the receiver operating characteristics curve (AUROC) values ranged from 0.81 for wound complications to 0.92 for prolonged ICU stay at UFH GNV center. At UFH JAX center, these values ranged from 0.73-0.74 for wound complications to 0.92-0.93 for hospital mortality. Federated learning models achieved comparable AUROC performance to central learning models, except for prolonged ICU stay, where the performance of federated learning models was slightly higher than central learning models at UFH GNV center, but slightly lower at UFH JAX center. In addition, our federated learning model obtained comparable performance to the best local learning model at each center, demonstrating strong generalizability. Conclusion: Federated learning is shown to be a useful tool to train robust and generalizable models from large scale data across multiple institutions where data protection barriers are high.")],-1),lt=e("h4",{id:"evolving-loss-functions-for-specific-image-augmentation-techniques",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evolving-loss-functions-for-specific-image-augmentation-techniques"},[e("span",null,"Evolving Loss Functions for Specific Image Augmentation Techniques")])],-1),ct=e("p",null,[e("strong",null,"Authors"),t(": Brandon Morgan, Dean Hougen")],-1),dt=e("strong",null,"Link",-1),ht={href:"http://arxiv.org/abs/2404.06633v1",target:"_blank",rel:"noopener noreferrer"},ut=e("p",null,[e("strong",null,"Abstract"),t(": Previous work in Neural Loss Function Search (NLFS) has shown a lack of correlation between smaller surrogate functions and large convolutional neural networks with massive regularization. We expand upon this research by revealing another disparity that exists, correlation between different types of image augmentation techniques. We show that different loss functions can perform well on certain image augmentation techniques, while performing poorly on others. We exploit this disparity by performing an evolutionary search on five types of image augmentation techniques in the hopes of finding image augmentation specific loss functions. The best loss functions from each evolution were then taken and transferred to WideResNet-28-10 on CIFAR-10 and CIFAR-100 across each of the five image augmentation techniques. The best from that were then taken and evaluated by fine-tuning EfficientNetV2Small on the CARS, Oxford-Flowers, and Caltech datasets across each of the five image augmentation techniques. Multiple loss functions were found that outperformed cross-entropy across multiple experiments. In the end, we found a single loss function, which we called the inverse bessel logarithm loss, that was able to outperform cross-entropy across the majority of experiments.")],-1),gt=e("h4",{id:"training-free-open-vocabulary-segmentation-with-offline-diffusion-augmented-prototype-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#training-free-open-vocabulary-segmentation-with-offline-diffusion-augmented-prototype-generation"},[e("span",null,"Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation")])],-1),pt=e("p",null,[e("strong",null,"Authors"),t(": Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara")],-1),mt=e("strong",null,"Link",-1),ft={href:"http://arxiv.org/abs/2404.06542v1",target:"_blank",rel:"noopener noreferrer"},vt=e("p",null,[e("strong",null,"Abstract"),t(": Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form. Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal alignments. However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts. Further, training on large-scale datasets inevitably brings significant computational costs. In this paper, we propose FreeDA, a training-free diffusion-augmented method for open-vocabulary semantic segmentation, which leverages the ability of diffusion models to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts. At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities. Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training.")],-1),bt=e("h4",{id:"hyperparameter-selection-in-continual-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#hyperparameter-selection-in-continual-learning"},[e("span",null,"Hyperparameter Selection in Continual Learning")])],-1),yt=e("p",null,[e("strong",null,"Authors"),t(": Thomas L. Lee, Sigrid Passano Hellan, Linus Ericsson, Elliot J. Crowley, Amos Storkey")],-1),wt=e("strong",null,"Link",-1),_t={href:"http://arxiv.org/abs/2404.06466v1",target:"_blank",rel:"noopener noreferrer"},xt=e("p",null,[e("strong",null,"Abstract"),t(": In continual learning (CL) -- where a learner trains on a stream of data -- standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time. This has prompted the development of CL-specific HPO frameworks. The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings. However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once. Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality? This paper answers this question by evaluating several realistic HPO frameworks. We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly. We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training.")],-1),kt=e("h4",{id:"matching-2d-images-in-3d-metric-relative-pose-from-metric-correspondences",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#matching-2d-images-in-3d-metric-relative-pose-from-metric-correspondences"},[e("span",null,"Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences")])],-1),At=e("p",null,[e("strong",null,"Authors"),t(": Axel Barroso-Laguna, Sowmya Munukutla, Victor Adrian Prisacariu, Eric Brachmann")],-1),Lt=e("strong",null,"Link",-1),Tt={href:"http://arxiv.org/abs/2404.06337v1",target:"_blank",rel:"noopener noreferrer"},Rt=e("p",null,[e("strong",null,"Abstract"),t(": Given two images, we can estimate the relative camera pose between them by establishing image-to-image correspondences. Usually, correspondences are 2D-to-2D and the pose we estimate is defined only up to scale. Some applications, aiming at instant augmented reality anywhere, require scale-metric pose estimates, and hence, they rely on external depth estimators to recover the scale. We present MicKey, a keypoint matching pipeline that is able to predict metric correspondences in 3D camera space. By learning to match 3D coordinates across images, we are able to infer the metric relative pose without depth measurements. Depth measurements are also not required for training, nor are scene reconstructions or image overlap information. MicKey is supervised only by pairs of images and their relative poses. MicKey achieves state-of-the-art performance on the Map-Free Relocalisation benchmark while requiring less supervision than competing approaches.")],-1),Mt=e("h4",{id:"open-source-ai-based-se-tools-opportunities-and-challenges-of-collaborative-software-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#open-source-ai-based-se-tools-opportunities-and-challenges-of-collaborative-software-learning"},[e("span",null,"Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning")])],-1),Ct=e("p",null,[e("strong",null,"Authors"),t(": Zhihao Lin, Wei Ma, Tao Lin, Yaowen Zheng, Jingquan Ge, Jun Wang, Jacques Klein, Tegawende Bissyande, Yang Liu, Li Li")],-1),St=e("strong",null,"Link",-1),It={href:"http://arxiv.org/abs/2404.06201v1",target:"_blank",rel:"noopener noreferrer"},Dt=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. Like traditional SE tools, open-source collaboration is key in realising the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data. However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community. Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organisations. Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance.")],-1),zt=e("h4",{id:"streamlined-transmission-a-semantic-aware-xr-deployment-framework-enhanced-by-generative-ai",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#streamlined-transmission-a-semantic-aware-xr-deployment-framework-enhanced-by-generative-ai"},[e("span",null,"Streamlined Transmission: A Semantic-Aware XR Deployment Framework Enhanced by Generative AI")])],-1),Et=e("p",null,[e("strong",null,"Authors"),t(": Wanting Yang, Zehui Xiong, Tony Q. S. Quek, Xuemin Shen")],-1),qt=e("strong",null,"Link",-1),Wt={href:"http://arxiv.org/abs/2404.06182v1",target:"_blank",rel:"noopener noreferrer"},Ht=e("p",null,[e("strong",null,"Abstract"),t(': In the era of 6G, featuring compelling visions of digital twins and metaverses, Extended Reality (XR) has emerged as a vital conduit connecting the digital and physical realms, garnering widespread interest. Ensuring a fully immersive wireless XR experience stands as a paramount technical necessity, demanding the liberation of XR from the confines of wired connections. In this paper, we first introduce the technologies applied in the wireless XR domain, delve into their benefits and limitations, and highlight the ongoing challenges. We then propose a novel deployment framework for a broad XR pipeline, termed "GeSa-XRF", inspired by the core philosophy of Semantic Communication (SemCom) which shifts the concern from "how" to transmit to "what" to transmit. Particularly, the framework comprises three stages: data collection, data analysis, and data delivery. In each stage, we integrate semantic awareness to achieve streamlined transmission and employ Generative Artificial Intelligence (GAI) to achieve collaborative refinements. For the data collection of multi-modal data with differentiated data volumes and heterogeneous latency requirements, we propose a novel SemCom paradigm based on multi-modal fusion and separation and a GAI-based robust superposition scheme. To perform a comprehensive data analysis, we employ multi-task learning to perform the prediction of field of view and personalized attention and discuss the possible preprocessing approaches assisted by GAI. Lastly, for the data delivery stage, we present a semantic-aware multicast-based delivery strategy aimed at reducing pixel level redundant transmissions and introduce the GAI collaborative refinement approach. The performance gain of the proposed GeSa-XRF is preliminarily demonstrated through a case study.')],-1),Pt=e("h4",{id:"hfnerf-learning-human-biomechanic-features-with-neural-radiance-fields",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#hfnerf-learning-human-biomechanic-features-with-neural-radiance-fields"},[e("span",null,"HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields")])],-1),Gt=e("p",null,[e("strong",null,"Authors"),t(": Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet")],-1),Ft=e("strong",null,"Link",-1),Ot={href:"http://arxiv.org/abs/2404.06152v1",target:"_blank",rel:"noopener noreferrer"},Vt=e("p",null,[e("strong",null,"Abstract"),t(": In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF.")],-1),Nt=e("h4",{id:"making-old-kurdish-publications-processable-by-augmenting-available-optical-character-recognition-engines",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#making-old-kurdish-publications-processable-by-augmenting-available-optical-character-recognition-engines"},[e("span",null,"Making Old Kurdish Publications Processable by Augmenting Available Optical Character Recognition Engines")])],-1),Bt=e("p",null,[e("strong",null,"Authors"),t(": Blnd Yaseen, Hossein Hassani")],-1),jt=e("strong",null,"Link",-1),Jt={href:"http://arxiv.org/abs/2404.06101v1",target:"_blank",rel:"noopener noreferrer"},Kt=e("p",null,[e("strong",null,"Abstract"),t(": Kurdish libraries have many historical publications that were printed back in the early days when printing devices were brought to Kurdistan. Having a good Optical Character Recognition (OCR) to help process these publications and contribute to the Kurdish languages resources which is crucial as Kurdish is considered a low-resource language. Current OCR systems are unable to extract text from historical documents as they have many issues, including being damaged, very fragile, having many marks left on them, and often written in non-standard fonts and more. This is a massive obstacle in processing these documents as currently processing them requires manual typing which is very time-consuming. In this study, we adopt an open-source OCR framework by Google, Tesseract version 5.0, that has been used to extract text for various languages. Currently, there is no public dataset, and we developed our own by collecting historical documents from Zheen Center for Documentation and Research, which were printed before 1950 and resulted in a dataset of 1233 images of lines with transcription of each. Then we used the Arabic model as our base model and trained the model using the dataset. We used different methods to evaluate our model, Tesseracts built-in evaluator lstmeval indicated a Character Error Rate (CER) of 0.755%. Additionally, Ocreval demonstrated an average character accuracy of 84.02%. Finally, we developed a web application to provide an easy- to-use interface for end-users, allowing them to interact with the model by inputting an image of a page and extracting the text. Having an extensive dataset is crucial to develop OCR systems with reasonable accuracy, as currently, no public datasets are available for historical Kurdish documents; this posed a significant challenge in our work. Additionally, the unaligned spaces between characters and words proved another challenge with our work.")],-1),Ut=e("h4",{id:"eve-enabling-anyone-to-train-robot-using-augmented-reality",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#eve-enabling-anyone-to-train-robot-using-augmented-reality"},[e("span",null,"EVE: Enabling Anyone to Train Robot using Augmented Reality")])],-1),Yt=e("p",null,[e("strong",null,"Authors"),t(": Jun Wang, Chun-Cheng Chang, Jiafei Duan, Dieter Fox, Ranjay Krishna")],-1),Xt=e("strong",null,"Link",-1),Zt={href:"http://arxiv.org/abs/2404.06089v1",target:"_blank",rel:"noopener noreferrer"},Qt=e("p",null,[e("strong",null,"Abstract"),t(": The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators. Consequently, only those with access to physical robots produce demonstrations to train robots. To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$). We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.")],-1),$t=e("h4",{id:"improving-facial-landmark-detection-accuracy-and-efficiency-with-knowledge-distillation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#improving-facial-landmark-detection-accuracy-and-efficiency-with-knowledge-distillation"},[e("span",null,"Improving Facial Landmark Detection Accuracy and Efficiency with Knowledge Distillation")])],-1),en=e("p",null,[e("strong",null,"Authors"),t(": Zong-Wei Hong, Yu-Chen Lin")],-1),tn=e("strong",null,"Link",-1),nn={href:"http://arxiv.org/abs/2404.06029v1",target:"_blank",rel:"noopener noreferrer"},an=e("p",null,[e("strong",null,"Abstract"),t(": The domain of computer vision has experienced significant advancements in facial-landmark detection, becoming increasingly essential across various applications such as augmented reality, facial recognition, and emotion analysis. Unlike object detection or semantic segmentation, which focus on identifying objects and outlining boundaries, faciallandmark detection aims to precisely locate and track critical facial features. However, deploying deep learning-based facial-landmark detection models on embedded systems with limited computational resources poses challenges due to the complexity of facial features, especially in dynamic settings. Additionally, ensuring robustness across diverse ethnicities and expressions presents further obstacles. Existing datasets often lack comprehensive representation of facial nuances, particularly within populations like those in Taiwan. This paper introduces a novel approach to address these challenges through the development of a knowledge distillation method. By transferring knowledge from larger models to smaller ones, we aim to create lightweight yet powerful deep learning models tailored specifically for facial-landmark detection tasks. Our goal is to design models capable of accurately locating facial landmarks under varying conditions, including diverse expressions, orientations, and lighting environments. The ultimate objective is to achieve high accuracy and real-time performance suitable for deployment on embedded systems. This method was successfully implemented and achieved a top 6th place finish out of 165 participants in the IEEE ICME 2024 PAIR competition.")],-1),on=e("h4",{id:"optimization-methods-for-personalizing-large-language-models-through-retrieval-augmentation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#optimization-methods-for-personalizing-large-language-models-through-retrieval-augmentation"},[e("span",null,"Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation")])],-1),rn=e("p",null,[e("strong",null,"Authors"),t(": Alireza Salemi, Surya Kallumadi, Hamed Zamani")],-1),sn=e("strong",null,"Link",-1),ln={href:"http://arxiv.org/abs/2404.05970v1",target:"_blank",rel:"noopener noreferrer"},cn=e("p",null,[e("strong",null,"Abstract"),t(": This paper studies retrieval-augmented approaches for personalizing large language models (LLMs), which potentially have a substantial impact on various applications and domains. We propose the first attempt to optimize the retrieval models that deliver a limited number of personal documents to large language models for the purpose of personalized generation. We develop two optimization algorithms that solicit feedback from the downstream personalized generation tasks for retrieval optimization--one based on reinforcement learning whose reward function is defined using any arbitrary metric for personalized generation and another based on knowledge distillation from the downstream LLM to the retrieval model. This paper also introduces a pre- and post-generation retriever selection model that decides what retriever to choose for each LLM input. Extensive experiments on diverse tasks from the language model personalization (LaMP) benchmark reveal statistically significant improvements in six out of seven datasets.")],-1),dn=e("h2",{id:"_2024-04-08",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-08"},[e("span",null,"2024-04-08")])],-1),hn=e("h4",{id:"the-hallucinations-leaderboard-an-open-effort-to-measure-hallucinations-in-large-language-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-hallucinations-leaderboard-an-open-effort-to-measure-hallucinations-in-large-language-models"},[e("span",null,"The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models")])],-1),un=e("p",null,[e("strong",null,"Authors"),t(": Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu Zhao, Laura Perez-Beltrachini, Max Ryabinin, Xuanli He, Pasquale Minervini")],-1),gn=e("strong",null,"Link",-1),pn={href:"http://arxiv.org/abs/2404.05904v1",target:"_blank",rel:"noopener noreferrer"},mn=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have transformed the Natural Language Processing (NLP) landscape with their remarkable ability to understand and generate human-like text. However, these models are prone to ``hallucinations'' -- outputs that do not align with factual reality or the input context. This paper introduces the Hallucinations Leaderboard, an open initiative to quantitatively measure and compare the tendency of each model to produce hallucinations. The leaderboard uses a comprehensive set of benchmarks focusing on different aspects of hallucinations, such as factuality and faithfulness, across various tasks, including question-answering, summarisation, and reading comprehension. Our analysis provides insights into the performance of different models, guiding researchers and practitioners in choosing the most reliable models for their applications.")],-1),fn=e("h4",{id:"with-or-without-permission-site-specific-augmented-reality-for-social-justice-chi-2024-workshop-proceedings",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#with-or-without-permission-site-specific-augmented-reality-for-social-justice-chi-2024-workshop-proceedings"},[e("span",null,"With or Without Permission: Site-Specific Augmented Reality for Social Justice CHI 2024 Workshop Proceedings")])],-1),vn=e("p",null,[e("strong",null,"Authors"),t(": Rafael M. L. Silva, Ana Mara Crdenas Gasca, Joshua A. Fisher, Erica Principe Cruz, Cinthya Jauregui, Amy Lueck, Fannie Liu, Andrs Monroy-Hernndez, Kai Lukoff")],-1),bn=e("strong",null,"Link",-1),yn={href:"http://arxiv.org/abs/2404.05889v2",target:"_blank",rel:"noopener noreferrer"},wn=e("p",null,[e("strong",null,"Abstract"),t(": This volume represents the proceedings of With or Without Permission: Site-Specific Augmented Reality for Social Justice CHI 2024 workshop.")],-1),_n=e("h4",{id:"a-realistic-surgical-simulator-for-non-rigid-and-contact-rich-manipulation-in-surgeries-with-the-da-vinci-research-kit",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-realistic-surgical-simulator-for-non-rigid-and-contact-rich-manipulation-in-surgeries-with-the-da-vinci-research-kit"},[e("span",null,"A Realistic Surgical Simulator for Non-Rigid and Contact-Rich Manipulation in Surgeries with the da Vinci Research Kit")])],-1),xn=e("p",null,[e("strong",null,"Authors"),t(": Yafei Ou, Sadra Zargarzadeh, Paniz Sedighi, Mahdi Tavakoli")],-1),kn=e("strong",null,"Link",-1),An={href:"http://arxiv.org/abs/2404.05888v1",target:"_blank",rel:"noopener noreferrer"},Ln=e("p",null,[e("strong",null,"Abstract"),t(": Realistic real-time surgical simulators play an increasingly important role in surgical robotics research, such as surgical robot learning and automation, and surgical skills assessment. Although there are a number of existing surgical simulators for research, they generally lack the ability to simulate the diverse types of objects and contact-rich manipulation tasks typically present in surgeries, such as tissue cutting and blood suction. In this work, we introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the da Vinci Research Kit (dVRK) that enables simulating various contact-rich surgical tasks involving different surgical instruments, soft tissue, and body fluids. The real-world dVRK console and the master tool manipulator (MTM) robots are incorporated into the system to allow for teleoperation through virtual reality (VR). To showcase the advantages and potentials of the simulator, we present three examples of surgical tasks, including tissue grasping and deformation, blood suction, and tissue cutting. These tasks are performed using the simulated surgical instruments, including the large needle driver, suction irrigator, and curved scissor, through VR-based teleoperation.")],-1),Tn=e("h4",{id:"on-the-fly-robotic-assisted-medical-instrument-planning-and-execution-using-mixed-reality",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#on-the-fly-robotic-assisted-medical-instrument-planning-and-execution-using-mixed-reality"},[e("span",null,"On the Fly Robotic-Assisted Medical Instrument Planning and Execution Using Mixed Reality")])],-1),Rn=e("p",null,[e("strong",null,"Authors"),t(": Letian Ai, Yihao Liu, Mehran Armand, Amir Kheradmand, Alejandro Martin-Gomez")],-1),Mn=e("strong",null,"Link",-1),Cn={href:"http://arxiv.org/abs/2404.05887v1",target:"_blank",rel:"noopener noreferrer"},Sn=e("p",null,[e("strong",null,"Abstract"),t(": Robotic-assisted medical systems (RAMS) have gained significant attention for their advantages in alleviating surgeons' fatigue and improving patients' outcomes. These systems comprise a range of human-computer interactions, including medical scene monitoring, anatomical target planning, and robot manipulation. However, despite its versatility and effectiveness, RAMS demands expertise in robotics, leading to a high learning cost for the operator. In this work, we introduce a novel framework using mixed reality technologies to ease the use of RAMS. The proposed framework achieves real-time planning and execution of medical instruments by providing 3D anatomical image overlay, human-robot collision detection, and robot programming interface. These features, integrated with an easy-to-use calibration method for head-mounted display, improve the effectiveness of human-robot interactions. To assess the feasibility of the framework, two medical applications are presented in this work: 1) coil placement during transcranial magnetic stimulation and 2) drill and injector device positioning during femoroplasty. Results from these use cases demonstrate its potential to extend to a wider range of medical scenarios.")],-1),In=e("h4",{id:"llm-augmented-retrieval-enhancing-retrieval-models-through-language-models-and-doc-level-embedding",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-augmented-retrieval-enhancing-retrieval-models-through-language-models-and-doc-level-embedding"},[e("span",null,"LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding")])],-1),Dn=e("p",null,[e("strong",null,"Authors"),t(": Mingrui Wu, Sheng Cao")],-1),zn=e("strong",null,"Link",-1),En={href:"http://arxiv.org/abs/2404.05825v1",target:"_blank",rel:"noopener noreferrer"},qn=e("p",null,[e("strong",null,"Abstract"),t(": Recently embedding-based retrieval or dense retrieval have shown state of the art results, compared with traditional sparse or bag-of-words based approaches. This paper introduces a model-agnostic doc-level embedding framework through large language model (LLM) augmentation. In addition, it also improves some important components in the retrieval model training process, such as negative sampling, loss function, etc. By implementing this LLM-augmented retrieval framework, we have been able to significantly improve the effectiveness of widely-used retriever models such as Bi-encoders (Contriever, DRAGON) and late-interaction models (ColBERTv2), thereby achieving state-of-the-art results on LoTTE datasets and BEIR datasets.")],-1),Wn=e("h4",{id:"ma-lmm-memory-augmented-large-multimodal-model-for-long-term-video-understanding",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ma-lmm-memory-augmented-large-multimodal-model-for-long-term-video-understanding"},[e("span",null,"MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding")])],-1),Hn=e("p",null,[e("strong",null,"Authors"),t(": Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim")],-1),Pn=e("strong",null,"Link",-1),Gn={href:"http://arxiv.org/abs/2404.05726v1",target:"_blank",rel:"noopener noreferrer"},Fn=e("p",null,[e("strong",null,"Abstract"),t(": With the success of large language models (LLMs), integrating the vision model into LLMs to build vision-language foundation models has gained much more interest recently. However, existing LLM-based large multimodal models (e.g., Video-LLaMA, VideoChat) can only take in a limited number of frames for short video understanding. In this study, we mainly focus on designing an efficient and effective model for long-term video understanding. Instead of trying to process more frames simultaneously like most existing work, we propose to process videos in an online manner and store past video information in a memory bank. This allows our model to reference historical video content for long-term analysis without exceeding LLMs' context length constraints or GPU memory limits. Our memory bank can be seamlessly integrated into current multimodal LLMs in an off-the-shelf manner. We conduct extensive experiments on various video understanding tasks, such as long-video understanding, video question answering, and video captioning, and our model can achieve state-of-the-art performances across multiple datasets. Code available at https://boheumd.github.io/MA-LMM/.")],-1),On=e("h4",{id:"evaluating-the-efficacy-of-cut-and-paste-data-augmentation-in-semantic-segmentation-for-satellite-imagery",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evaluating-the-efficacy-of-cut-and-paste-data-augmentation-in-semantic-segmentation-for-satellite-imagery"},[e("span",null,"Evaluating the Efficacy of Cut-and-Paste Data Augmentation in Semantic Segmentation for Satellite Imagery")])],-1),Vn=e("p",null,[e("strong",null,"Authors"),t(": Ionut M. Motoi, Leonardo Saraceni, Daniele Nardi, Thomas A. Ciarfuglia")],-1),Nn=e("strong",null,"Link",-1),Bn={href:"http://arxiv.org/abs/2404.05693v1",target:"_blank",rel:"noopener noreferrer"},jn=e("p",null,[e("strong",null,"Abstract"),t(": Satellite imagery is crucial for tasks like environmental monitoring and urban planning. Typically, it relies on semantic segmentation or Land Use Land Cover (LULC) classification to categorize each pixel. Despite the advancements brought about by Deep Neural Networks (DNNs), their performance in segmentation tasks is hindered by challenges such as limited availability of labeled data, class imbalance and the inherent variability and complexity of satellite images. In order to mitigate those issues, our study explores the effectiveness of a Cut-and-Paste augmentation technique for semantic segmentation in satellite images. We adapt this augmentation, which usually requires labeled instances, to the case of semantic segmentation. By leveraging the connected components in the semantic segmentation labels, we extract instances that are then randomly pasted during training. Using the DynamicEarthNet dataset and a U-Net model for evaluation, we found that this augmentation significantly enhances the mIoU score on the test set from 37.9 to 44.1. This finding highlights the potential of the Cut-and-Paste augmentation to improve the generalization capabilities of semantic segmentation models in satellite imagery.")],-1),Jn=e("h4",{id:"retrieval-augmented-open-vocabulary-object-detection",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#retrieval-augmented-open-vocabulary-object-detection"},[e("span",null,"Retrieval-Augmented Open-Vocabulary Object Detection")])],-1),Kn=e("p",null,[e("strong",null,"Authors"),t(": Jooyeon Kim, Eulrang Cho, Sehyung Kim, Hyunwoo J. Kim")],-1),Un=e("strong",null,"Link",-1),Yn={href:"http://arxiv.org/abs/2404.05687v1",target:"_blank",rel:"noopener noreferrer"},Xn=e("p",null,[e("strong",null,"Abstract"),t(": Open-vocabulary object detection (OVD) has been studied with Vision-Language Models (VLMs) to detect novel objects beyond the pre-trained categories. Previous approaches improve the generalization ability to expand the knowledge of the detector, using 'positive' pseudo-labels with additional 'class' names, e.g., sock, iPod, and alligator. To extend the previous methods in two aspects, we propose Retrieval-Augmented Losses and visual Features (RALF). Our method retrieves related 'negative' classes and augments loss functions. Also, visual features are augmented with 'verbalized concepts' of classes, e.g., worn on the feet, handheld music player, and sharp teeth. Specifically, RALF consists of two modules: Retrieval Augmented Losses (RAL) and Retrieval-Augmented visual Features (RAF). RAL constitutes two losses reflecting the semantic similarity with negative vocabularies. In addition, RAF augments visual features with the verbalized concepts from a large language model (LLM). Our experiments demonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets. We achieve improvement up to 3.4 box AP$"),e("em",null,"{50}^{\\text{N}}$ on novel categories of the COCO dataset and 3.6 mask AP$"),t("{\\text{r}}$ gains on the LVIS dataset. Code is available at https://github.com/mlvlab/RALF .")],-1),Zn=e("h4",{id:"two-person-interaction-augmentation-with-skeleton-priors",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#two-person-interaction-augmentation-with-skeleton-priors"},[e("span",null,"Two-Person Interaction Augmentation with Skeleton Priors")])],-1),Qn=e("p",null,[e("strong",null,"Authors"),t(": Baiyi Li, Edmond S. L. Ho, Hubert P. H. Shum, He Wang")],-1),$n=e("strong",null,"Link",-1),ea={href:"http://arxiv.org/abs/2404.05490v2",target:"_blank",rel:"noopener noreferrer"},ta=e("p",null,[e("strong",null,"Abstract"),t(": Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep learning method for two-body skeletal interaction motion augmentation, which can generate variations of contact-rich interactions with varying body sizes and proportions while retaining the key geometric/topological relations between two bodies. Our system can learn effectively from a relatively small amount of data and generalize to drastically different skeleton sizes. Through exhaustive evaluation and comparison, we show it can generate high-quality motions, has strong generalizability and outperforms traditional optimization-based methods and alternative deep learning solutions.")],-1),na=e("h4",{id:"nanomolecular-oled-pixelization-enabling-electroluminescent-metasurfaces",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#nanomolecular-oled-pixelization-enabling-electroluminescent-metasurfaces"},[e("span",null,"Nanomolecular OLED Pixelization Enabling Electroluminescent Metasurfaces")])],-1),aa=e("p",null,[e("strong",null,"Authors"),t(": Tommaso Marcato, Jiwoo Oh, Zhan-Hong Lin, Sunil B. Shivarudraiah, Sudhir Kumar, Shuangshuang Zeng, Chih-Jen Shih")],-1),ia=e("strong",null,"Link",-1),oa={href:"http://arxiv.org/abs/2404.05336v1",target:"_blank",rel:"noopener noreferrer"},ra=e("p",null,[e("strong",null,"Abstract"),t(": Miniaturization of light-emitting diodes (LEDs) can enable high-resolution augmented and virtual reality displays and on-chip light sources for ultra-broadband chiplet communication. However, unlike silicon scaling in electronic integrated circuits, patterning of inorganic III-V semiconductors in LEDs considerably compromises device efficiencies at submicrometer scales. Here, we present the scalable fabrication of nanoscale organic LEDs (nano-OLEDs), with the highest array density (>84,000 pixels per inch) and the smallest pixel size (~100 nm) ever reported to date. Direct nanomolecular patterning of organic semiconductors is realized by self-aligned evaporation through nanoapertures fabricated on a free-standing silicon nitride film adhering to the substrate. The average external quantum efficiencies (EQEs) extracted from a nano-OLED device of more than 4 megapixels reach up to 10%. At the subwavelength scale, individual pixels act as electroluminescent meta-atoms forming metasurfaces that directly convert electricity into modulated light. The diffractive coupling between nano-pixels enables control over the far-field emission properties, including directionality and polarization. The results presented here lay the foundation for bright surface light sources of dimension smaller than the Abbe diffraction limit, offering new technological platforms for super-resolution imaging, spectroscopy, sensing, and hybrid integrated photonics.")],-1),sa=e("h4",{id:"webxr-a-frame-and-networked-aframe-as-a-basis-for-an-open-metaverse-a-conceptual-architecture",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#webxr-a-frame-and-networked-aframe-as-a-basis-for-an-open-metaverse-a-conceptual-architecture"},[e("span",null,"WebXR, A-Frame and Networked-Aframe as a Basis for an Open Metaverse: A Conceptual Architecture")])],-1),la=e("p",null,[e("strong",null,"Authors"),t(": Giuseppe Macario")],-1),ca=e("strong",null,"Link",-1),da={href:"http://arxiv.org/abs/2404.05317v2",target:"_blank",rel:"noopener noreferrer"},ha=e("p",null,[e("strong",null,"Abstract"),t(": This work proposes a WebXR-based cross-platform conceptual architecture, leveraging the A-Frame and Networked-Aframe frameworks, in order to facilitate the development of an open, accessible, and interoperable metaverse. By introducing the concept of spatial web app, this research contributes to the discourse on the metaverse, offering an architecture that democratizes access to virtual environments and extended reality through the web, and aligns with Tim Berners-Lee's original vision of the World Wide Web as an open platform in the digital realm.")],-1),ua=e("h4",{id:"can-edge-computing-fulfill-the-requirements-of-automated-vehicular-services-using-5g-network",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-edge-computing-fulfill-the-requirements-of-automated-vehicular-services-using-5g-network"},[e("span",null,"Can Edge Computing fulfill the requirements of automated vehicular services using 5G network ?")])],-1),ga=e("p",null,[e("strong",null,"Authors"),t(": Wendlasida Ouedraogo, Andrea Araldo, Badii Jouaber, Hind Castel, Remy Grunblatt")],-1),pa=e("strong",null,"Link",-1),ma={href:"http://arxiv.org/abs/2404.05296v1",target:"_blank",rel:"noopener noreferrer"},fa=e("p",null,[e("strong",null,"Abstract"),t(": Communication and computation services supporting Connected and Automated Vehicles (CAVs) are characterized by stringent requirements, in terms of response time and reliability. Fulfilling these requirements is crucial for ensuring road safety and traffic optimization. The conceptually simple solution of hosting these services in the vehicles increases their cost (mainly due to the installation and maintenance of computation infrastructure) and may drain their battery excessively. Such disadvantages can be tackled via Multi-Access Edge Computing (MEC), consisting in deploying computation capability in network nodes deployed close to the devices (vehicles in this case), such as to satisfy the stringent CAV requirements. However, it is not yet clear under which conditions MEC can support CAV requirements and for which services. To shed light on this question, we conduct a simulation campaign using well-known open-source simulation tools, namely OMNeT++, Simu5G, Veins, INET, and SUMO. We are thus able to provide a reality check on MEC for CAV, pinpointing what are the computation capacities that must be installed in the MEC, to support the different services, and the amount of vehicles that a single MEC node can support. We find that such parameters must vary a lot, depending on the service considered. This study can serve as a preliminary basis for network operators to plan future deployment of MEC to support CAV.")],-1),va=e("h4",{id:"progressive-alignment-with-vlm-llm-feature-to-augment-defect-classification-for-the-ase-dataset",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#progressive-alignment-with-vlm-llm-feature-to-augment-defect-classification-for-the-ase-dataset"},[e("span",null,"Progressive Alignment with VLM-LLM Feature to Augment Defect Classification for the ASE Dataset")])],-1),ba=e("p",null,[e("strong",null,"Authors"),t(": Chih-Chung Hsu, Chia-Ming Lee, Chun-Hung Sun, Kuang-Ming Wu")],-1),ya=e("strong",null,"Link",-1),wa={href:"http://arxiv.org/abs/2404.05183v1",target:"_blank",rel:"noopener noreferrer"},_a=e("p",null,[e("strong",null,"Abstract"),t(`: Traditional defect classification approaches are facing with two barriers. (1) Insufficient training data and unstable data quality. Collecting sufficient defective sample is expensive and time-costing, consequently leading to dataset variance. It introduces the difficulty on recognition and learning. (2) Over-dependence on visual modality. When the image pattern and texture is monotonic for all defect classes in a given dataset, the performance of conventional AOI system cannot be guaranteed. In scenarios where image quality is compromised due to mechanical failures or when defect information is inherently difficult to discern, the performance of deep models cannot be guaranteed. A main question is, "how to solve those two problems when they occur at the same time?" The feasible strategy is to explore another feature within dataset and combine an eminent vision-language model (VLM) and Large-Language model (LLM) with their astonishing zero-shot capability. In this work, we propose the special ASE dataset, including rich data description recorded on image, for defect classification, but the defect feature is uneasy to learn directly. Secondly, We present the prompting for VLM-LLM against defect classification with the proposed ASE dataset to activate extra-modality feature from images to enhance performance. Then, We design the novel progressive feature alignment (PFA) block to refine image-text feature to alleviate the difficulty of alignment under few-shot scenario. Finally, the proposed Cross-modality attention fusion (CMAF) module can effectively fuse different modality feature. Experiment results have demonstrated our method's effectiveness over several defect classification methods for the ASE dataset.`)],-1),xa=e("h4",{id:"stitch-augmented-dexterity-for-suture-throws-including-thread-coordination-and-handoffs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#stitch-augmented-dexterity-for-suture-throws-including-thread-coordination-and-handoffs"},[e("span",null,"STITCH: Augmented Dexterity for Suture Throws Including Thread Coordination and Handoffs")])],-1),ka=e("p",null,[e("strong",null,"Authors"),t(": Kush Hari, Hansoul Kim, Will Panitch, Kishore Srinivas, Vincent Schorp, Karthik Dharmarajan, Shreya Ganti, Tara Sadjadpour, Ken Goldberg")],-1),Aa=e("strong",null,"Link",-1),La={href:"http://arxiv.org/abs/2404.05151v1",target:"_blank",rel:"noopener noreferrer"},Ta=e("p",null,[e("strong",null,"Abstract"),t(": We present STITCH: an augmented dexterity pipeline that performs Suture Throws Including Thread Coordination and Handoffs. STITCH iteratively performs needle insertion, thread sweeping, needle extraction, suture cinching, needle handover, and needle pose correction with failure recovery policies. We introduce a novel visual 6D needle pose estimation framework using a stereo camera pair and new suturing motion primitives. We compare STITCH to baselines, including a proprioception-only and a policy without visual servoing. In physical experiments across 15 trials, STITCH achieves an average of 2.93 sutures without human intervention and 4.47 sutures with human intervention. See https://sites.google.com/berkeley.edu/stitch for code and supplemental materials.")],-1),Ra=e("h4",{id:"oracle-complexity-of-augmented-lagrangian-methods-for-nonsmooth-manifold-optimization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#oracle-complexity-of-augmented-lagrangian-methods-for-nonsmooth-manifold-optimization"},[e("span",null,"Oracle complexity of augmented Lagrangian methods for nonsmooth manifold optimization")])],-1),Ma=e("p",null,[e("strong",null,"Authors"),t(": Kangkang Deng, Jiang Hu, Zaiwen Wen")],-1),Ca=e("strong",null,"Link",-1),Sa={href:"http://arxiv.org/abs/2404.05121v1",target:"_blank",rel:"noopener noreferrer"},Ia=e("p",null,[e("strong",null,"Abstract"),t(": In this paper, we introduce a novel manifold inexact augmented Lagrangian method (\\textbf{ManIAL}) and provide an oracle complexity analysis. To the best of our knowledge, this is the first complexity result for the augmented Lagrangian method for solving nonsmooth problems on Riemannian manifolds. By using the Riemannian gradient method as a subroutine, we establish an oracle complexity result of $\\mathcal{O}(\\epsilon^{-3})$, matching the best-known complexity result. Our algorithm relies on the careful selection of penalty parameters and the precise control of termination criteria for subproblems. Furthermore, we also propose a stochastic manifold inexact augmented Lagrangian method (\\textbf{StoManIAL}) for cases where the smooth term follows an expectation form. By using a Riemannian recursive momentum method as a subroutine, we prove that \\textbf{StoManIAL} achieves an oracle complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$, which is better than the best-known $\\mathcal{O}(\\epsilon^{-4})$ result. Numerical experiments conducted on sparse principal component analysis and sparse canonical correlation analysis demonstrate that our proposed methods outperform an existing method with the previously best-known complexity result.")],-1),Da=e("h2",{id:"_2024-04-07",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-07"},[e("span",null,"2024-04-07")])],-1),za=e("h4",{id:"havtr-improving-video-text-retrieval-through-augmentation-using-large-foundation-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#havtr-improving-video-text-retrieval-through-augmentation-using-large-foundation-models"},[e("span",null,"HaVTR: Improving Video-Text Retrieval Through Augmentation Using Large Foundation Models")])],-1),Ea=e("p",null,[e("strong",null,"Authors"),t(": Yimu Wang, Shuai Yuan, Xiangru Jian, Wei Pang, Mushi Wang, Ning Yu")],-1),qa=e("strong",null,"Link",-1),Wa={href:"http://arxiv.org/abs/2404.05083v1",target:"_blank",rel:"noopener noreferrer"},Ha=e("p",null,[e("strong",null,"Abstract"),t(": While recent progress in video-text retrieval has been driven by the exploration of powerful model architectures and training strategies, the representation learning ability of video-text retrieval models is still limited due to low-quality and scarce training data annotations. To address this issue, we present a novel video-text learning paradigm, HaVTR, which augments video and text data to learn more generalized features. Specifically, we first adopt a simple augmentation method, which generates self-similar data by randomly duplicating or dropping subwords and frames. In addition, inspired by the recent advancement in visual and language generative models, we propose a more powerful augmentation method through textual paraphrasing and video stylization using large language models (LLMs) and visual generative models (VGMs). Further, to bring richer information into video and text, we propose a hallucination-based augmentation method, where we use LLMs and VGMs to generate and add new relevant information to the original data. Benefiting from the enriched data, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of HaVTR over existing methods.")],-1),Pa=e("h4",{id:"reduction-of-forgetting-by-contextual-variation-during-encoding-using-360-degree-video-based-immersive-virtual-environments",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#reduction-of-forgetting-by-contextual-variation-during-encoding-using-360-degree-video-based-immersive-virtual-environments"},[e("span",null,"Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments")])],-1),Ga=e("p",null,[e("strong",null,"Authors"),t(": Takato Mizuho, Takuji Narumi, Hideaki Kuzuoka")],-1),Fa=e("strong",null,"Link",-1),Oa={href:"http://arxiv.org/abs/2404.05007v1",target:"_blank",rel:"noopener noreferrer"},Va=e("p",null,[e("strong",null,"Abstract"),t(": Recall impairment in a different environmental context from learning is called context-dependent forgetting. Two learning methods have been proposed to prevent context-dependent forgetting: reinstatement and decontextualization. Reinstatement matches the environmental context between learning and retrieval, whereas decontextualization involves repeated learning in various environmental contexts and eliminates the context dependency of memory. Conventionally, these methods have been validated by switching between physical rooms. However, in this study, we use immersive virtual environments (IVEs) as the environmental context assisted by virtual reality (VR), which is known for its low cost and high reproducibility compared to traditional manipulation. Whereas most existing studies using VR have failed to reveal the reinstatement effect, we test its occurrence using a 360-degree video-based IVE with improved familiarity and realism instead of a computer graphics-based IVE. Furthermore, we are the first to address decontextualization using VR. Our experiment showed that repeated learning in the same constant IVE as retrieval did not significantly reduce forgetting compared to repeated learning in different constant IVEs. Conversely, repeated learning in various IVEs significantly reduced forgetting than repeated learning in constant IVEs. These findings contribute to the design of IVEs for VR-based applications, particularly in educational settings.")],-1),Na=e("h4",{id:"pairaug-what-can-augmented-image-text-pairs-do-for-radiology",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#pairaug-what-can-augmented-image-text-pairs-do-for-radiology"},[e("span",null,"PairAug: What Can Augmented Image-Text Pairs Do for Radiology?")])],-1),Ba=e("p",null,[e("strong",null,"Authors"),t(": Yutong Xie, Qi Chen, Sinuo Wang, Minh-Son To, Iris Lee, Ee Win Khoo, Kerolos Hendy, Daniel Koh, Yong Xia, Qi Wu")],-1),ja=e("strong",null,"Link",-1),Ja={href:"http://arxiv.org/abs/2404.04960v1",target:"_blank",rel:"noopener noreferrer"},Ka=e("p",null,[e("strong",null,"Abstract"),t(": Current vision-language pre-training (VLP) methodologies predominantly depend on paired image-text datasets, a resource that is challenging to acquire in radiology due to privacy considerations and labelling complexities. Data augmentation provides a practical solution to overcome the issue of data scarcity, however, most augmentation methods exhibit a limited focus, prioritising either image or text augmentation exclusively. Acknowledging this limitation, our objective is to devise a framework capable of concurrently augmenting medical image and text data. We design a Pairwise Augmentation (PairAug) approach that contains an Inter-patient Augmentation (InterAug) branch and an Intra-patient Augmentation (IntraAug) branch. Specifically, the InterAug branch of our approach generates radiology images using synthesised yet plausible reports derived from a Large Language Model (LLM). The generated pairs can be considered a collection of new patient cases since they are artificially created and may not exist in the original dataset. In contrast, the IntraAug branch uses newly generated reports to manipulate images. This process allows us to create new paired data for each individual with diverse medical conditions. Our extensive experiments on various downstream tasks covering medical image classification zero-shot and fine-tuning analysis demonstrate that our PairAug, concurrently expanding both image and text data, substantially outperforms image-/text-only expansion baselines and advanced medical VLP baselines. Our code is released at \\url{https://github.com/YtongXie/PairAug}.")],-1),Ua=e("h4",{id:"low-resource-machine-translation-through-retrieval-augmented-llm-prompting-a-study-on-the-mambai-language",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#low-resource-machine-translation-through-retrieval-augmented-llm-prompting-a-study-on-the-mambai-language"},[e("span",null,"Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language")])],-1),Ya=e("p",null,[e("strong",null,"Authors"),t(": Raphal Merx, Aso Mahmudi, Katrina Langford, Leo Alberto de Araujo, Ekaterina Vylomova")],-1),Xa=e("strong",null,"Link",-1),Za={href:"http://arxiv.org/abs/2404.04809v1",target:"_blank",rel:"noopener noreferrer"},Qa=e("p",null,[e("strong",null,"Abstract"),t(": This study explores the use of large language models (LLMs) for translating English into Mambai, a low-resource Austronesian language spoken in Timor-Leste, with approximately 200,000 native speakers. Leveraging a novel corpus derived from a Mambai language manual and additional sentences translated by a native speaker, we examine the efficacy of few-shot LLM prompting for machine translation (MT) in this low-resource context. Our methodology involves the strategic selection of parallel sentences and dictionary entries for prompting, aiming to enhance translation accuracy, using open-source and proprietary LLMs (LlaMa 2 70b, Mixtral 8x7B, GPT-4). We find that including dictionary entries in prompts and a mix of sentences retrieved through TF-IDF and semantic embeddings significantly improves translation quality. However, our findings reveal stark disparities in translation performance across test sets, with BLEU scores reaching as high as 21.2 on materials from the language manual, in contrast to a maximum of 4.4 on a test set provided by a native speaker. These results underscore the importance of diverse and representative corpora in assessing MT for low-resource languages. Our research provides insights into few-shot LLM prompting for low-resource MT, and makes available an initial corpus for the Mambai language.")],-1),$a=e("h4",{id:"application-of-the-modular-bayesian-approach-for-inverse-uncertainty-quantification-in-nuclear-thermal-hydraulics-systems",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#application-of-the-modular-bayesian-approach-for-inverse-uncertainty-quantification-in-nuclear-thermal-hydraulics-systems"},[e("span",null,"Application of the Modular Bayesian Approach for Inverse Uncertainty Quantification in Nuclear Thermal-Hydraulics Systems")])],-1),ei=e("p",null,[e("strong",null,"Authors"),t(": Chen Wang")],-1),ti=e("strong",null,"Link",-1),ni={href:"http://arxiv.org/abs/2404.04774v1",target:"_blank",rel:"noopener noreferrer"},ai=e("p",null,[e("strong",null,"Abstract"),t(': In the framework of BEPU (Best Estimate plus Uncertainty) methodology, the uncertainties involved in the simulations must be quantified to prove that the investigated design is acceptable. The output uncertainties are usually calculated by propagating input uncertainties through the simulation model, which requires knowledge of the model input uncertainties. However, in some best-estimate Thermal-Hydraulics (TH) codes such as TRACE, the physical model parameters used in empirical correlations may have large uncertainties, which are unknown to the code users. Therefore, obtaining uncertainty distributions of those parameters becomes crucial if we want to study the predictive uncertainty or output sensitivity. In this study, we present a Modular Bayesian approach that considers the presence model discrepancy during Bayesian calibration. Several TRACE physical model parameters are selected as calibration parameters in this work. Model discrepancy, also referred to as model inadequacy or model bias, accounts for the inaccuracy in computer simulation caused by underlying missing/insufficient physics, numerical approximation errors, and other errors of a computer code, even if all its parameters are fixed at their "true" values. Model discrepancy always exists in computer models because they are reduced representations of the reality. The consideration of model discrepancy is important because it can help avoid the "overfitting" problem in Bayesian calibration. This paper uses a set of steady-state experimental data from PSBT benchmark and it mainly aims at: (1) quantifying the uncertainties of TRACE physical model parameters based on experiment data; (2) quantifying the uncertainties in TRACE outputs based on inversely quantified physical model parameters uncertainties.')],-1),ii=e("h2",{id:"_2024-04-06",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-06"},[e("span",null,"2024-04-06")])],-1),oi=e("h4",{id:"teleaware-robot-designing-awareness-augmented-telepresence-robot-for-remote-collaborative-locomotion",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#teleaware-robot-designing-awareness-augmented-telepresence-robot-for-remote-collaborative-locomotion"},[e("span",null,"TeleAware Robot: Designing Awareness-augmented Telepresence Robot for Remote Collaborative Locomotion")])],-1),ri=e("p",null,[e("strong",null,"Authors"),t(": Ruyi Li, Yaxin Zhu, Min Liu, Yihang Zeng, Shanning Zhuang, Jiayi Fu, Yi Lu, Guyue Zhou, Can Liu, Jiangtao Gong")],-1),si=e("strong",null,"Link",-1),li={href:"http://arxiv.org/abs/2404.04579v1",target:"_blank",rel:"noopener noreferrer"},ci=e("p",null,[e("strong",null,"Abstract"),t(": Telepresence robots can be used to support users to navigate an environment remotely and share the visiting experience with their social partners. Although such systems allow users to see and hear the remote environment and communicate with their partners via live video feed, this does not provide enough awareness of the environment and their remote partner's activities. In this paper, we introduce an awareness framework for collaborative locomotion in scenarios of onsite and remote users visiting a place together. From an observational study of small groups of people visiting exhibitions, we derived four design goals for enhancing the environmental and social awareness between social partners, and developed a set of awareness-enhancing techniques to add to a standard telepresence robot - named TeleAware robot. Through a controlled experiment simulating a guided exhibition visiting task, TeleAware robot showed the ability to lower the workload, facilitate closer social proximity, and improve mutual awareness and social presence compared with the standard one. We discuss the impact of mobility and roles of local and remote users, and provide insights for the future design of awareness-enhancing telepresence robot systems that facilitate collaborative locomotion.")],-1),di=e("h2",{id:"_2024-04-05",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-05"},[e("span",null,"2024-04-05")])],-1),hi=e("h4",{id:"high-frequency-capacitive-sensing-for-electrohydraulic-soft-actuators",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#high-frequency-capacitive-sensing-for-electrohydraulic-soft-actuators"},[e("span",null,"High-Frequency Capacitive Sensing for Electrohydraulic Soft Actuators")])],-1),ui=e("p",null,[e("strong",null,"Authors"),t(": Michel R. Vogt, Maximilian Eberlein, Clemens C. Christoph, Felix Baumann, Fabrice Bourquin, Wim Wende, Fabio Schaub, Amirhossein Kazemipour, Robert K. Katzschmann")],-1),gi=e("strong",null,"Link",-1),pi={href:"http://arxiv.org/abs/2404.04071v2",target:"_blank",rel:"noopener noreferrer"},mi=e("p",null,[e("strong",null,"Abstract"),t(": The need for compliant and proprioceptive actuators has grown more evident in pursuing more adaptable and versatile robotic systems. Hydraulically Amplified Self-Healing Electrostatic (HASEL) actuators offer distinctive advantages with their inherent softness and flexibility, making them promising candidates for various robotic tasks, including delicate interactions with humans and animals, biomimetic locomotion, prosthetics, and exoskeletons. This has resulted in a growing interest in the capacitive self-sensing capabilities of HASEL actuators to create miniature displacement estimation circuitry that does not require external sensors. However, achieving HASEL self-sensing for actuation frequencies above 1 Hz and with miniature high-voltage power supplies has remained limited. In this paper, we introduce the F-HASEL actuator, which adds an additional electrode pair used exclusively for capacitive sensing to a Peano-HASEL actuator. We demonstrate displacement estimation of the F-HASEL during high-frequency actuation up to 20 Hz and during external loading using miniaturized circuitry comprised of low-cost off-the-shelf components and a miniature high-voltage power supply. Finally, we propose a circuitry to estimate the displacement of multiple F-HASELs and demonstrate it in a wearable application to track joint rotations of a virtual reality user in real-time.")],-1),fi=e("h4",{id:"data-augmentation-with-in-context-learning-and-comparative-evaluation-in-math-word-problem-solving",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#data-augmentation-with-in-context-learning-and-comparative-evaluation-in-math-word-problem-solving"},[e("span",null,"Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving")])],-1),vi=e("p",null,[e("strong",null,"Authors"),t(": Gulsum Yigit, Mehmet Fatih Amasyali")],-1),bi=e("strong",null,"Link",-1),yi={href:"http://arxiv.org/abs/2404.03938v1",target:"_blank",rel:"noopener noreferrer"},wi=e("p",null,[e("strong",null,"Abstract"),t(": Math Word Problem (MWP) solving presents a challenging task in Natural Language Processing (NLP). This study aims to provide MWP solvers with a more diverse training set, ultimately improving their ability to solve various math problems. We propose several methods for data augmentation by modifying the problem texts and equations, such as synonym replacement, rule-based: question replacement, and rule based: reversing question methodologies over two English MWP datasets. This study extends by introducing a new in-context learning augmentation method, employing the Llama-7b language model. This approach involves instruction-based prompting for rephrasing the math problem texts. Performance evaluations are conducted on 9 baseline models, revealing that augmentation methods outperform baseline models. Moreover, concatenating examples generated by various augmentation methods further improves performance.")],-1),_i=e("h4",{id:"effects-of-multisensory-feedback-on-the-perception-and-performance-of-virtual-reality-hand-retargeted-interaction",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#effects-of-multisensory-feedback-on-the-perception-and-performance-of-virtual-reality-hand-retargeted-interaction"},[e("span",null,"Effects of Multisensory Feedback on the Perception and Performance of Virtual Reality Hand-Retargeted Interaction")])],-1),xi=e("p",null,[e("strong",null,"Authors"),t(": Hyunyoung Jang, Jinwook Kim, Jeongmi Lee")],-1),ki=e("strong",null,"Link",-1),Ai={href:"http://arxiv.org/abs/2404.03899v1",target:"_blank",rel:"noopener noreferrer"},Li=e("p",null,[e("strong",null,"Abstract"),t(": Retargeting methods that modify the visual representation of real movements have been widely used to expand the interaction space and create engaging virtual reality experiences. For optimal user experience and performance, it is essential to specify the perception of retargeting and utilize the appropriate range of modification parameters. However, previous studies mostly concentrated on whether users perceived the target sense or not and rarely examined the perceptual accuracy and sensitivity to retargeting. Moreover, it is unknown how the perception and performance in hand-retargeted interactions are influenced by multisensory feedback. In this study, we used rigorous psychophysical methods to specify users' perceptual accuracy and sensitivity to hand-retargeting and provide acceptable ranges of retargeting parameters. We also presented different multisensory feedback simultaneously with the retargeting to probe its effect on users' perception and task performance. The experimental results showed that providing continuous multisensory feedback, proportionate to the distance between the virtual hand and the targeted destination, heightened the accuracy of users' perception of hand retargeting without altering their perceptual sensitivity. Furthermore, the utilization of multisensory feedback considerably improved the precision of task performance, particularly at lower gain factors. Based on these findings, we propose design guidelines and potential applications of VR hand-retargeted interactions and multisensory feedback for optimal user experience and performance.")],-1),Ti=e("h2",{id:"_2024-04-04",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-04"},[e("span",null,"2024-04-04")])],-1),Ri=e("h4",{id:"approximate-bayesian-computation-as-an-informed-fuzzing-inference-system",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#approximate-bayesian-computation-as-an-informed-fuzzing-inference-system"},[e("span",null,"Approximate Bayesian Computation As An Informed Fuzzing-Inference System")])],-1),Mi=e("p",null,[e("strong",null,"Authors"),t(": Chris Vaisnor")],-1),Ci=e("strong",null,"Link",-1),Si={href:"http://arxiv.org/abs/2404.04303v2",target:"_blank",rel:"noopener noreferrer"},Ii=e("p",null,[e("strong",null,"Abstract"),t(": The power of fuzz testing lies in its random, often brute-force, generation and execution of inputs to trigger unexpected behaviors and vulnerabilities in software applications. However, given the reality of infinite possible input sequences, pursuing all test combinations would not only be computationally expensive, but practically impossible. Approximate Bayesian Computation (ABC), a form of Bayesian simulation, represents a novel, probabilistic approach to addressing this problem. The parameter space for working with these types of problems is effectively infinite, and the application of these techniques is untested in relevant literature. We use a relaxed, manual implementation of two ABC methods, a Sequential Monte Carlo (SMC) simulation, and a Markov Chain Monte Carlo (MCMC) simulation. We found promising results with the SMC posterior and mixed results with MCMC posterior distributions on our white-box fuzz-test function.")],-1),Di=e("h4",{id:"cbr-rag-case-based-reasoning-for-retrieval-augmented-generation-in-llms-for-legal-question-answering",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#cbr-rag-case-based-reasoning-for-retrieval-augmented-generation-in-llms-for-legal-question-answering"},[e("span",null,"CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering")])],-1),zi=e("p",null,[e("strong",null,"Authors"),t(": Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-Orji, Ruvan Weerasinghe, Anne Liret, Bruno Fleisch")],-1),Ei=e("strong",null,"Link",-1),qi={href:"http://arxiv.org/abs/2404.04302v1",target:"_blank",rel:"noopener noreferrer"},Wi=e("p",null,[e("strong",null,"Abstract"),t(": Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning (CBR) presents key opportunities to structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG, where CBR cycle's initial retrieval stage, its indexing vocabulary, and similarity knowledge containers are used to enhance LLM queries with contextually relevant cases. This integration augments the original LLM query, providing a richer prompt. We present an evaluation of CBR-RAG, and examine different representations (i.e. general and domain-specific embeddings) and methods of comparison (i.e. inter, intra and hybrid similarity) on the task of legal question-answering. Our results indicate that the context provided by CBR's case reuse enforces similarity between relevant components of the questions and the evidence base leading to significant improvements in the quality of generated answers.")],-1),Hi=e("h4",{id:"i-did-not-notice-a-comparison-of-immersive-analytics-with-augmented-and-virtual-reality",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#i-did-not-notice-a-comparison-of-immersive-analytics-with-augmented-and-virtual-reality"},[e("span",null,"I Did Not Notice: A Comparison of Immersive Analytics with Augmented and Virtual Reality")])],-1),Pi=e("p",null,[e("strong",null,"Authors"),t(": Xiaoyan Zhou, Anil Ufuk Batmaz, Adam S. Williams, Dylan Schreiber, Francisco Ortega")],-1),Gi=e("strong",null,"Link",-1),Fi={href:"http://arxiv.org/abs/2404.03814v1",target:"_blank",rel:"noopener noreferrer"},Oi=e("p",null,[e("strong",null,"Abstract"),t(": Immersive environments enable users to engage in embodied interaction, enhancing the sensemaking processes involved in completing tasks such as immersive analytics. Previous comparative studies on immersive analytics using augmented and virtual realities have revealed that users employ different strategies for data interpretation and text-based analytics depending on the environment. Our study seeks to investigate how augmented and virtual reality influences sensemaking processes in quantitative immersive analytics. Our results, derived from a diverse group of participants, indicate that users demonstrate comparable performance in both environments. However, it was observed that users exhibit a higher tolerance for cognitive load in VR and travel further in AR. Based on our findings, we recommend providing users with the option to switch between AR and VR, thereby enabling them to select an environment that aligns with their preferences and task requirements.")],-1),Vi=e("h4",{id:"exploration-is-harder-than-prediction-cryptographically-separating-reinforcement-learning-from-supervised-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploration-is-harder-than-prediction-cryptographically-separating-reinforcement-learning-from-supervised-learning"},[e("span",null,"Exploration is Harder than Prediction: Cryptographically Separating Reinforcement Learning from Supervised Learning")])],-1),Ni=e("p",null,[e("strong",null,"Authors"),t(": Noah Golowich, Ankur Moitra, Dhruv Rohatgi")],-1),Bi=e("strong",null,"Link",-1),ji={href:"http://arxiv.org/abs/2404.03774v1",target:"_blank",rel:"noopener noreferrer"},Ji=e("p",null,[e("strong",null,"Abstract"),t(": Supervised learning is often computationally easy in practice. But to what extent does this mean that other modes of learning, such as reinforcement learning (RL), ought to be computationally easy by extension? In this work we show the first cryptographic separation between RL and supervised learning, by exhibiting a class of block MDPs and associated decoding functions where reward-free exploration is provably computationally harder than the associated regression problem. We also show that there is no computationally efficient algorithm for reward-directed RL in block MDPs, even when given access to an oracle for this regression problem. It is known that being able to perform regression in block MDPs is necessary for finding a good policy; our results suggest that it is not sufficient. Our separation lower bound uses a new robustness property of the Learning Parities with Noise (LPN) hardness assumption, which is crucial in handling the dependent nature of RL data. We argue that separations and oracle lower bounds, such as ours, are a more meaningful way to prove hardness of learning because the constructions better reflect the practical reality that supervised learning by itself is often not the computational bottleneck.")],-1),Ki=e("h4",{id:"is-clip-the-main-roadblock-for-fine-grained-open-world-perception",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#is-clip-the-main-roadblock-for-fine-grained-open-world-perception"},[e("span",null,"Is CLIP the main roadblock for fine-grained open-world perception?")])],-1),Ui=e("p",null,[e("strong",null,"Authors"),t(": Lorenzo Bianchi, Fabio Carrara, Nicola Messina, Fabrizio Falchi")],-1),Yi=e("strong",null,"Link",-1),Xi={href:"http://arxiv.org/abs/2404.03539v1",target:"_blank",rel:"noopener noreferrer"},Zi=e("p",null,[e("strong",null,"Abstract"),t(": Modern applications increasingly demand flexible computer vision models that adapt to novel concepts not encountered during training. This necessity is pivotal in emerging domains like extended reality, robotics, and autonomous driving, which require the ability to respond to open-world stimuli. A key ingredient is the ability to identify objects based on free-form textual queries defined at inference time - a task known as open-vocabulary object detection. Multimodal backbones like CLIP are the main enabling technology for current open-world perception solutions. Despite performing well on generic queries, recent studies highlighted limitations on the fine-grained recognition capabilities in open-vocabulary settings - i.e., for distinguishing subtle object features like color, shape, and material. In this paper, we perform a detailed examination of these open-vocabulary object recognition limitations to find the root cause. We evaluate the performance of CLIP, the most commonly used vision-language backbone, against a fine-grained object-matching benchmark, revealing interesting analogies between the limitations of open-vocabulary object detectors and their backbones. Experiments suggest that the lack of fine-grained understanding is caused by the poor separability of object characteristics in the CLIP latent space. Therefore, we try to understand whether fine-grained knowledge is present in CLIP embeddings but not exploited at inference time due, for example, to the unsuitability of the cosine similarity matching function, which may discard important object characteristics. Our preliminary experiments show that simple CLIP latent-space re-projections help separate fine-grained concepts, paving the way towards the development of backbones inherently able to process fine-grained details. The code for reproducing these experiments is available at https://github.com/lorebianchi98/FG-CLIP.")],-1),Qi=e("h4",{id:"integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work"},[e("span",null,"Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work")])],-1),$i=e("p",null,[e("strong",null,"Authors"),t(": Somin Park, Carol C. Menassa, Vineet R. Kamat")],-1),eo=e("strong",null,"Link",-1),to={href:"http://arxiv.org/abs/2404.03498v1",target:"_blank",rel:"noopener noreferrer"},no=e("p",null,[e("strong",null,"Abstract"),t(": In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement. This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants. This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots. By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting. Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs. The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry.")],-1),ao=e("h4",{id:"influence-of-gameplay-duration-hand-tracking-and-controller-based-control-methods-on-ux-in-vr",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#influence-of-gameplay-duration-hand-tracking-and-controller-based-control-methods-on-ux-in-vr"},[e("span",null,"Influence of Gameplay Duration, Hand Tracking, and Controller Based Control Methods on UX in VR")])],-1),io=e("p",null,[e("strong",null,"Authors"),t(": Tanja Koji, Maurizio Vergari, Simon Knuth, Maximilian Warsinke, Sebastian Mller, Jan-Niklas Voigt-Antons")],-1),oo=e("strong",null,"Link",-1),ro={href:"http://arxiv.org/abs/2404.03337v1",target:"_blank",rel:"noopener noreferrer"},so=e("p",null,[e("strong",null,"Abstract"),t(": Inside-out tracking is growing popular in consumer VR, enhancing accessibility. It uses HMD camera data and neural networks for effective hand tracking. However, limited user experience studies have compared this method to traditional controllers, with no consensus on the optimal control technique. This paper investigates the impact of control methods and gaming duration on VR user experience, hypothesizing hand tracking might be preferred for short sessions and by users new to VR due to its simplicity. Through a lab study with twenty participants, evaluating presence, emotional response, UX quality, and flow, findings revealed control type and session length affect user experience without significant interaction. Controllers were generally superior, attributed to their reliability, and longer sessions increased presence and realism. The study found that individuals with more VR experience were more inclined to recommend hand tracking to others, which contradicted predictions.")],-1),lo=e("h4",{id:"fusion-of-mixture-of-experts-and-generative-artificial-intelligence-in-mobile-edge-metaverse",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#fusion-of-mixture-of-experts-and-generative-artificial-intelligence-in-mobile-edge-metaverse"},[e("span",null,"Fusion of Mixture of Experts and Generative Artificial Intelligence in Mobile Edge Metaverse")])],-1),co=e("p",null,[e("strong",null,"Authors"),t(": Guangyuan Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Shiwen Mao, Dong In Kim")],-1),ho=e("strong",null,"Link",-1),uo={href:"http://arxiv.org/abs/2404.03321v1",target:"_blank",rel:"noopener noreferrer"},go=e("p",null,[e("strong",null,"Abstract"),t(": In the digital transformation era, Metaverse offers a fusion of virtual reality (VR), augmented reality (AR), and web technologies to create immersive digital experiences. However, the evolution of the Metaverse is slowed down by the challenges of content creation, scalability, and dynamic user interaction. Our study investigates an integration of Mixture of Experts (MoE) models with Generative Artificial Intelligence (GAI) for mobile edge computing to revolutionize content creation and interaction in the Metaverse. Specifically, we harness an MoE model's ability to efficiently manage complex data and complex tasks by dynamically selecting the most relevant experts running various sub-models to enhance the capabilities of GAI. We then present a novel framework that improves video content generation quality and consistency, and demonstrate its application through case studies. Our findings underscore the efficacy of MoE and GAI integration to redefine virtual experiences by offering a scalable, efficient pathway to harvest the Metaverse's full potential.")],-1),po=e("h4",{id:"exploring-emotions-in-multi-componential-space-using-interactive-vr-games",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-emotions-in-multi-componential-space-using-interactive-vr-games"},[e("span",null,"Exploring Emotions in Multi-componential Space using Interactive VR Games")])],-1),mo=e("p",null,[e("strong",null,"Authors"),t(": Rukshani Somarathna, Gelareh Mohammadi")],-1),fo=e("strong",null,"Link",-1),vo={href:"http://arxiv.org/abs/2404.03239v1",target:"_blank",rel:"noopener noreferrer"},bo=e("p",null,[e("strong",null,"Abstract"),t(": Emotion understanding is a complex process that involves multiple components. The ability to recognise emotions not only leads to new context awareness methods but also enhances system interaction's effectiveness by perceiving and expressing emotions. Despite the attention to discrete and dimensional models, neuroscientific evidence supports those emotions as being complex and multi-faceted. One framework that resonated well with such findings is the Component Process Model (CPM), a theory that considers the complexity of emotions with five interconnected components: appraisal, expression, motivation, physiology and feeling. However, the relationship between CPM and discrete emotions has not yet been fully explored. Therefore, to better understand emotions underlying processes, we operationalised a data-driven approach using interactive Virtual Reality (VR) games and collected multimodal measures (self-reports, physiological and facial signals) from 39 participants. We used Machine Learning (ML) methods to identify the unique contributions of each component to emotion differentiation. Our results showed the role of different components in emotion differentiation, with the model including all components demonstrating the most significant contribution. Moreover, we found that at least five dimensions are needed to represent the variation of emotions in our dataset. These findings also have implications for using VR environments in emotion research and highlight the role of physiological signals in emotion recognition within such environments.")],-1),yo=e("h2",{id:"_2024-04-03",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-03"},[e("span",null,"2024-04-03")])],-1),wo=e("h4",{id:"self-supervised-6-dof-robot-grasping-by-demonstration-via-augmented-reality-teleoperation-system",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#self-supervised-6-dof-robot-grasping-by-demonstration-via-augmented-reality-teleoperation-system"},[e("span",null,"Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System")])],-1),_o=e("p",null,[e("strong",null,"Authors"),t(": Xiwen Dengxiong, Xueting Wang, Shi Bai, Yunbo Zhang")],-1),xo=e("strong",null,"Link",-1),ko={href:"http://arxiv.org/abs/2404.03067v1",target:"_blank",rel:"noopener noreferrer"},Ao=e("p",null,[e("strong",null,"Abstract"),t(": Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area. To this end, we propose a self-supervised 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations. Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration. For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations.")],-1),Lo=e("h4",{id:"ai-augmented-automation-for-real-driving-prediction-an-industrial-use-case",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ai-augmented-automation-for-real-driving-prediction-an-industrial-use-case"},[e("span",null,"AI-augmented Automation for Real Driving Prediction: an Industrial Use Case")])],-1),To=e("p",null,[e("strong",null,"Authors"),t(": Romina Eramo, Hamzeh Eyal Salman, Matteo Spezialetti, Darko Stern, Pierre Quinton, Antonio Cicchetti")],-1),Ro=e("strong",null,"Link",-1),Mo={href:"http://arxiv.org/abs/2404.02841v1",target:"_blank",rel:"noopener noreferrer"},Co=e("p",null,[e("strong",null,"Abstract"),t(": The risen complexity of automotive systems requires new development strategies and methods to master the upcoming challenges. Traditional methods need thus to be changed by an increased level of automation, and a faster continuous improvement cycle. In this context, current vehicle performance tests represent a very time-consuming and expensive task due to the need to perform the tests in real driving conditions. As a consequence, agile/iterative processes like DevOps are largely hindered by the necessity of triggering frequent tests. This paper reports on a practical experience of developing an AI-augmented solution based on Machine Learning and Model-based Engineering to support continuous vehicle development and testing. In particular, historical data collected in real driving conditions is leveraged to synthesize a high-fidelity driving simulator and hence enable performance tests in virtual environments. Based on this practical experience, this paper also proposes a conceptual framework to support predictions based on real driving behavior.")],-1),So=e("h4",{id:"retrieving-examples-from-memory-for-retrieval-augmented-neural-machine-translation-a-systematic-comparison",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#retrieving-examples-from-memory-for-retrieval-augmented-neural-machine-translation-a-systematic-comparison"},[e("span",null,"Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison")])],-1),Io=e("p",null,[e("strong",null,"Authors"),t(": Maxime Bouthors, Josep Crego, Francois Yvon")],-1),Do=e("strong",null,"Link",-1),zo={href:"http://arxiv.org/abs/2404.02835v1",target:"_blank",rel:"noopener noreferrer"},Eo=e("p",null,[e("strong",null,"Abstract"),t(": Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process. While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored. In this paper, we study the effect of varying retrieval methods for several translation architectures, to better understand the interplay between these two processes. We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning. Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures. We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board.")],-1),qo=e("h4",{id:"are-adversarial-phishing-webpages-a-threat-in-reality-understanding-the-users-perception-of-adversarial-webpages",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#are-adversarial-phishing-webpages-a-threat-in-reality-understanding-the-users-perception-of-adversarial-webpages"},[e("span",null,`"Are Adversarial Phishing Webpages a Threat in Reality?" Understanding the Users' Perception of Adversarial Webpages`)])],-1),Wo=e("p",null,[e("strong",null,"Authors"),t(": Ying Yuan, Qingying Hao, Giovanni Apruzzese, Mauro Conti, Gang Wang")],-1),Ho=e("strong",null,"Link",-1),Po={href:"http://arxiv.org/abs/2404.02832v1",target:"_blank",rel:"noopener noreferrer"},Go=e("p",null,[e("strong",null,"Abstract"),t(": Machine learning based phishing website detectors (ML-PWD) are a critical part of today's anti-phishing solutions in operation. Unfortunately, ML-PWD are prone to adversarial evasions, evidenced by both academic studies and analyses of real-world adversarial phishing webpages. However, existing works mostly focused on assessing adversarial phishing webpages against ML-PWD, while neglecting a crucial aspect: investigating whether they can deceive the actual target of phishing -- the end users. In this paper, we fill this gap by conducting two user studies (n=470) to examine how human users perceive adversarial phishing webpages, spanning both synthetically crafted ones (which we create by evading a state-of-the-art ML-PWD) as well as real adversarial webpages (taken from the wild Web) that bypassed a production-grade ML-PWD. Our findings confirm that adversarial phishing is a threat to both users and ML-PWD, since most adversarial phishing webpages have comparable effectiveness on users w.r.t. unperturbed ones. However, not all adversarial perturbations are equally effective. For example, those with added typos are significantly more noticeable to users, who tend to overlook perturbations of higher visual magnitude (such as replacing the background). We also show that users' self-reported frequency of visiting a brand's website has a statistically negative correlation with their phishing detection accuracy, which is likely caused by overconfidence. We release our resources.")],-1),Fo=e("h4",{id:"improving-topic-relevance-model-by-mix-structured-summarization-and-llm-based-data-augmentation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#improving-topic-relevance-model-by-mix-structured-summarization-and-llm-based-data-augmentation"},[e("span",null,"Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation")])],-1),Oo=e("p",null,[e("strong",null,"Authors"),t(": Yizhu Liu, Ran Tao, Shengyu Guo, Yifan Yang")],-1),Vo=e("strong",null,"Link",-1),No={href:"http://arxiv.org/abs/2404.02616v1",target:"_blank",rel:"noopener noreferrer"},Bo=e("p",null,[e("strong",null,"Abstract"),t(": Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user's requirement. In most social search scenarios such as Dianping, modeling search relevance always faces two challenges. One is that many documents in social search are very long and have much redundant information. The other is that the training data for search relevance model is difficult to get, especially for multi-classification relevance model. To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document. Then, we utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query from queries and documents in existing training data, which can construct new query-document pairs as training data. Extensive offline experiments and online A/B tests show that the proposed approaches effectively improve the performance of relevance modeling.")],-1),jo=e("h4",{id:"unbiased-learning-to-rank-meets-reality-lessons-from-baidu-s-large-scale-search-dataset",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unbiased-learning-to-rank-meets-reality-lessons-from-baidu-s-large-scale-search-dataset"},[e("span",null,"Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset")])],-1),Jo=e("p",null,[e("strong",null,"Authors"),t(": Philipp Hager, Romain Deffayet, Jean-Michel Renders, Onno Zoeter, Maarten de Rijke")],-1),Ko=e("strong",null,"Link",-1),Uo={href:"http://arxiv.org/abs/2404.02543v1",target:"_blank",rel:"noopener noreferrer"},Yo=e("p",null,[e("strong",null,"Abstract"),t(": Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data. While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines. The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques. We revisit and extend the available experiments. We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features. Our experiments reveal that ULTR robustly improves click prediction. However, these gains in click prediction do not translate to enhanced ranking performance on expert relevance annotations, implying that conclusions strongly depend on how success is measured in this benchmark.")],-1),Xo=e("h4",{id:"enhancing-sum-rate-performance-in-constrained-multicell-networks-a-low-information-exchange-approach",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-sum-rate-performance-in-constrained-multicell-networks-a-low-information-exchange-approach"},[e("span",null,"Enhancing Sum-Rate Performance in Constrained Multicell Networks: A Low-Information Exchange Approach")])],-1),Zo=e("p",null,[e("strong",null,"Authors"),t(": Youjin Kim, Jonggyu Jang, Hyun Jong Yang")],-1),Qo=e("strong",null,"Link",-1),$o={href:"http://arxiv.org/abs/2404.02477v1",target:"_blank",rel:"noopener noreferrer"},er=e("p",null,[e("strong",null,"Abstract"),t(": Despite the extensive research on massive MIMO systems for 5G telecommunications and beyond, the reality is that many deployed base stations are equipped with a limited number of antennas rather than supporting massive MIMO configurations. Furthermore, while the cell-less network concept, which eliminates cell boundaries, is under investigation, practical deployments often grapple with significantly limited backhaul connection capacities between base stations. This letter explores techniques to maximize the sum-rate performance within the constraints of these more realistically equipped multicell networks. We propose an innovative approach that dramatically reduces the need for information exchange between base stations to a mere few bits, in stark contrast to conventional methods that require the exchange of hundreds of bits. Our proposed method not only addresses the limitations imposed by current network infrastructure but also showcases significantly improved performance under these constrained conditions.")],-1),tr=e("h2",{id:"_2024-04-02",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-02"},[e("span",null,"2024-04-02")])],-1),nr=e("h4",{id:"semantic-augmentation-in-images-using-language",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#semantic-augmentation-in-images-using-language"},[e("span",null,"Semantic Augmentation in Images using Language")])],-1),ar=e("p",null,[e("strong",null,"Authors"),t(": Sahiti Yerramilli, Jayant Sravan Tamarapalli, Tanmay Girish Kulkarni, Jonathan Francis, Eric Nyberg")],-1),ir=e("strong",null,"Link",-1),or={href:"http://arxiv.org/abs/2404.02353v1",target:"_blank",rel:"noopener noreferrer"},rr=e("p",null,[e("strong",null,"Abstract"),t(": Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.")],-1),sr=e("h4",{id:"a-change-of-scenery-transformative-insights-from-retrospective-vr-embodied-perspective-taking-of-conflict-with-a-close-other",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-change-of-scenery-transformative-insights-from-retrospective-vr-embodied-perspective-taking-of-conflict-with-a-close-other"},[e("span",null,"A Change of Scenery: Transformative Insights from Retrospective VR Embodied Perspective-Taking of Conflict With a Close Other")])],-1),lr=e("p",null,[e("strong",null,"Authors"),t(": Seraphina Yong, Leo Cui, Evan Suma Rosenberg, Svetlana Yarosh")],-1),cr=e("strong",null,"Link",-1),dr={href:"http://arxiv.org/abs/2404.02277v1",target:"_blank",rel:"noopener noreferrer"},hr=e("p",null,[e("strong",null,"Abstract"),t(": Close relationships are irreplaceable social resources, yet prone to high-risk conflict. Building on findings from the fields of HCI, virtual reality, and behavioral therapy, we evaluate the unexplored potential of retrospective VR-embodied perspective-taking to fundamentally influence conflict resolution in close others. We develop a biographically-accurate Retrospective Embodied Perspective-Taking system (REPT) and conduct a mixed-methods evaluation of its influence on close others' reflection and communication, compared to video-based reflection methods currently used in therapy (treatment as usual, or TAU). Our key findings provide evidence that REPT was able to significantly improve communication skills and positive sentiment of both partners during conflict, over TAU. The qualitative data also indicated that REPT surpassed basic perspective-taking by exclusively stimulating users to embody and reflect on both their own and their partner's experiences at the same level. In light of these findings, we provide implications and an agenda for social embodiment in HCI design: conceptualizing the use of `embodied social cognition,' and envisioning socially-embodied experiences as an interactive context.")],-1),ur=e("h4",{id:"rat-retrieval-augmented-transformer-for-click-through-rate-prediction",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#rat-retrieval-augmented-transformer-for-click-through-rate-prediction"},[e("span",null,"RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction")])],-1),gr=e("p",null,[e("strong",null,"Authors"),t(": Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang, Shu-Tao Xia")],-1),pr=e("strong",null,"Link",-1),mr={href:"http://arxiv.org/abs/2404.02249v2",target:"_blank",rel:"noopener noreferrer"},fr=e("p",null,[e("strong",null,"Abstract"),t(": Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and suggest its advantage in long-tail scenarios. The code has been open-sourced at \\url{https://github.com/YushenLi807/WWW24-RAT}.")],-1),vr=e("h4",{id:"robustly-estimating-heterogeneity-in-factorial-data-using-rashomon-partitions",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robustly-estimating-heterogeneity-in-factorial-data-using-rashomon-partitions"},[e("span",null,"Robustly estimating heterogeneity in factorial data using Rashomon Partitions")])],-1),br=e("p",null,[e("strong",null,"Authors"),t(": Aparajithan Venkateswaran, Anirudh Sankar, Arun G. Chandrasekhar, Tyler H. McCormick")],-1),yr=e("strong",null,"Link",-1),wr={href:"http://arxiv.org/abs/2404.02141v1",target:"_blank",rel:"noopener noreferrer"},_r=e("p",null,[e("strong",null,"Abstract"),t(": Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into "),e("code",null,"pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single "),t("optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Sets (RPSs). Each item in the RPS partitions the space of covariates using a tree-like geometry. RPSs incorporate all partitions that have posterior values near the maximum a posteriori partition, even if they offer substantively different explanations, and do so using a prior that makes no assumptions about associations between covariates. This prior is the $\\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate the posterior of any measurable function of the feature effects vector on outcomes, conditional on being in the RPS. We also characterize approximation error relative to the entire posterior and provide bounds on the size of the RPS. Simulations demonstrate this framework allows for robust conclusions relative to conventional regularization techniques. We apply our method to three empirical settings: price effects on charitable giving, chromosomal structure (telomere length), and the introduction of microfinance.")],-1),xr=e("h4",{id:"causality-based-transfer-of-driving-scenarios-to-unseen-intersections",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#causality-based-transfer-of-driving-scenarios-to-unseen-intersections"},[e("span",null,"Causality-based Transfer of Driving Scenarios to Unseen Intersections")])],-1),kr=e("p",null,[e("strong",null,"Authors"),t(": Christoph Glasmacher, Michael Schuldes, Sleiman El Masri, Lutz Eckstein")],-1),Ar=e("strong",null,"Link",-1),Lr={href:"http://arxiv.org/abs/2404.02046v1",target:"_blank",rel:"noopener noreferrer"},Tr=e("p",null,[e("strong",null,"Abstract"),t(": Scenario-based testing of automated driving functions has become a promising method to reduce time and cost compared to real-world testing. In scenario-based testing automated functions are evaluated in a set of pre-defined scenarios. These scenarios provide information about vehicle behaviors, environmental conditions, or road characteristics using parameters. To create realistic scenarios, parameters and parameter dependencies have to be fitted utilizing real-world data. However, due to the large variety of intersections and movement constellations found in reality, data may not be available for certain scenarios. This paper proposes a methodology to systematically analyze relations between parameters of scenarios. Bayesian networks are utilized to analyze causal dependencies in order to decrease the amount of required data and to transfer causal patterns creating unseen scenarios. Thereby, infrastructural influences on movement patterns are investigated to generate realistic scenarios on unobserved intersections. For evaluation, scenarios and underlying parameters are extracted from the inD dataset. Movement patterns are estimated, transferred and checked against recorded data from those initially unseen intersections.")],-1),Rr=e("h4",{id:"improving-retrieval-augmented-open-domain-question-answering-with-vectorized-contexts",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#improving-retrieval-augmented-open-domain-question-answering-with-vectorized-contexts"},[e("span",null,"Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts")])],-1),Mr=e("p",null,[e("strong",null,"Authors"),t(": Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu")],-1),Cr=e("strong",null,"Link",-1),Sr={href:"http://arxiv.org/abs/2404.02022v1",target:"_blank",rel:"noopener noreferrer"},Ir=e("p",null,[e("strong",null,"Abstract"),t(": In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings.")],-1),Dr=e("h4",{id:"a-rationale-centric-counterfactual-data-augmentation-method-for-cross-document-event-coreference-resolution",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-rationale-centric-counterfactual-data-augmentation-method-for-cross-document-event-coreference-resolution"},[e("span",null,"A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution")])],-1),zr=e("p",null,[e("strong",null,"Authors"),t(": Bowen Ding, Qingkai Min, Shengkun Ma, Yingjie Li, Linyi Yang, Yue Zhang")],-1),Er=e("strong",null,"Link",-1),qr={href:"http://arxiv.org/abs/2404.01921v1",target:"_blank",rel:"noopener noreferrer"},Wr=e("p",null,[e("strong",null,"Abstract"),t(": Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the existing system exhibits an excessive reliance on the `triggers lexical matching' spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three popular cross-document ECR benchmarks and demonstrates robustness in out-of-domain scenarios.")],-1),Hr=e("h4",{id:"learning-based-model-augmentation-with-lfrs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learning-based-model-augmentation-with-lfrs"},[e("span",null,"Learning-based model augmentation with LFRs")])],-1),Pr=e("p",null,[e("strong",null,"Authors"),t(": Jan H. Hoekstra, Chris Verhoek, Roland Tth, Maarten Schoukens")],-1),Gr=e("strong",null,"Link",-1),Fr={href:"http://arxiv.org/abs/2404.01901v1",target:"_blank",rel:"noopener noreferrer"},Or=e("p",null,[e("strong",null,"Abstract"),t(": Artificial neural networks (ANN) have proven to be effective in dealing with the identification nonlinear models for highly complex systems. To still make use of the prior information available from baseline models derived from, e.g., first-principles (FP), methods have been developed that integrate the prior knowledge into the identification algorithm for the ANN in a variety of methods. These methods have shown better estimation speeds and/or accuracy on unseen data. Among these methods are model augmentation structures. A variety of these structures have been considered in literature, there is however no unifying theory to these. In this paper, we propose a flexible linear-fractional-representation (LFR) based model augmentation structure. This model structure is able to represent many common model augmentation structures, thus unifying them under the proposed model structure. Furthermore, we introduce an identification algorithm capable of estimating the proposed model augmentation structure. The performance and generalization capabilities of the identification algorithm and the augmentation structure is demonstrated on a hardening mass-spring-damper simulation example.")],-1),Vr=e("h4",{id:"_3d-scene-generation-from-scene-graphs-and-self-attention",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_3d-scene-generation-from-scene-graphs-and-self-attention"},[e("span",null,"3D scene generation from scene graphs and self-attention")])],-1),Nr=e("p",null,[e("strong",null,"Authors"),t(": Pietro Bonazzi")],-1),Br=e("strong",null,"Link",-1),jr={href:"http://arxiv.org/abs/2404.01887v2",target:"_blank",rel:"noopener noreferrer"},Jr=e("p",null,[e("strong",null,"Abstract"),t(": Synthesizing realistic and diverse indoor 3D scene layouts in a controllable fashion opens up applications in simulated navigation and virtual reality. As concise and robust representations of a scene, scene graphs have proven to be well-suited as the semantic control on the generated layout. We present a variant of the conditional variational autoencoder (cVAE) model to synthesize 3D scenes from scene graphs and floor plans. We exploit the properties of self-attention layers to capture high-level relationships between objects in a scene, and use these as the building blocks of our model. Our model, leverages graph transformers to estimate the size, dimension and orientation of the objects in a room while satisfying relationships in the given scene graph. Our experiments shows self-attention layers leads to sparser (7.9x compared to Graphto3D) and more diverse scenes (16%).")],-1),Kr=e("h4",{id:"identification-and-characterization-of-three-dimensional-crack-propagation-mechanism-in-the-aluminium-alloy-aa2024-t3-using-high-resolution-digital-image-correlation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#identification-and-characterization-of-three-dimensional-crack-propagation-mechanism-in-the-aluminium-alloy-aa2024-t3-using-high-resolution-digital-image-correlation"},[e("span",null,"Identification and characterization of three-dimensional crack propagation mechanism in the Aluminium alloy AA2024-T3 using high-resolution Digital Image Correlation")])],-1),Ur=e("p",null,[e("strong",null,"Authors"),t(": Vanessa Schne, Florian Paysan, Eric Breitbarth")],-1),Yr=e("strong",null,"Link",-1),Xr={href:"http://arxiv.org/abs/2404.01852v1",target:"_blank",rel:"noopener noreferrer"},Zr=e("p",null,[e("strong",null,"Abstract"),t(": Fatigue crack growth is usually a three-dimensional problem, but it is often simplified to two dimensions to reduce complexity. However, this study investigates the relationships between microscopic effects such as crack kinking, shear lips, and plasticity that are present in reality. Therefore, crack propagation tests were carried out on 2-mm-thick MT-160 specimens of AA2024-T3 sheet material in L-T and T-L orientation. Using high-resolution digital image correlation (DIC), the plastic zone was identified and measured on the samples surface. The fracture surfaces were then digitized and their 3D shape characterized. Finite element simulations confirm the presence of a local mixed-mode I/II/III state along the crack front for a slant or double shear-fracture type. A derived mapping function enables the determination of the fracture type from the surface plastic zone, along with the current crack tip loading during the test. Finally, a transition of the fracture type also leads to a short-term delay in the crack propagation rate. Based on this information crack propagation curves are computed with regards to the local 3D crack orientation.")],-1),Qr=e("h4",{id:"tensorized-neuroevolution-of-augmenting-topologies-for-gpu-acceleration",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tensorized-neuroevolution-of-augmenting-topologies-for-gpu-acceleration"},[e("span",null,"Tensorized NeuroEvolution of Augmenting Topologies for GPU Acceleration")])],-1),$r=e("p",null,[e("strong",null,"Authors"),t(": Lishuang Wang, Mengfei Zhao, Enyu Liu, Kebin Sun, Ran Cheng")],-1),es=e("strong",null,"Link",-1),ts={href:"http://arxiv.org/abs/2404.01817v3",target:"_blank",rel:"noopener noreferrer"},ns=e("p",null,[e("strong",null,"Abstract"),t(": The NeuroEvolution of Augmenting Topologies (NEAT) algorithm has received considerable recognition in the field of neuroevolution. Its effectiveness is derived from initiating with simple networks and incrementally evolving both their topologies and weights. Although its capability across various challenges is evident, the algorithm's computational efficiency remains an impediment, limiting its scalability potential. In response, this paper introduces a tensorization method for the NEAT algorithm, enabling the transformation of its diverse network topologies and associated operations into uniformly shaped tensors for computation. This advancement facilitates the execution of the NEAT algorithm in a parallelized manner across the entire population. Furthermore, we develop TensorNEAT, a library that implements the tensorized NEAT algorithm and its variants, such as CPPN and HyperNEAT. Building upon JAX, TensorNEAT promotes efficient parallel computations via automated function vectorization and hardware acceleration. Moreover, the TensorNEAT library supports various benchmark environments including Gym, Brax, and gymnax. Through evaluations across a spectrum of robotics control environments in Brax, TensorNEAT achieves up to 500x speedups compared to the existing implementations such as NEAT-Python. Source codes are available at: https://github.com/EMI-Group/tensorneat.")],-1),as=e("h4",{id:"generative-ai-for-immersive-communication-the-next-frontier-in-internet-of-senses-through-6g",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#generative-ai-for-immersive-communication-the-next-frontier-in-internet-of-senses-through-6g"},[e("span",null,"Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G")])],-1),is=e("p",null,[e("strong",null,"Authors"),t(": Nassim Sehad, Lina Bariah, Wassim Hamidouche, Hamed Hellaoui, Riku Jntti, Mrouane Debbah")],-1),os=e("strong",null,"Link",-1),rs={href:"http://arxiv.org/abs/2404.01713v1",target:"_blank",rel:"noopener noreferrer"},ss=e("p",null,[e("strong",null,"Abstract"),t(": Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that leverages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical applications of generative AI for immersive media while addressing the challenges and outlining future trajectories.")],-1),ls=e("h4",{id:"task-integration-distillation-for-object-detectors",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#task-integration-distillation-for-object-detectors"},[e("span",null,"Task Integration Distillation for Object Detectors")])],-1),cs=e("p",null,[e("strong",null,"Authors"),t(": Hai Su, ZhenWen Jian, Songsen Yu")],-1),ds=e("strong",null,"Link",-1),hs={href:"http://arxiv.org/abs/2404.01699v1",target:"_blank",rel:"noopener noreferrer"},us=e("p",null,[e("strong",null,"Abstract"),t(": Knowledge distillation is a widely adopted technique for model lightening. However, the performance of most knowledge distillation methods in the domain of object detection is not satisfactory. Typically, knowledge distillation approaches consider only the classification task among the two sub-tasks of an object detector, largely overlooking the regression task. This oversight leads to a partial understanding of the object detector's comprehensive task, resulting in skewed estimations and potentially adverse effects. Therefore, we propose a knowledge distillation method that addresses both the classification and regression tasks, incorporating a task significance strategy. By evaluating the importance of features based on the output of the detector's two sub-tasks, our approach ensures a balanced consideration of both classification and regression tasks in object detection. Drawing inspiration from real-world teaching processes and the definition of learning condition, we introduce a method that focuses on both key and weak areas. By assessing the value of features for knowledge distillation based on their importance differences, we accurately capture the current model's learning situation. This method effectively prevents the issue of biased predictions about the model's learning reality caused by an incomplete utilization of the detector's outputs.")],-1),gs=e("h4",{id:"ft2ra-a-fine-tuning-inspired-approach-to-retrieval-augmented-code-completion",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ft2ra-a-fine-tuning-inspired-approach-to-retrieval-augmented-code-completion"},[e("span",null,"FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion")])],-1),ps=e("p",null,[e("strong",null,"Authors"),t(": Qi Guo, Xiaohong Li, Xiaofei Xie, Shangqing Liu, Ze Tang, Ruitao Feng, Junjie Wang, Jidong Ge, Lei Bu")],-1),ms=e("strong",null,"Link",-1),fs={href:"http://arxiv.org/abs/2404.01554v1",target:"_blank",rel:"noopener noreferrer"},vs=e("p",null,[e("strong",null,"Abstract"),t(": The rise of code pre-trained models has significantly enhanced various coding tasks, such as code completion, and tools like GitHub Copilot. However, the substantial size of these models, especially large models, poses a significant challenge when it comes to fine-tuning them for specific downstream tasks. As an alternative approach, retrieval-based methods have emerged as a promising solution, augmenting model predictions without the need for fine-tuning. Despite their potential, a significant challenge is that the designs of these methods often rely on heuristics, leaving critical questions about what information should be stored or retrieved and how to interpolate such information for augmenting predictions. To tackle this challenge, we first perform a theoretical analysis of the fine-tuning process, highlighting the importance of delta logits as a catalyst for improving model predictions. Building on this insight, we develop a novel retrieval-based method, FT2Ra, which aims to mimic genuine fine-tuning. While FT2Ra adopts a retrieval-based mechanism, it uniquely adopts a paradigm with a learning rate and multi-epoch retrievals, which is similar to fine-tuning.In token-level completion, which represents a relatively easier task, FT2Ra achieves a 4.29% improvement in accuracy compared to the best baseline method on UniXcoder. In the more challenging line-level completion task, we observe a substantial more than twice increase in Exact Match (EM) performance, indicating the significant advantages of our theoretical analysis. Notably, even when operating without actual fine-tuning, FT2Ra exhibits competitive performance compared to the models with real fine-tuning.")],-1),bs=e("h4",{id:"efficient-3d-implicit-head-avatar-with-mesh-anchored-hash-table-blendshapes",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#efficient-3d-implicit-head-avatar-with-mesh-anchored-hash-table-blendshapes"},[e("span",null,"Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes")])],-1),ys=e("p",null,[e("strong",null,"Authors"),t(": Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang")],-1),ws=e("strong",null,"Link",-1),_s={href:"http://arxiv.org/abs/2404.01543v1",target:"_blank",rel:"noopener noreferrer"},xs=e("p",null,[e("strong",null,"Abstract"),t(": 3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions.")],-1),ks=e("h2",{id:"_2024-04-01",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-01"},[e("span",null,"2024-04-01")])],-1),As=e("h4",{id:"aadam-at-semeval-2024-task-1-augmentation-and-adaptation-for-multilingual-semantic-textual-relatedness",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#aadam-at-semeval-2024-task-1-augmentation-and-adaptation-for-multilingual-semantic-textual-relatedness"},[e("span",null,"AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness")])],-1),Ls=e("p",null,[e("strong",null,"Authors"),t(": Miaoran Zhang, Mingyang Wang, Jesujoba O. Alabi, Dietrich Klakow")],-1),Ts=e("strong",null,"Link",-1),Rs={href:"http://arxiv.org/abs/2404.01490v1",target:"_blank",rel:"noopener noreferrer"},Ms=e("p",null,[e("strong",null,"Abstract"),t(": This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages. The shared task aims at measuring the semantic textual relatedness between pairs of sentences, with a focus on a range of under-represented languages. In this work, we propose using machine translation for data augmentation to address the low-resource challenge of limited training data. Moreover, we apply task-adaptive pre-training on unlabeled task data to bridge the gap between pre-training and task adaptation. For model training, we investigate both full fine-tuning and adapter-based tuning, and adopt the adapter framework for effective zero-shot cross-lingual transfer. We achieve competitive results in the shared task: our system performs the best among all ranked teams in both subtask A (supervised learning) and subtask C (cross-lingual transfer).")],-1),Cs=e("h4",{id:"are-large-language-models-superhuman-chemists",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#are-large-language-models-superhuman-chemists"},[e("span",null,"Are large language models superhuman chemists?")])],-1),Ss=e("p",null,[e("strong",null,"Authors"),t(": Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir Mohammad Elahi, Maximilian Greiner, Caroline T. Holick, Tanya Gupta, Mehrdad Asgari, Christina Glaubitz, Lea C. Klepsch, Yannik Kster, Jakob Meyer, Santiago Miret, Tim Hoffmann, Fabian Alexander Kreth, Michael Ringleb, Nicole Roesner, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, Kevin Maik Jablonka")],-1),Is=e("strong",null,"Link",-1),Ds={href:"http://arxiv.org/abs/2404.01475v1",target:"_blank",rel:"noopener noreferrer"},zs=e("p",null,[e("strong",null,"Abstract"),t(`: Large language models (LLMs) have gained widespread interest due to their ability to process human language and perform tasks on which they have not been explicitly trained. This is relevant for the chemical sciences, which face the problem of small and diverse datasets that are frequently in the form of text. LLMs have shown promise in addressing these issues and are increasingly being harnessed to predict chemical properties, optimize reactions, and even design and conduct experiments autonomously. However, we still have only a very limited systematic understanding of the chemical reasoning capabilities of LLMs, which would be required to improve models and mitigate potential harms. Here, we introduce "ChemBench," an automated framework designed to rigorously evaluate the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of human chemists. We curated more than 7,000 question-answer pairs for a wide array of subfields of the chemical sciences, evaluated leading open and closed-source LLMs, and found that the best models outperformed the best human chemists in our study on average. The models, however, struggle with some chemical reasoning tasks that are easy for human experts and provide overconfident, misleading predictions, such as about chemicals' safety profiles. These findings underscore the dual reality that, although LLMs demonstrate remarkable proficiency in chemical tasks, further research is critical to enhancing their safety and utility in chemical sciences. Our findings also indicate a need for adaptations to chemistry curricula and highlight the importance of continuing to develop evaluation frameworks to improve safe and useful LLMs.`)],-1),Es=e("h4",{id:"nvins-robust-visual-inertial-navigation-fused-with-nerf-augmented-camera-pose-regressor-and-uncertainty-quantification",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#nvins-robust-visual-inertial-navigation-fused-with-nerf-augmented-camera-pose-regressor-and-uncertainty-quantification"},[e("span",null,"NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented Camera Pose Regressor and Uncertainty Quantification")])],-1),qs=e("p",null,[e("strong",null,"Authors"),t(": Juyeop Han, Lukas Lao Beyer, Guilherme V. Cavalheiro, Sertac Karaman")],-1),Ws=e("strong",null,"Link",-1),Hs={href:"http://arxiv.org/abs/2404.01400v1",target:"_blank",rel:"noopener noreferrer"},Ps=e("p",null,[e("strong",null,"Abstract"),t(": In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful tool for 3D reconstruction and novel view synthesis. However, the computational cost of NeRF rendering and degradation in quality due to the presence of artifacts pose significant challenges for its application in real-time and robust robotic tasks, especially on embedded systems. This paper introduces a novel framework that integrates NeRF-derived localization information with Visual-Inertial Odometry(VIO) to provide a robust solution for robotic navigation in a real-time. By training an absolute pose regression network with augmented image data rendered from a NeRF and quantifying its uncertainty, our approach effectively counters positional drift and enhances system reliability. We also establish a mathematically sound foundation for combining visual inertial navigation with camera localization neural networks, considering uncertainty under a Bayesian framework. Experimental validation in the photorealistic simulation environment demonstrates significant improvements in accuracy compared to a conventional VIO approach.")],-1),Gs=e("h4",{id:"scalable-scene-modeling-from-perspective-imaging-physics-based-appearance-and-geometry-inference",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#scalable-scene-modeling-from-perspective-imaging-physics-based-appearance-and-geometry-inference"},[e("span",null,"Scalable Scene Modeling from Perspective Imaging: Physics-based Appearance and Geometry Inference")])],-1),Fs=e("p",null,[e("strong",null,"Authors"),t(": Shuang Song")],-1),Os=e("strong",null,"Link",-1),Vs={href:"http://arxiv.org/abs/2404.01248v1",target:"_blank",rel:"noopener noreferrer"},Ns=e("p",null,[e("strong",null,"Abstract"),t(": 3D scene modeling techniques serve as the bedrocks in the geospatial engineering and computer science, which drives many applications ranging from automated driving, terrain mapping, navigation, virtual, augmented, mixed, and extended reality (for gaming and movie industry etc.). This dissertation presents a fraction of contributions that advances 3D scene modeling to its state of the art, in the aspects of both appearance and geometry modeling. In contrast to the prevailing deep learning methods, as a core contribution, this thesis aims to develop algorithms that follow first principles, where sophisticated physic-based models are introduced alongside with simpler learning and inference tasks. The outcomes of these algorithms yield processes that can consume much larger volume of data for highly accurate reconstructing 3D scenes at a scale without losing methodological generality, which are not possible by contemporary complex-model based deep learning methods. Specifically, the dissertation introduces three novel methodologies that address the challenges of inferring appearance and geometry through physics-based modeling. Overall, the research encapsulated in this dissertation marks a series of methodological triumphs in the processing of complex datasets. By navigating the confluence of deep learning, computational geometry, and photogrammetry, this work lays down a robust framework for future exploration and practical application in the rapidly evolving field of 3D scene reconstruction. The outcomes of these studies are evidenced through rigorous experiments and comparisons with existing state-of-the-art methods, demonstrating the efficacy and scalability of the proposed approaches.")],-1),Bs=e("h4",{id:"imd4gc-incomplete-multimodal-data-integration-to-advance-precise-treatment-response-prediction-and-survival-analysis-for-gastric-cancer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#imd4gc-incomplete-multimodal-data-integration-to-advance-precise-treatment-response-prediction-and-survival-analysis-for-gastric-cancer"},[e("span",null,"iMD4GC: Incomplete Multimodal Data Integration to Advance Precise Treatment Response Prediction and Survival Analysis for Gastric Cancer")])],-1),js=e("p",null,[e("strong",null,"Authors"),t(": Fengtao Zhou, Yingxue Xu, Yanfen Cui, Shenyan Zhang, Yun Zhu, Weiyang He, Jiguang Wang, Xin Wang, Ronald Chan, Louis Ho Shing Lau, Chu Han, Dafu Zhang, Zhenhui Li, Hao Chen")],-1),Js=e("strong",null,"Link",-1),Ks={href:"http://arxiv.org/abs/2404.01192v1",target:"_blank",rel:"noopener noreferrer"},Us=e("p",null,[e("strong",null,"Abstract"),t(": Gastric cancer (GC) is a prevalent malignancy worldwide, ranking as the fifth most common cancer with over 1 million new cases and 700 thousand deaths in 2020. Locally advanced gastric cancer (LAGC) accounts for approximately two-thirds of GC diagnoses, and neoadjuvant chemotherapy (NACT) has emerged as the standard treatment for LAGC. However, the effectiveness of NACT varies significantly among patients, with a considerable subset displaying treatment resistance. Ineffective NACT not only leads to adverse effects but also misses the optimal therapeutic window, resulting in lower survival rate. However, existing multimodal learning methods assume the availability of all modalities for each patient, which does not align with the reality of clinical practice. The limited availability of modalities for each patient would cause information loss, adversely affecting predictive accuracy. In this study, we propose an incomplete multimodal data integration framework for GC (iMD4GC) to address the challenges posed by incomplete multimodal data, enabling precise response prediction and survival analysis. Specifically, iMD4GC incorporates unimodal attention layers for each modality to capture intra-modal information. Subsequently, the cross-modal interaction layers explore potential inter-modal interactions and capture complementary information across modalities, thereby enabling information compensation for missing modalities. To evaluate iMD4GC, we collected three multimodal datasets for GC study: GastricRes (698 cases) for response prediction, GastricSur (801 cases) for survival analysis, and TCGA-STAD (400 cases) for survival analysis. The scale of our datasets is significantly larger than previous studies. The iMD4GC achieved impressive performance with an 80.2% AUC on GastricRes, 71.4% C-index on GastricSur, and 66.1% C-index on TCGA-STAD, significantly surpassing other compared methods.")],-1),Ys=e("h4",{id:"detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms"},[e("span",null,"Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs")])],-1),Xs=e("p",null,[e("strong",null,"Authors"),t(": Jialou Wang, Manli Zhu, Yulei Li, Honglei Li, Longzhi Yang, Wai Lok Woo")],-1),Zs=e("strong",null,"Link",-1),Qs={href:"http://arxiv.org/abs/2404.01151v1",target:"_blank",rel:"noopener noreferrer"},$s=e("p",null,[e("strong",null,"Abstract"),t(`: Localization plays a crucial role in enhancing the practicality and precision of VQA systems. By enabling fine-grained identification and interaction with specific parts of an object, it significantly improves the system's ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality. However, traditional systems face challenges in accurately mapping objects within images to generate nuanced and spatially aware responses. In this work, we introduce "Detect2Interact", which addresses these challenges by introducing an advanced approach for fine-grained object visual key field detection. First, we use the segment anything model (SAM) to generate detailed spatial maps of objects in images. Next, we use Vision Studio to extract semantic object descriptions. Third, we employ GPT-4's common sense knowledge, bridging the gap between an object's semantics and its spatial map. As a result, Detect2Interact achieves consistent qualitative results on object key field detection across extensive test cases and outperforms the existing VQA system with object detection by providing a more reasonable and finer visual representation.`)],-1),el=e("h4",{id:"aigcoiqa2024-perceptual-quality-assessment-of-ai-generated-omnidirectional-images",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#aigcoiqa2024-perceptual-quality-assessment-of-ai-generated-omnidirectional-images"},[e("span",null,"AIGCOIQA2024: Perceptual Quality Assessment of AI Generated Omnidirectional Images")])],-1),tl=e("p",null,[e("strong",null,"Authors"),t(": Liu Yang, Huiyu Duan, Long Teng, Yucheng Zhu, Xiaohong Liu, Menghan Hu, Xiongkuo Min, Guangtao Zhai, Patrick Le Callet")],-1),nl=e("strong",null,"Link",-1),al={href:"http://arxiv.org/abs/2404.01024v1",target:"_blank",rel:"noopener noreferrer"},il=e("p",null,[e("strong",null,"Abstract"),t(": In recent years, the rapid advancement of Artificial Intelligence Generated Content (AIGC) has attracted widespread attention. Among the AIGC, AI generated omnidirectional images hold significant potential for Virtual Reality (VR) and Augmented Reality (AR) applications, hence omnidirectional AIGC techniques have also been widely studied. AI-generated omnidirectional images exhibit unique distortions compared to natural omnidirectional images, however, there is no dedicated Image Quality Assessment (IQA) criteria for assessing them. This study addresses this gap by establishing a large-scale AI generated omnidirectional image IQA database named AIGCOIQA2024 and constructing a comprehensive benchmark. We first generate 300 omnidirectional images based on 5 AIGC models utilizing 25 text prompts. A subjective IQA experiment is conducted subsequently to assess human visual preferences from three perspectives including quality, comfortability, and correspondence. Finally, we conduct a benchmark experiment to evaluate the performance of state-of-the-art IQA models on our database. The database will be released to facilitate future research.")],-1),ol=e("h4",{id:"badpart-unified-black-box-adversarial-patch-attacks-against-pixel-wise-regression-tasks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#badpart-unified-black-box-adversarial-patch-attacks-against-pixel-wise-regression-tasks"},[e("span",null,"BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks")])],-1),rl=e("p",null,[e("strong",null,"Authors"),t(": Zhiyuan Cheng, Zhaoyi Liu, Tengda Guo, Shiwei Feng, Dongfang Liu, Mingjie Tang, Xiangyu Zhang")],-1),sl=e("strong",null,"Link",-1),ll={href:"http://arxiv.org/abs/2404.00924v1",target:"_blank",rel:"noopener noreferrer"},cl=e("p",null,[e("strong",null,"Abstract"),t(": Pixel-wise regression tasks (e.g., monocular depth estimation (MDE) and optical flow estimation (OFE)) have been widely involved in our daily life in applications like autonomous driving, augmented reality and video composition. Although certain applications are security-critical or bear societal significance, the adversarial robustness of such models are not sufficiently studied, especially in the black-box scenario. In this work, we introduce the first unified black-box adversarial patch attack framework against pixel-wise regression tasks, aiming to identify the vulnerabilities of these models under query-based black-box attacks. We propose a novel square-based adversarial patch optimization framework and employ probabilistic square sampling and score-based gradient estimation techniques to generate the patch effectively and efficiently, overcoming the scalability problem of previous black-box patch attacks. Our attack prototype, named BadPart, is evaluated on both MDE and OFE tasks, utilizing a total of 7 models. BadPart surpasses 3 baseline methods in terms of both attack performance and efficiency. We also apply BadPart on the Google online service for portrait depth estimation, causing 43.5% relative distance error with 50K queries. State-of-the-art (SOTA) countermeasures cannot defend our attack effectively.")],-1),dl=e("h4",{id:"caap-class-dependent-automatic-data-augmentation-based-on-adaptive-policies-for-time-series",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#caap-class-dependent-automatic-data-augmentation-based-on-adaptive-policies-for-time-series"},[e("span",null,"CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series")])],-1),hl=e("p",null,[e("strong",null,"Authors"),t(": Tien-Yu Chang, Hao Dai, Vincent S. Tseng")],-1),ul=e("strong",null,"Link",-1),gl={href:"http://arxiv.org/abs/2404.00898v1",target:"_blank",rel:"noopener noreferrer"},pl=e("p",null,[e("strong",null,"Abstract"),t(": Data Augmentation is a common technique used to enhance the performance of deep learning models by expanding the training dataset. Automatic Data Augmentation (ADA) methods are getting popular because of their capacity to generate policies for various datasets. However, existing ADA methods primarily focused on overall performance improvement, neglecting the problem of class-dependent bias that leads to performance reduction in specific classes. This bias poses significant challenges when deploying models in real-world applications. Furthermore, ADA for time series remains an underexplored domain, highlighting the need for advancements in this field. In particular, applying ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling example due to its potential in medical domains such as heart disease diagnostics. We propose a novel deep learning-based approach called Class-dependent Automatic Adaptive Policies (CAAP) framework to overcome the notable class-dependent bias problem while maintaining the overall improvement in time-series data augmentation. Specifically, we utilize the policy network to generate effective sample-wise policies with balanced difficulty through class and feature information extraction. Second, we design the augmentation probability regulation method to minimize class-dependent bias. Third, we introduce the information region concepts into the ADA framework to preserve essential regions in the sample. Through a series of experiments on real-world ECG datasets, we demonstrate that CAAP outperforms representative methods in achieving lower class-dependent bias combined with superior overall performance. These results highlight the reliability of CAAP as a promising ADA method for time series modeling that fits for the demands of real-world applications.")],-1),ml=e("h4",{id:"voice-conversion-augmentation-for-speaker-recognition-on-defective-datasets",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#voice-conversion-augmentation-for-speaker-recognition-on-defective-datasets"},[e("span",null,"Voice Conversion Augmentation for Speaker Recognition on Defective Datasets")])],-1),fl=e("p",null,[e("strong",null,"Authors"),t(": Ruijie Tao, Zhan Shi, Yidi Jiang, Tianchi Liu, Haizhou Li")],-1),vl=e("strong",null,"Link",-1),bl={href:"http://arxiv.org/abs/2404.00863v1",target:"_blank",rel:"noopener noreferrer"},yl=e("p",null,[e("strong",null,"Abstract"),t(": Modern speaker recognition system relies on abundant and balanced datasets for classification training. However, diverse defective datasets, such as partially-labelled, small-scale, and imbalanced datasets, are common in real-world applications. Previous works usually studied specific solutions for each scenario from the algorithm perspective. However, the root cause of these problems lies in dataset imperfections. To address these challenges with a unified solution, we propose the Voice Conversion Augmentation (VCA) strategy to obtain pseudo speech from the training set. Furthermore, to guarantee generation quality, we designed the VCA-NN~(nearest neighbours) strategy to select source speech from utterances that are close to the target speech in the representation space. Our experimental results on three created datasets demonstrated that VCA-NN effectively mitigates these dataset problems, which provides a new direction for handling the speaker recognition problems from the data aspect.")],-1),wl=e("h2",{id:"_2024-03-31",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-31"},[e("span",null,"2024-03-31")])],-1),_l=e("h4",{id:"couda-coherence-evaluation-via-unified-data-augmentation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#couda-coherence-evaluation-via-unified-data-augmentation"},[e("span",null,"CoUDA: Coherence Evaluation via Unified Data Augmentation")])],-1),xl=e("p",null,[e("strong",null,"Authors"),t(": Dawei Zhu, Wenhao Wu, Yifan Song, Fangwei Zhu, Ziqiang Cao, Sujian Li")],-1),kl=e("strong",null,"Link",-1),Al={href:"http://arxiv.org/abs/2404.00681v1",target:"_blank",rel:"noopener noreferrer"},Ll=e("p",null,[e("strong",null,"Abstract"),t(": Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of large language models. Due to the scarcity of annotated data, data augmentation is commonly used for training coherence evaluation models. However, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance. In this paper, we take inspiration from linguistic theory of discourse structure, and propose a data augmentation framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively. Especially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. During inference, CoUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse. Extensive experiments in coherence evaluation show that, with only 233M parameters, CoUDA achieves state-of-the-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics.")],-1),Tl=e("h4",{id:"rq-rag-learning-to-refine-queries-for-retrieval-augmented-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#rq-rag-learning-to-refine-queries-for-retrieval-augmented-generation"},[e("span",null,"RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation")])],-1),Rl=e("p",null,[e("strong",null,"Authors"),t(": Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, Jie Fu")],-1),Ml=e("strong",null,"Link",-1),Cl={href:"http://arxiv.org/abs/2404.00610v1",target:"_blank",rel:"noopener noreferrer"},Sl=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.")],-1),Il=e("h2",{id:"_2024-03-30",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-30"},[e("span",null,"2024-03-30")])],-1),Dl=e("h4",{id:"multiway-point-cloud-mosaicking-with-diffusion-and-global-optimization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#multiway-point-cloud-mosaicking-with-diffusion-and-global-optimization"},[e("span",null,"Multiway Point Cloud Mosaicking with Diffusion and Global Optimization")])],-1),zl=e("p",null,[e("strong",null,"Authors"),t(": Shengze Jin, Iro Armeni, Marc Pollefeys, Daniel Barath")],-1),El=e("strong",null,"Link",-1),ql={href:"http://arxiv.org/abs/2404.00429v1",target:"_blank",rel:"noopener noreferrer"},Wl=e("p",null,[e("strong",null,"Abstract"),t(": We introduce a novel framework for multiway point cloud mosaicking (named Wednesday), designed to co-align sets of partially overlapping point clouds -- typically obtained from 3D scanners or moving RGB-D cameras -- into a unified coordinate system. At the core of our approach is ODIN, a learned pairwise registration algorithm that iteratively identifies overlaps and refines attention scores, employing a diffusion-based process for denoising pairwise correlation matrices to enhance matching accuracy. Further steps include constructing a pose graph from all point clouds, performing rotation averaging, a novel robust algorithm for re-estimating translations optimally in terms of consensus maximization and translation optimization. Finally, the point cloud rotations and positions are optimized jointly by a diffusion-based approach. Tested on four diverse, large-scale datasets, our method achieves state-of-the-art pairwise and multiway registration results by a large margin on all benchmarks. Our code and models are available at https://github.com/jinsz/Multiway-Point-Cloud-Mosaicking-with-Diffusion-and-Global-Optimization.")],-1);function Hl(Pl,Gl){const n=o("ExternalLinkIcon");return r(),s("div",null,[d,e("p",null,[h,t(": "),e("a",u,[t("http://arxiv.org/abs/2404.07938v1"),a(n)])]),g,p,m,e("p",null,[f,t(": "),e("a",v,[t("http://arxiv.org/abs/2404.07792v1"),a(n)])]),b,y,w,e("p",null,[_,t(": "),e("a",x,[t("http://arxiv.org/abs/2404.07645v1"),a(n)])]),k,A,L,e("p",null,[T,t(": "),e("a",R,[t("http://arxiv.org/abs/2404.07530v1"),a(n)])]),M,C,S,e("p",null,[I,t(": "),e("a",D,[t("http://arxiv.org/abs/2404.07514v1"),a(n)])]),z,E,q,e("p",null,[W,t(": "),e("a",H,[t("http://arxiv.org/abs/2404.07507v1"),a(n)])]),P,G,F,e("p",null,[O,t(": "),e("a",V,[t("http://arxiv.org/abs/2404.07501v1"),a(n)])]),N,B,j,e("p",null,[J,t(": "),e("a",K,[t("http://arxiv.org/abs/2404.07479v1"),a(n)])]),U,Y,X,e("p",null,[Z,t(": "),e("a",Q,[t("http://arxiv.org/abs/2404.07462v1"),a(n)])]),$,ee,te,ne,e("p",null,[ae,t(": "),e("a",ie,[t("http://arxiv.org/abs/2404.07348v1"),a(n)])]),oe,re,se,e("p",null,[le,t(": "),e("a",ce,[t("http://arxiv.org/abs/2404.07161v1"),a(n)])]),de,he,ue,e("p",null,[ge,t(": "),e("a",pe,[t("http://arxiv.org/abs/2404.07159v1"),a(n)])]),me,fe,ve,e("p",null,[be,t(": "),e("a",ye,[t("http://arxiv.org/abs/2404.07103v1"),a(n)])]),we,_e,xe,e("p",null,[ke,t(": "),e("a",Ae,[t("http://arxiv.org/abs/2404.07060v1"),a(n)])]),Le,Te,Re,e("p",null,[Me,t(": "),e("a",Ce,[t("http://arxiv.org/abs/2404.06910v1"),a(n)])]),Se,Ie,De,e("p",null,[ze,t(": "),e("a",Ee,[t("http://arxiv.org/abs/2404.06906v1"),a(n)])]),qe,We,He,e("p",null,[Pe,t(": "),e("a",Ge,[t("http://arxiv.org/abs/2404.06903v1"),a(n)])]),Fe,Oe,Ve,e("p",null,[Ne,t(": "),e("a",Be,[t("http://arxiv.org/abs/2404.06765v1"),a(n)])]),je,Je,Ke,e("p",null,[Ue,t(": "),e("a",Ye,[t("http://arxiv.org/abs/2404.06741v1"),a(n)])]),Xe,Ze,Qe,e("p",null,[$e,t(": "),e("a",et,[t("http://arxiv.org/abs/2404.06715v1"),a(n)])]),tt,nt,at,it,e("p",null,[ot,t(": "),e("a",rt,[t("http://arxiv.org/abs/2404.06641v1"),a(n)])]),st,lt,ct,e("p",null,[dt,t(": "),e("a",ht,[t("http://arxiv.org/abs/2404.06633v1"),a(n)])]),ut,gt,pt,e("p",null,[mt,t(": "),e("a",ft,[t("http://arxiv.org/abs/2404.06542v1"),a(n)])]),vt,bt,yt,e("p",null,[wt,t(": "),e("a",_t,[t("http://arxiv.org/abs/2404.06466v1"),a(n)])]),xt,kt,At,e("p",null,[Lt,t(": "),e("a",Tt,[t("http://arxiv.org/abs/2404.06337v1"),a(n)])]),Rt,Mt,Ct,e("p",null,[St,t(": "),e("a",It,[t("http://arxiv.org/abs/2404.06201v1"),a(n)])]),Dt,zt,Et,e("p",null,[qt,t(": "),e("a",Wt,[t("http://arxiv.org/abs/2404.06182v1"),a(n)])]),Ht,Pt,Gt,e("p",null,[Ft,t(": "),e("a",Ot,[t("http://arxiv.org/abs/2404.06152v1"),a(n)])]),Vt,Nt,Bt,e("p",null,[jt,t(": "),e("a",Jt,[t("http://arxiv.org/abs/2404.06101v1"),a(n)])]),Kt,Ut,Yt,e("p",null,[Xt,t(": "),e("a",Zt,[t("http://arxiv.org/abs/2404.06089v1"),a(n)])]),Qt,$t,en,e("p",null,[tn,t(": "),e("a",nn,[t("http://arxiv.org/abs/2404.06029v1"),a(n)])]),an,on,rn,e("p",null,[sn,t(": "),e("a",ln,[t("http://arxiv.org/abs/2404.05970v1"),a(n)])]),cn,dn,hn,un,e("p",null,[gn,t(": "),e("a",pn,[t("http://arxiv.org/abs/2404.05904v1"),a(n)])]),mn,fn,vn,e("p",null,[bn,t(": "),e("a",yn,[t("http://arxiv.org/abs/2404.05889v2"),a(n)])]),wn,_n,xn,e("p",null,[kn,t(": "),e("a",An,[t("http://arxiv.org/abs/2404.05888v1"),a(n)])]),Ln,Tn,Rn,e("p",null,[Mn,t(": "),e("a",Cn,[t("http://arxiv.org/abs/2404.05887v1"),a(n)])]),Sn,In,Dn,e("p",null,[zn,t(": "),e("a",En,[t("http://arxiv.org/abs/2404.05825v1"),a(n)])]),qn,Wn,Hn,e("p",null,[Pn,t(": "),e("a",Gn,[t("http://arxiv.org/abs/2404.05726v1"),a(n)])]),Fn,On,Vn,e("p",null,[Nn,t(": "),e("a",Bn,[t("http://arxiv.org/abs/2404.05693v1"),a(n)])]),jn,Jn,Kn,e("p",null,[Un,t(": "),e("a",Yn,[t("http://arxiv.org/abs/2404.05687v1"),a(n)])]),Xn,Zn,Qn,e("p",null,[$n,t(": "),e("a",ea,[t("http://arxiv.org/abs/2404.05490v2"),a(n)])]),ta,na,aa,e("p",null,[ia,t(": "),e("a",oa,[t("http://arxiv.org/abs/2404.05336v1"),a(n)])]),ra,sa,la,e("p",null,[ca,t(": "),e("a",da,[t("http://arxiv.org/abs/2404.05317v2"),a(n)])]),ha,ua,ga,e("p",null,[pa,t(": "),e("a",ma,[t("http://arxiv.org/abs/2404.05296v1"),a(n)])]),fa,va,ba,e("p",null,[ya,t(": "),e("a",wa,[t("http://arxiv.org/abs/2404.05183v1"),a(n)])]),_a,xa,ka,e("p",null,[Aa,t(": "),e("a",La,[t("http://arxiv.org/abs/2404.05151v1"),a(n)])]),Ta,Ra,Ma,e("p",null,[Ca,t(": "),e("a",Sa,[t("http://arxiv.org/abs/2404.05121v1"),a(n)])]),Ia,Da,za,Ea,e("p",null,[qa,t(": "),e("a",Wa,[t("http://arxiv.org/abs/2404.05083v1"),a(n)])]),Ha,Pa,Ga,e("p",null,[Fa,t(": "),e("a",Oa,[t("http://arxiv.org/abs/2404.05007v1"),a(n)])]),Va,Na,Ba,e("p",null,[ja,t(": "),e("a",Ja,[t("http://arxiv.org/abs/2404.04960v1"),a(n)])]),Ka,Ua,Ya,e("p",null,[Xa,t(": "),e("a",Za,[t("http://arxiv.org/abs/2404.04809v1"),a(n)])]),Qa,$a,ei,e("p",null,[ti,t(": "),e("a",ni,[t("http://arxiv.org/abs/2404.04774v1"),a(n)])]),ai,ii,oi,ri,e("p",null,[si,t(": "),e("a",li,[t("http://arxiv.org/abs/2404.04579v1"),a(n)])]),ci,di,hi,ui,e("p",null,[gi,t(": "),e("a",pi,[t("http://arxiv.org/abs/2404.04071v2"),a(n)])]),mi,fi,vi,e("p",null,[bi,t(": "),e("a",yi,[t("http://arxiv.org/abs/2404.03938v1"),a(n)])]),wi,_i,xi,e("p",null,[ki,t(": "),e("a",Ai,[t("http://arxiv.org/abs/2404.03899v1"),a(n)])]),Li,Ti,Ri,Mi,e("p",null,[Ci,t(": "),e("a",Si,[t("http://arxiv.org/abs/2404.04303v2"),a(n)])]),Ii,Di,zi,e("p",null,[Ei,t(": "),e("a",qi,[t("http://arxiv.org/abs/2404.04302v1"),a(n)])]),Wi,Hi,Pi,e("p",null,[Gi,t(": "),e("a",Fi,[t("http://arxiv.org/abs/2404.03814v1"),a(n)])]),Oi,Vi,Ni,e("p",null,[Bi,t(": "),e("a",ji,[t("http://arxiv.org/abs/2404.03774v1"),a(n)])]),Ji,Ki,Ui,e("p",null,[Yi,t(": "),e("a",Xi,[t("http://arxiv.org/abs/2404.03539v1"),a(n)])]),Zi,Qi,$i,e("p",null,[eo,t(": "),e("a",to,[t("http://arxiv.org/abs/2404.03498v1"),a(n)])]),no,ao,io,e("p",null,[oo,t(": "),e("a",ro,[t("http://arxiv.org/abs/2404.03337v1"),a(n)])]),so,lo,co,e("p",null,[ho,t(": "),e("a",uo,[t("http://arxiv.org/abs/2404.03321v1"),a(n)])]),go,po,mo,e("p",null,[fo,t(": "),e("a",vo,[t("http://arxiv.org/abs/2404.03239v1"),a(n)])]),bo,yo,wo,_o,e("p",null,[xo,t(": "),e("a",ko,[t("http://arxiv.org/abs/2404.03067v1"),a(n)])]),Ao,Lo,To,e("p",null,[Ro,t(": "),e("a",Mo,[t("http://arxiv.org/abs/2404.02841v1"),a(n)])]),Co,So,Io,e("p",null,[Do,t(": "),e("a",zo,[t("http://arxiv.org/abs/2404.02835v1"),a(n)])]),Eo,qo,Wo,e("p",null,[Ho,t(": "),e("a",Po,[t("http://arxiv.org/abs/2404.02832v1"),a(n)])]),Go,Fo,Oo,e("p",null,[Vo,t(": "),e("a",No,[t("http://arxiv.org/abs/2404.02616v1"),a(n)])]),Bo,jo,Jo,e("p",null,[Ko,t(": "),e("a",Uo,[t("http://arxiv.org/abs/2404.02543v1"),a(n)])]),Yo,Xo,Zo,e("p",null,[Qo,t(": "),e("a",$o,[t("http://arxiv.org/abs/2404.02477v1"),a(n)])]),er,tr,nr,ar,e("p",null,[ir,t(": "),e("a",or,[t("http://arxiv.org/abs/2404.02353v1"),a(n)])]),rr,sr,lr,e("p",null,[cr,t(": "),e("a",dr,[t("http://arxiv.org/abs/2404.02277v1"),a(n)])]),hr,ur,gr,e("p",null,[pr,t(": "),e("a",mr,[t("http://arxiv.org/abs/2404.02249v2"),a(n)])]),fr,vr,br,e("p",null,[yr,t(": "),e("a",wr,[t("http://arxiv.org/abs/2404.02141v1"),a(n)])]),_r,xr,kr,e("p",null,[Ar,t(": "),e("a",Lr,[t("http://arxiv.org/abs/2404.02046v1"),a(n)])]),Tr,Rr,Mr,e("p",null,[Cr,t(": "),e("a",Sr,[t("http://arxiv.org/abs/2404.02022v1"),a(n)])]),Ir,Dr,zr,e("p",null,[Er,t(": "),e("a",qr,[t("http://arxiv.org/abs/2404.01921v1"),a(n)])]),Wr,Hr,Pr,e("p",null,[Gr,t(": "),e("a",Fr,[t("http://arxiv.org/abs/2404.01901v1"),a(n)])]),Or,Vr,Nr,e("p",null,[Br,t(": "),e("a",jr,[t("http://arxiv.org/abs/2404.01887v2"),a(n)])]),Jr,Kr,Ur,e("p",null,[Yr,t(": "),e("a",Xr,[t("http://arxiv.org/abs/2404.01852v1"),a(n)])]),Zr,Qr,$r,e("p",null,[es,t(": "),e("a",ts,[t("http://arxiv.org/abs/2404.01817v3"),a(n)])]),ns,as,is,e("p",null,[os,t(": "),e("a",rs,[t("http://arxiv.org/abs/2404.01713v1"),a(n)])]),ss,ls,cs,e("p",null,[ds,t(": "),e("a",hs,[t("http://arxiv.org/abs/2404.01699v1"),a(n)])]),us,gs,ps,e("p",null,[ms,t(": "),e("a",fs,[t("http://arxiv.org/abs/2404.01554v1"),a(n)])]),vs,bs,ys,e("p",null,[ws,t(": "),e("a",_s,[t("http://arxiv.org/abs/2404.01543v1"),a(n)])]),xs,ks,As,Ls,e("p",null,[Ts,t(": "),e("a",Rs,[t("http://arxiv.org/abs/2404.01490v1"),a(n)])]),Ms,Cs,Ss,e("p",null,[Is,t(": "),e("a",Ds,[t("http://arxiv.org/abs/2404.01475v1"),a(n)])]),zs,Es,qs,e("p",null,[Ws,t(": "),e("a",Hs,[t("http://arxiv.org/abs/2404.01400v1"),a(n)])]),Ps,Gs,Fs,e("p",null,[Os,t(": "),e("a",Vs,[t("http://arxiv.org/abs/2404.01248v1"),a(n)])]),Ns,Bs,js,e("p",null,[Js,t(": "),e("a",Ks,[t("http://arxiv.org/abs/2404.01192v1"),a(n)])]),Us,Ys,Xs,e("p",null,[Zs,t(": "),e("a",Qs,[t("http://arxiv.org/abs/2404.01151v1"),a(n)])]),$s,el,tl,e("p",null,[nl,t(": "),e("a",al,[t("http://arxiv.org/abs/2404.01024v1"),a(n)])]),il,ol,rl,e("p",null,[sl,t(": "),e("a",ll,[t("http://arxiv.org/abs/2404.00924v1"),a(n)])]),cl,dl,hl,e("p",null,[ul,t(": "),e("a",gl,[t("http://arxiv.org/abs/2404.00898v1"),a(n)])]),pl,ml,fl,e("p",null,[vl,t(": "),e("a",bl,[t("http://arxiv.org/abs/2404.00863v1"),a(n)])]),yl,wl,_l,xl,e("p",null,[kl,t(": "),e("a",Al,[t("http://arxiv.org/abs/2404.00681v1"),a(n)])]),Ll,Tl,Rl,e("p",null,[Ml,t(": "),e("a",Cl,[t("http://arxiv.org/abs/2404.00610v1"),a(n)])]),Sl,Il,Dl,zl,e("p",null,[El,t(": "),e("a",ql,[t("http://arxiv.org/abs/2404.00429v1"),a(n)])]),Wl])}const Ol=i(c,[["render",Hl],["__file","XR.html.vue"]]),Vl=JSON.parse('{"path":"/posts/XR/paper/XR.html","title":"XR","lang":"en-US","frontmatter":{"description":"XR 2024-04-11 Toward ultra-efficient high fidelity predictions of wind turbine wakes: Augmenting the accuracy of engineering models via LES-trained machine learning Authors: Chr...","head":[["meta",{"property":"og:url","content":"https://opendesign.world/posts/XR/paper/XR.html"}],["meta",{"property":"og:site_name","content":"OpenDesign"}],["meta",{"property":"og:title","content":"XR"}],["meta",{"property":"og:description","content":"XR 2024-04-11 Toward ultra-efficient high fidelity predictions of wind turbine wakes: Augmenting the accuracy of engineering models via LES-trained machine learning Authors: Chr..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"XR\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[]}"]]},"headers":[{"level":2,"title":"2024-04-11","slug":"_2024-04-11","link":"#_2024-04-11","children":[]},{"level":2,"title":"2024-04-10","slug":"_2024-04-10","link":"#_2024-04-10","children":[]},{"level":2,"title":"2024-04-09","slug":"_2024-04-09","link":"#_2024-04-09","children":[]},{"level":2,"title":"2024-04-08","slug":"_2024-04-08","link":"#_2024-04-08","children":[]},{"level":2,"title":"2024-04-07","slug":"_2024-04-07","link":"#_2024-04-07","children":[]},{"level":2,"title":"2024-04-06","slug":"_2024-04-06","link":"#_2024-04-06","children":[]},{"level":2,"title":"2024-04-05","slug":"_2024-04-05","link":"#_2024-04-05","children":[]},{"level":2,"title":"2024-04-04","slug":"_2024-04-04","link":"#_2024-04-04","children":[]},{"level":2,"title":"2024-04-03","slug":"_2024-04-03","link":"#_2024-04-03","children":[]},{"level":2,"title":"2024-04-02","slug":"_2024-04-02","link":"#_2024-04-02","children":[]},{"level":2,"title":"2024-04-01","slug":"_2024-04-01","link":"#_2024-04-01","children":[]},{"level":2,"title":"2024-03-31","slug":"_2024-03-31","link":"#_2024-03-31","children":[]},{"level":2,"title":"2024-03-30","slug":"_2024-03-30","link":"#_2024-03-30","children":[]}],"git":{"updatedTime":null,"contributors":[]},"filePathRelative":"posts/XR/paper/XR.md","autoDesc":true,"excerpt":"\\n<h2>2024-04-11</h2>\\n<h4>Toward ultra-efficient high fidelity predictions of wind turbine wakes: Augmenting the accuracy of engineering models via LES-trained machine learning</h4>\\n<p><strong>Authors</strong>: Christian Santoni, Dichang Zhang, Zexia Zhang, Dimitris Samaras, Fotis Sotiropoulos, Ali Khosronejad</p>"}');export{Ol as comp,Vl as data};
