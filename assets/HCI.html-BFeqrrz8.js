import{_ as a,r as o,o as s,c as r,a as e,b as t,d as i,e as l}from"./app-DmGAXvh9.js";const c={},h=l('<h1 id="hci" tabindex="-1"><a class="header-anchor" href="#hci"><span>HCI</span></a></h1><h2 id="_2024-04-04" tabindex="-1"><a class="header-anchor" href="#_2024-04-04"><span>2024-04-04</span></a></h2><h4 id="creator-hearts-investigating-the-impact-positive-signals-from-youtube-creators-in-shaping-comment-section-behavior" tabindex="-1"><a class="header-anchor" href="#creator-hearts-investigating-the-impact-positive-signals-from-youtube-creators-in-shaping-comment-section-behavior"><span>Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior</span></a></h4><p><strong>Authors</strong>: Frederick Choi, Charlotte Lambert, Vinay Koshy, Sowmya Pratipati, Tue Do, Eshwar Chandrasekharan</p>',4),d=e("strong",null,"Link",-1),u={href:"http://arxiv.org/abs/2404.03612v1",target:"_blank",rel:"noopener noreferrer"},p=e("p",null,[e("strong",null,"Abstract"),t(': Much of the research in online moderation focuses on punitive actions. However, emerging research has shown that positive reinforcement is effective at encouraging desirable behavior on online platforms. We extend this research by studying the "creator heart" feature on YouTube, quantifying their primary effects on comments that receive hearts and on videos where hearts have been given. We find that creator hearts increased the visibility of comments, and increased the amount of positive engagement they received from other users. We also find that the presence of a creator hearted comment soon after a video is published can incentivize viewers to comment, increasing the total engagement with the video over time. We discuss the potential for creators to use hearts to shape behavior in their communities by highlighting, rewarding, and incentivizing desirable behaviors from users. We discuss avenues for extending our study to understanding positive signals from moderators on other platforms.')],-1),g=e("h4",{id:"integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work"},[e("span",null,"Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work")])],-1),m=e("p",null,[e("strong",null,"Authors"),t(": Somin Park, Carol C. Menassa, Vineet R. Kamat")],-1),f=e("strong",null,"Link",-1),v={href:"http://arxiv.org/abs/2404.03498v1",target:"_blank",rel:"noopener noreferrer"},b=e("p",null,[e("strong",null,"Abstract"),t(": In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement. This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants. This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots. By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting. Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs. The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry.")],-1),y=e("h4",{id:"agora-elevator-bodily-sensation-study-a-report",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#agora-elevator-bodily-sensation-study-a-report"},[e("span",null,"Agora Elevator Bodily Sensation Study -- a report")])],-1),w=e("p",null,[e("strong",null,"Authors"),t(": Rebekah Rousi")],-1),_=e("strong",null,"Link",-1),k={href:"http://arxiv.org/abs/2404.03356v1",target:"_blank",rel:"noopener noreferrer"},x=e("p",null,[e("strong",null,"Abstract"),t(": This study set out to examine the relationship between expressed social emotions (i.e. that what people say they are feeling) and physical sensations, the connection between emotion and bodily experience. It additionally provided the opportunity to investigate how the neurological findings of gender differences can be observed in practice, what difference does it make in behaviour and judgment that we have varying levels of mirror neuron activity? The following report documents the study, procedure, results and findings.")],-1),A=e("h4",{id:"influence-of-gameplay-duration-hand-tracking-and-controller-based-control-methods-on-ux-in-vr",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#influence-of-gameplay-duration-hand-tracking-and-controller-based-control-methods-on-ux-in-vr"},[e("span",null,"Influence of Gameplay Duration, Hand Tracking, and Controller Based Control Methods on UX in VR")])],-1),L=e("p",null,[e("strong",null,"Authors"),t(": Tanja Kojić, Maurizio Vergari, Simon Knuth, Maximilian Warsinke, Sebastian Möller, Jan-Niklas Voigt-Antons")],-1),T=e("strong",null,"Link",-1),I={href:"http://arxiv.org/abs/2404.03337v1",target:"_blank",rel:"noopener noreferrer"},M=e("p",null,[e("strong",null,"Abstract"),t(": Inside-out tracking is growing popular in consumer VR, enhancing accessibility. It uses HMD camera data and neural networks for effective hand tracking. However, limited user experience studies have compared this method to traditional controllers, with no consensus on the optimal control technique. This paper investigates the impact of control methods and gaming duration on VR user experience, hypothesizing hand tracking might be preferred for short sessions and by users new to VR due to its simplicity. Through a lab study with twenty participants, evaluating presence, emotional response, UX quality, and flow, findings revealed control type and session length affect user experience without significant interaction. Controllers were generally superior, attributed to their reliability, and longer sessions increased presence and realism. The study found that individuals with more VR experience were more inclined to recommend hand tracking to others, which contradicted predictions.")],-1),S=e("h4",{id:"exploring-emotions-in-multi-componential-space-using-interactive-vr-games",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-emotions-in-multi-componential-space-using-interactive-vr-games"},[e("span",null,"Exploring Emotions in Multi-componential Space using Interactive VR Games")])],-1),C=e("p",null,[e("strong",null,"Authors"),t(": Rukshani Somarathna, Gelareh Mohammadi")],-1),z=e("strong",null,"Link",-1),W={href:"http://arxiv.org/abs/2404.03239v1",target:"_blank",rel:"noopener noreferrer"},P=e("p",null,[e("strong",null,"Abstract"),t(": Emotion understanding is a complex process that involves multiple components. The ability to recognise emotions not only leads to new context awareness methods but also enhances system interaction's effectiveness by perceiving and expressing emotions. Despite the attention to discrete and dimensional models, neuroscientific evidence supports those emotions as being complex and multi-faceted. One framework that resonated well with such findings is the Component Process Model (CPM), a theory that considers the complexity of emotions with five interconnected components: appraisal, expression, motivation, physiology and feeling. However, the relationship between CPM and discrete emotions has not yet been fully explored. Therefore, to better understand emotions underlying processes, we operationalised a data-driven approach using interactive Virtual Reality (VR) games and collected multimodal measures (self-reports, physiological and facial signals) from 39 participants. We used Machine Learning (ML) methods to identify the unique contributions of each component to emotion differentiation. Our results showed the role of different components in emotion differentiation, with the model including all components demonstrating the most significant contribution. Moreover, we found that at least five dimensions are needed to represent the variation of emotions in our dataset. These findings also have implications for using VR environments in emotion research and highlight the role of physiological signals in emotion recognition within such environments.")],-1),R=e("h4",{id:"nlp4gov-a-comprehensive-library-for-computational-policy-analysis",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#nlp4gov-a-comprehensive-library-for-computational-policy-analysis"},[e("span",null,"NLP4Gov: A Comprehensive Library for Computational Policy Analysis")])],-1),H=e("p",null,[e("strong",null,"Authors"),t(": Mahasweta Chakraborti, Sailendra Akash Bonagiri, Santiago Virgüez-Ruiz, Seth Frey")],-1),E=e("strong",null,"Link",-1),D={href:"http://arxiv.org/abs/2404.03206v1",target:"_blank",rel:"noopener noreferrer"},q=e("p",null,[e("strong",null,"Abstract"),t(": Formal rules and policies are fundamental in formally specifying a social system: its operation, boundaries, processes, and even ontology. Recent scholarship has highlighted the role of formal policy in collective knowledge creation, game communities, the production of digital public goods, and national social media governance. Researchers have shown interest in how online communities convene tenable self-governance mechanisms to regulate member activities and distribute rights and privileges by designating responsibilities, roles, and hierarchies. We present NLP4Gov, an interactive kit to train and aid scholars and practitioners alike in computational policy analysis. The library explores and integrates methods and capabilities from computational linguistics and NLP to generate semantic and symbolic representations of community policies from text records. Versatile, documented, and accessible, NLP4Gov provides granular and comparative views into institutional structures and interactions, along with other information extraction capabilities for downstream analysis.")],-1),B=e("h4",{id:"towards-collaborative-family-centered-design-for-online-safety-privacy-and-security",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-collaborative-family-centered-design-for-online-safety-privacy-and-security"},[e("span",null,"Towards Collaborative Family-Centered Design for Online Safety, Privacy and Security")])],-1),G=e("p",null,[e("strong",null,"Authors"),t(": Mamtaj Akter, Zainab Agha, Ashwaq Alsoubai, Naima Ali, Pamela Wisniewski")],-1),N=e("strong",null,"Link",-1),V={href:"http://arxiv.org/abs/2404.03165v1",target:"_blank",rel:"noopener noreferrer"},O=e("p",null,[e("strong",null,"Abstract"),t(": Traditional online safety technologies often overly restrict teens and invade their privacy, while parents often lack knowledge regarding their digital privacy. As such, prior researchers have called for more collaborative approaches on adolescent online safety and networked privacy. In this paper, we propose family-centered approaches to foster parent-teen collaboration in ensuring their mobile privacy and online safety while respecting individual privacy, to enhance open discussion and teens' self-regulation. However, challenges such as power imbalances and conflicts with family values arise when implementing such approaches, making parent-teen collaboration difficult. Therefore, attending the family-centered design workshop will provide an invaluable opportunity for us to discuss these challenges and identify best research practices for the future of collaborative online safety and privacy within families.")],-1),j=e("h4",{id:"biodegradable-interactive-materials",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#biodegradable-interactive-materials"},[e("span",null,"Biodegradable Interactive Materials")])],-1),F=e("p",null,[e("strong",null,"Authors"),t(": Zhihan Zhang, Mallory Parker, Kuotian Liao, Jerry Cao, Anandghan Waghmare, Joseph Breda, Chris Matsumura, Serena Eley, Eleftheria Roumeli, Shwetak Patel, Vikram Iyer")],-1),J=e("strong",null,"Link",-1),U={href:"http://arxiv.org/abs/2404.03130v1",target:"_blank",rel:"noopener noreferrer"},K=e("p",null,[e("strong",null,"Abstract"),t(": The sense of touch is fundamental to how we interact with the physical and digital world. Conventional interactive surfaces and tactile interfaces use electronic sensors embedded into objects, however this approach poses serious challenges both for environmental sustainability and a future of truly ubiquitous interaction systems where information is encoded into everyday objects. In this work, we present Biodegradable Interactive Materials: backyard-compostable interactive interfaces that leverage information encoded in material properties. Inspired by natural systems, we propose an architecture that programmatically encodes multidimensional information into materials themselves and combines them with wearable devices that extend human senses to perceive the embedded data. We combine unrefined biological matter from plants and algae like chlorella with natural minerals like graphite and magnetite to produce materials with varying electrical, magnetic, and surface properties. We perform in-depth analysis using physics models, computational simulations, and real-world experiments to characterize their information density and develop decoding methods. Our passive, chip-less materials can robustly encode 12 bits of information, equivalent to 4096 unique classes. We further develop wearable device prototypes that can decode this information during touch interactions using off-the-shelf sensors. We demonstrate sample applications such as customized buttons, tactile maps, and interactive surfaces. We further demonstrate the natural degradation of these interactive materials in degrade outdoors within 21 days and perform a comparative environmental analysis of the benefits of this approach.")],-1),Z=e("h2",{id:"_2024-04-03",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-03"},[e("span",null,"2024-04-03")])],-1),Y=e("h4",{id:"writing-with-ai-lowers-psychological-ownership-but-longer-prompts-can-help",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#writing-with-ai-lowers-psychological-ownership-but-longer-prompts-can-help"},[e("span",null,"Writing with AI Lowers Psychological Ownership, but Longer Prompts Can Help")])],-1),X=e("p",null,[e("strong",null,"Authors"),t(": Nikhita Joshi, Daniel Vogel")],-1),Q=e("strong",null,"Link",-1),$={href:"http://arxiv.org/abs/2404.03108v1",target:"_blank",rel:"noopener noreferrer"},ee=e("p",null,[e("strong",null,"Abstract"),t(': Feelings of something belonging to someone is called "psychological ownership." A common assumption is that writing with generative AI lowers psychological ownership, but the extent to which this occurs and the role of prompt length are unclear. We report on two experiments to better understand the relationship between psychological ownership and prompt length. Participants wrote short stories either completely by themselves or wrote prompts of varying lengths, enforced through word limits. Results show that when participants wrote longer prompts, they had higher levels of psychological ownership. Their comments suggest they felt encouraged to think more about their prompts and include more details about the story plot. However, these benefits plateaued when the prompt length was 75-100% of the target story length. Based on these results, we propose prompt entry interface designs that nudge users with soft and hard constraints to write longer prompts for increased psychological ownership.')],-1),te=e("h4",{id:"talaria-interactively-optimizing-machine-learning-models-for-efficient-inference",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#talaria-interactively-optimizing-machine-learning-models-for-efficient-inference"},[e("span",null,"Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference")])],-1),ne=e("p",null,[e("strong",null,"Authors"),t(": Fred Hohman, Chaoqun Wang, Jinmook Lee, Jochen Görtler, Dominik Moritz, Jeffrey P Bigham, Zhile Ren, Cecile Foret, Qi Shan, Xiaoyi Zhang")],-1),ie=e("strong",null,"Link",-1),ae={href:"http://arxiv.org/abs/2404.03085v1",target:"_blank",rel:"noopener noreferrer"},oe=e("p",null,[e("strong",null,"Abstract"),t(": On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences. However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power. To help practitioners create efficient ML models, we designed and developed Talaria: a model visualization and optimization system. Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics. Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria.")],-1),se=e("h4",{id:"toward-safe-evolution-of-artificial-intelligence-ai-based-conversational-agents-to-support-adolescent-mental-and-sexual-health-knowledge-discovery",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#toward-safe-evolution-of-artificial-intelligence-ai-based-conversational-agents-to-support-adolescent-mental-and-sexual-health-knowledge-discovery"},[e("span",null,"Toward Safe Evolution of Artificial Intelligence (AI) based Conversational Agents to Support Adolescent Mental and Sexual Health Knowledge Discovery")])],-1),re=e("p",null,[e("strong",null,"Authors"),t(": Jinkyung Park, Vivek Singh, Pamela Wisniewski")],-1),le=e("strong",null,"Link",-1),ce={href:"http://arxiv.org/abs/2404.03023v1",target:"_blank",rel:"noopener noreferrer"},he=e("p",null,[e("strong",null,"Abstract"),t(": Following the recent release of various Artificial Intelligence (AI) based Conversation Agents (CAs), adolescents are increasingly using CAs for interactive knowledge discovery on sensitive topics, including mental and sexual health topics. Exploring such sensitive topics through online search has been an essential part of adolescent development, and CAs can support their knowledge discovery on such topics through human-like dialogues. Yet, unintended risks have been documented with adolescents' interactions with AI-based CAs, such as being exposed to inappropriate content, false information, and/or being given advice that is detrimental to their mental and physical well-being (e.g., to self-harm). In this position paper, we discuss the current landscape and opportunities for CAs to support adolescents' mental and sexual health knowledge discovery. We also discuss some of the challenges related to ensuring the safety of adolescents when interacting with CAs regarding sexual and mental health topics. We call for a discourse on how to set guardrails for the safe evolution of AI-based CAs for adolescents.")],-1),de=e("h4",{id:"asap-interpretable-analysis-and-summarization-of-ai-generated-image-patterns-at-scale",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#asap-interpretable-analysis-and-summarization-of-ai-generated-image-patterns-at-scale"},[e("span",null,"ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale")])],-1),ue=e("p",null,[e("strong",null,"Authors"),t(": Jinbin Huang, Chen Chen, Aditi Mishra, Bum Chul Kwon, Zhicheng Liu, Chris Bryan")],-1),pe=e("strong",null,"Link",-1),ge={href:"http://arxiv.org/abs/2404.02990v1",target:"_blank",rel:"noopener noreferrer"},me=e("p",null,[e("strong",null,"Abstract"),t(`: Generative image models have emerged as a promising technology to produce realistic images. Despite potential benefits, concerns grow about its misuse, particularly in generating deceptive images that could raise significant ethical, legal, and societal issues. Consequently, there is growing demand to empower users to effectively discern and comprehend patterns of AI-generated images. To this end, we developed ASAP, an interactive visualization system that automatically extracts distinct patterns of AI-generated images and allows users to interactively explore them via various views. To uncover fake patterns, ASAP introduces a novel image encoder, adapted from CLIP, which transforms images into compact "distilled" representations, enriched with information for differentiating authentic and fake images. These representations generate gradients that propagate back to the attention maps of CLIP's transformer block. This process quantifies the relative importance of each pixel to image authenticity or fakeness, exposing key deceptive patterns. ASAP enables the at scale interactive analysis of these patterns through multiple, coordinated visualizations. This includes a representation overview with innovative cell glyphs to aid in the exploration and qualitative evaluation of fake patterns across a vast array of images, as well as a pattern view that displays authenticity-indicating patterns in images and quantifies their impact. ASAP supports the analysis of cutting-edge generative models with the latest architectures, including GAN-based models like proGAN and diffusion models like the latent diffusion model. We demonstrate ASAP's usefulness through two usage scenarios using multiple fake image detection benchmark datasets, revealing its ability to identify and understand hidden patterns in AI-generated images, especially in detecting fake human faces produced by diffusion-based techniques.`)],-1),fe=e("h4",{id:"fragmented-moments-balanced-choices-how-do-people-make-use-of-their-waiting-time",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#fragmented-moments-balanced-choices-how-do-people-make-use-of-their-waiting-time"},[e("span",null,"Fragmented Moments, Balanced Choices: How Do People Make Use of Their Waiting Time?")])],-1),ve=e("p",null,[e("strong",null,"Authors"),t(": Jian Zheng, Ge Gao")],-1),be=e("strong",null,"Link",-1),ye={href:"http://arxiv.org/abs/2404.02880v1",target:"_blank",rel:"noopener noreferrer"},we=e("p",null,[e("strong",null,"Abstract"),t(": Everyone spends some time waiting every day. HCI research has developed tools for boosting productivity while waiting. However, little is known about how people naturally spend their waiting time. We conducted an experience sampling study with 21 working adults who used a mobile app to report their daily waiting time activities over two weeks. The aim of this study is to understand the activities people do while waiting and the effect of situational factors. We found that participants spent about 60% of their waiting time on leisure activities, 20% on productive activities, and 20% on maintenance activities. These choices are sensitive to situational factors, including accessible device, location, and certain routines of the day. Our study complements previous ones by demonstrating that people purpose waiting time for various goals beyond productivity and to maintain work-life balance. Our findings shed light on future empirical research and system design for time management.")],-1),_e=e("h4",{id:"the-realhumaneval-evaluating-large-language-models-abilities-to-support-programmers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-realhumaneval-evaluating-large-language-models-abilities-to-support-programmers"},[e("span",null,"The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers")])],-1),ke=e("p",null,[e("strong",null,"Authors"),t(": Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, David Sontag")],-1),xe=e("strong",null,"Link",-1),Ae={href:"http://arxiv.org/abs/2404.02806v1",target:"_blank",rel:"noopener noreferrer"},Le=e("p",null,[e("strong",null,"Abstract"),t(": Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional -- a trend that holds across both forms of LLM support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals. We also open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.")],-1),Te=e("h4",{id:"ai-and-personalized-learning-bridging-the-gap-with-modern-educational-goals",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ai-and-personalized-learning-bridging-the-gap-with-modern-educational-goals"},[e("span",null,"AI and personalized learning: bridging the gap with modern educational goals")])],-1),Ie=e("p",null,[e("strong",null,"Authors"),t(": Kristjan-Julius Laak, Jaan Aru")],-1),Me=e("strong",null,"Link",-1),Se={href:"http://arxiv.org/abs/2404.02798v1",target:"_blank",rel:"noopener noreferrer"},Ce=e("p",null,[e("strong",null,"Abstract"),t(": Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education. Technology-based PL solutions have shown notable effectiveness in enhancing learning performance. However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas. In this paper, we examine the characteristics of AI-driven PL solutions in light of the OECD Learning Compass 2030 goals. Our analysis indicates a gap between the objectives of modern education and the current direction of PL. We identify areas where most present-day PL technologies could better embrace essential elements of contemporary education, such as collaboration, cognitive engagement, and the development of general competencies. While the present PL solutions are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system. Finally, we explore the potential of large language models, such as ChatGPT, and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning.")],-1),ze=e("h4",{id:"ieee-vis-workshop-on-visualization-for-climate-action-and-sustainability",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ieee-vis-workshop-on-visualization-for-climate-action-and-sustainability"},[e("span",null,"IEEE VIS Workshop on Visualization for Climate Action and Sustainability")])],-1),We=e("p",null,[e("strong",null,"Authors"),t(": Benjamin Bach, Fanny Chevalier, Helen-Nicole Kostis, Mark Subbaro, Yvonne Jansen, Robert Soden")],-1),Pe=e("strong",null,"Link",-1),Re={href:"http://arxiv.org/abs/2404.02743v1",target:"_blank",rel:"noopener noreferrer"},He=e("p",null,[e("strong",null,"Abstract"),t(": This first workshop on visualization for climate action and sustainability aims to explore and consolidate the role of data visualization in accelerating action towards addressing the current environmental crisis. Given the urgency and impact of the environmental crisis, we ask how our skills, research methods, and innovations can help by empowering people and organizations. We believe visualization holds an enormous power to aid understanding, decision making, communication, discussion, participation, education, and exploration of complex topics around climate action and sustainability. Hence, this workshop invites submissions and discussion around these topics with the goal of establishing a visible and actionable link between these fields and their respective stakeholders. The workshop solicits work-in-progress and research papers as well as pictorials and interactive demos from the whole range of visualization research (dashboards, interactive spaces, scientific visualization, storytelling, visual analytics, explainability etc.), within the context of environmentalism (climate science, sustainability, energy, circular economy, biodiversity, etc.) and across a range of scenarios from public awareness and understanding, visual analysis, expert decision making, science communication, personal decision making etc. After presentations of submissions, the workshop will feature dedicated discussion groups around data driven interactive experiences for the public, and tools for personal and professional decision making.")],-1),Ee=e("h4",{id:"evolving-agents-interactive-simulation-of-dynamic-and-diverse-human-personalities",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evolving-agents-interactive-simulation-of-dynamic-and-diverse-human-personalities"},[e("span",null,"Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities")])],-1),De=e("p",null,[e("strong",null,"Authors"),t(": Jiale Li, Jiayang Li, Jiahao Chen, Yifan Li, Shijie Wang, Hugo Zhou, Minjun Ye, Yunsheng Su")],-1),qe=e("strong",null,"Link",-1),Be={href:"http://arxiv.org/abs/2404.02718v1",target:"_blank",rel:"noopener noreferrer"},Ge=e("p",null,[e("strong",null,"Abstract"),t(": Human-like Agents with diverse and dynamic personality could serve as an important design probe in the process of user-centered design, thereby enabling designers to enhance the user experience of interactive application.In this article, we introduce Evolving Agents, a novel agent architecture that consists of two systems: Personality and Behavior. The Personality system includes three modules: Cognition, Emotion and Character Growth. The Behavior system comprises two modules: Planning and Action. We also build a simulation platform that enables agents to interact with the environment and other agents. Evolving Agents can simulate the human personality evolution process. Compared to its initial state, agents' personality and behavior patterns undergo believable development after several days of simulation. Agents reflect on their behavior to reason and develop new personality traits. These traits, in turn, generate new behavior patterns, forming a feedback loop-like personality evolution.In our experiment, we utilized simulation platform with 10 agents for evaluation. During the evaluation, these agents experienced believable and inspirational personality evolution. Through ablation and control experiments, we demonstrated the outstanding effectiveness of agent personality evolution and all modules of our agent architecture contribute to creating believable human-like agents with diverse and dynamic personalities. We also demonstrated through workshops how Evolving Agents could inspire designers.")],-1),Ne=e("h4",{id:"unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm"},[e("span",null,"Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM")])],-1),Ve=e("p",null,[e("strong",null,"Authors"),t(": Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Yuekai Huang, Jun Hu, Qing Wang")],-1),Oe=e("strong",null,"Link",-1),je={href:"http://arxiv.org/abs/2404.02706v1",target:"_blank",rel:"noopener noreferrer"},Fe=e("p",null,[e("strong",null,"Abstract"),t(": Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text. These issues are mostly caused by developers' lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI.")],-1),Je=e("h4",{id:"spatial-summation-of-localized-pressure-for-haptic-sensory-prostheses",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#spatial-summation-of-localized-pressure-for-haptic-sensory-prostheses"},[e("span",null,"Spatial Summation of Localized Pressure for Haptic Sensory Prostheses")])],-1),Ue=e("p",null,[e("strong",null,"Authors"),t(": Sreela Kodali, Cihualpilli Camino Cruz, Thomas C. Bulea, Kevin S. Rao Diana Bharucha-Goebel, Alexander T. Chesler, Carsten G. Bonnemann, Allison M. Okamura")],-1),Ke=e("strong",null,"Link",-1),Ze={href:"http://arxiv.org/abs/2404.02565v1",target:"_blank",rel:"noopener noreferrer"},Ye=e("p",null,[e("strong",null,"Abstract"),t(": A host of medical conditions, including amputations, diabetes, stroke, and genetic disease, result in loss of touch sensation. Because most types of sensory loss have no pharmacological treatment or rehabilitative therapy, we propose a haptic sensory prosthesis that provides substitutive feedback. The wrist and forearm are compelling locations for feedback due to available skin area and not occluding the hands, but have reduced mechanoreceptor density compared to the fingertips. Focusing on localized pressure as the feedback modality, we hypothesize that we can improve on prior devices by invoking a wider range of stimulus intensity using multiple points of pressure to evoke spatial summation, which is the cumulative perceptual experience from multiple points of stimuli. We conducted a preliminary perceptual test to investigate this idea and found that just noticeable difference is reduced with two points of pressure compared to one, motivating future work using spatial summation in sensory prostheses.")],-1),Xe=e("h4",{id:"promptrpa-generating-robotic-process-automation-on-smartphones-from-textual-prompts",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#promptrpa-generating-robotic-process-automation-on-smartphones-from-textual-prompts"},[e("span",null,"PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts")])],-1),Qe=e("p",null,[e("strong",null,"Authors"),t(": Tian Huang, Chun Yu, Weinan Shi, Zijian Peng, David Yang, Weiqi Sun, Yuanchun Shi")],-1),$e=e("strong",null,"Link",-1),et={href:"http://arxiv.org/abs/2404.02475v1",target:"_blank",rel:"noopener noreferrer"},tt=e("p",null,[e("strong",null,"Abstract"),t(": Robotic Process Automation (RPA) offers a valuable solution for efficiently automating tasks on the graphical user interface (GUI), by emulating human interactions, without modifying existing code. However, its broader adoption is constrained by the need for expertise in both scripting languages and workflow design. To address this challenge, we present PromptRPA, a system designed to comprehend various task-related textual prompts (e.g., goals, procedures), thereby generating and performing corresponding RPA tasks. PromptRPA incorporates a suite of intelligent agents that mimic human cognitive functions, specializing in interpreting user intent, managing external information for RPA generation, and executing operations on smartphones. The agents can learn from user feedback and continuously improve their performance based on the accumulated knowledge. Experimental results indicated a performance jump from a 22.28% success rate in the baseline to 95.21% with PromptRPA, requiring an average of 1.66 user interventions for each new task. PromptRPA presents promising applications in fields such as tutorial creation, smart assistance, and customer service.")],-1),nt=e("h4",{id:"a-neuroergonomics-model-to-evaluating-nuclear-power-plants-operators-performance-under-heat-stress-driven-by-ecg-time-frequency-spectrums-and-fnirs-prefrontal-cortex-network-a-cnn-gat-fusion-model",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-neuroergonomics-model-to-evaluating-nuclear-power-plants-operators-performance-under-heat-stress-driven-by-ecg-time-frequency-spectrums-and-fnirs-prefrontal-cortex-network-a-cnn-gat-fusion-model"},[e("span",null,"A neuroergonomics model to evaluating nuclear power plants operators' performance under heat stress driven by ECG time-frequency spectrums and fNIRS prefrontal cortex network: a CNN-GAT fusion model")])],-1),it=e("p",null,[e("strong",null,"Authors"),t(": Yan Zhang, Ming Jia, Meng Li, JianYu Wang, XiangMin Hu, ZhiHui Xu, Tao Chen")],-1),at=e("strong",null,"Link",-1),ot={href:"http://arxiv.org/abs/2404.02439v1",target:"_blank",rel:"noopener noreferrer"},st=e("p",null,[e("strong",null,"Abstract"),t(": Operators experience complicated physiological and psychological states when exposed to extreme heat stress, which can impair cognitive function and decrease performance significantly, ultimately leading to severe secondary disasters. Therefore, there is an urgent need for a feasible technique to identify their abnormal states to enhance the reliability of human-cybernetics systems. With the advancement of deep learning in physiological modeling, a model for evaluating operators' performance driven by electrocardiogram (ECG) and functional near-infrared spectroscopy (fNIRS) was proposed, demonstrating high ecological validity. The model fused a convolutional neural network (CNN) backbone and a graph attention network (GAT) backbone to extract discriminative features from ECG time-frequency spectrums and fNIRS prefrontal cortex (PFC) network respectively with deeper neuroscience domain knowledge, and eventually achieved 0.90 AUC. Results supported that handcrafted features extracted by specialized neuroscience methods can alleviate overfitting. Inspired by the small-world nature of the brain network, the fNIRS PFC network was organized as an undirected graph and embedded by GAT. It is proven to perform better in information aggregation and delivery compared to a simple non-linear transformation. The model provides a potential neuroergonomics application for evaluating the human state in vital human-cybernetics systems under industry 5.0 scenarios.")],-1),rt=e("h4",{id:"a-unified-editing-method-for-co-speech-gesture-generation-via-diffusion-inversion",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-unified-editing-method-for-co-speech-gesture-generation-via-diffusion-inversion"},[e("span",null,"A Unified Editing Method for Co-Speech Gesture Generation via Diffusion Inversion")])],-1),lt=e("p",null,[e("strong",null,"Authors"),t(": Zeyu Zhao, Nan Gao, Zhi Zeng, Guixuan Zhang, Jie Liu, Shuwu Zhang")],-1),ct=e("strong",null,"Link",-1),ht={href:"http://arxiv.org/abs/2404.02411v1",target:"_blank",rel:"noopener noreferrer"},dt=e("p",null,[e("strong",null,"Abstract"),t(": Diffusion models have shown great success in generating high-quality co-speech gestures for interactive humanoid robots or digital avatars from noisy input with the speech audio or text as conditions. However, they rarely focus on providing rich editing capabilities for content creators other than high-level specialized measures like style conditioning. To resolve this, we propose a unified framework utilizing diffusion inversion that enables multi-level editing capabilities for co-speech gesture generation without re-training. The method takes advantage of two key capabilities of invertible diffusion models. The first is that through inversion, we can reconstruct the intermediate noise from gestures and regenerate new gestures from the noise. This can be used to obtain gestures with high-level similarities to the original gestures for different speech conditions. The second is that this reconstruction reduces activation caching requirements during gradient calculation, making the direct optimization on input noises possible on current hardware with limited memory. With different loss functions designed for, e.g., joint rotation or velocity, we can control various low-level details by automatically tweaking the input noises through optimization. Extensive experiments on multiple use cases show that this framework succeeds in unifying high-level and low-level co-speech gesture editing.")],-1),ut=e("h2",{id:"_2024-04-02",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-02"},[e("span",null,"2024-04-02")])],-1),pt=e("h4",{id:"from-delays-to-densities-exploring-data-uncertainty-through-speech-text-and-visualization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#from-delays-to-densities-exploring-data-uncertainty-through-speech-text-and-visualization"},[e("span",null,"From Delays to Densities: Exploring Data Uncertainty through Speech, Text, and Visualization")])],-1),gt=e("p",null,[e("strong",null,"Authors"),t(": Chase Stokes, Chelsea Sanker, Bridget Cogley, Vidya Setlur")],-1),mt=e("strong",null,"Link",-1),ft={href:"http://arxiv.org/abs/2404.02317v1",target:"_blank",rel:"noopener noreferrer"},vt=e("p",null,[e("strong",null,"Abstract"),t(": Understanding and communicating data uncertainty is crucial for making informed decisions in sectors like finance and healthcare. Previous work has explored how to express uncertainty in various modes. For example, uncertainty can be expressed visually with quantile dot plots or linguistically with hedge words and prosody. Our research aims to systematically explore how variations within each mode contribute to communicating uncertainty to the user; this allows us to better understand each mode's affordances and limitations. We completed an exploration of the uncertainty design space based on pilot studies and ran two crowdsourced experiments examining how speech, text, and visualization modes and variants within them impact decision-making with uncertain data. Visualization and text were most effective for rational decision-making, though text resulted in lower confidence. Speech garnered the highest trust despite sometimes leading to risky decisions. Results from these studies indicate meaningful trade-offs among modes of information and encourage exploration of multimodal data representations.")],-1),bt=e("h4",{id:"a-change-of-scenery-transformative-insights-from-retrospective-vr-embodied-perspective-taking-of-conflict-with-a-close-other",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-change-of-scenery-transformative-insights-from-retrospective-vr-embodied-perspective-taking-of-conflict-with-a-close-other"},[e("span",null,"A Change of Scenery: Transformative Insights from Retrospective VR Embodied Perspective-Taking of Conflict With a Close Other")])],-1),yt=e("p",null,[e("strong",null,"Authors"),t(": Seraphina Yong, Leo Cui, Evan Suma Rosenberg, Svetlana Yarosh")],-1),wt=e("strong",null,"Link",-1),_t={href:"http://arxiv.org/abs/2404.02277v1",target:"_blank",rel:"noopener noreferrer"},kt=e("p",null,[e("strong",null,"Abstract"),t(": Close relationships are irreplaceable social resources, yet prone to high-risk conflict. Building on findings from the fields of HCI, virtual reality, and behavioral therapy, we evaluate the unexplored potential of retrospective VR-embodied perspective-taking to fundamentally influence conflict resolution in close others. We develop a biographically-accurate Retrospective Embodied Perspective-Taking system (REPT) and conduct a mixed-methods evaluation of its influence on close others' reflection and communication, compared to video-based reflection methods currently used in therapy (treatment as usual, or TAU). Our key findings provide evidence that REPT was able to significantly improve communication skills and positive sentiment of both partners during conflict, over TAU. The qualitative data also indicated that REPT surpassed basic perspective-taking by exclusively stimulating users to embody and reflect on both their own and their partner's experiences at the same level. In light of these findings, we provide implications and an agenda for social embodiment in HCI design: conceptualizing the use of `embodied social cognition,' and envisioning socially-embodied experiences as an interactive context.")],-1),xt=e("h4",{id:"exploring-how-multiple-levels-of-gpt-generated-programming-hints-support-or-disappoint-novices",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-how-multiple-levels-of-gpt-generated-programming-hints-support-or-disappoint-novices"},[e("span",null,"Exploring How Multiple Levels of GPT-Generated Programming Hints Support or Disappoint Novices")])],-1),At=e("p",null,[e("strong",null,"Authors"),t(": Ruiwei Xiao, Xinying Hou, John Stamper")],-1),Lt=e("strong",null,"Link",-1),Tt={href:"http://arxiv.org/abs/2404.02213v1",target:"_blank",rel:"noopener noreferrer"},It=e("p",null,[e("strong",null,"Abstract"),t(": Recent studies have integrated large language models (LLMs) into diverse educational contexts, including providing adaptive programming hints, a type of feedback focuses on helping students move forward during problem-solving. However, most existing LLM-based hint systems are limited to one single hint type. To investigate whether and how different levels of hints can support students' problem-solving and learning, we conducted a think-aloud study with 12 novices using the LLM Hint Factory, a system providing four levels of hints from general natural language guidance to concrete code assistance, varying in format and granularity. We discovered that high-level natural language hints alone can be helpless or even misleading, especially when addressing next-step or syntax-related help requests. Adding lower-level hints, like code examples with in-line comments, can better support students. The findings open up future work on customizing help responses from content, format, and granularity levels to accurately identify and meet students' learning needs.")],-1),Mt=e("h4",{id:"harder-better-faster-stronger-interactive-visualization-for-human-centered-ai-tools",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#harder-better-faster-stronger-interactive-visualization-for-human-centered-ai-tools"},[e("span",null,"Harder, Better, Faster, Stronger: Interactive Visualization for Human-Centered AI Tools")])],-1),St=e("p",null,[e("strong",null,"Authors"),t(": Md Naimul Hoque, Sungbok Shin, Niklas Elmqvist")],-1),Ct=e("strong",null,"Link",-1),zt={href:"http://arxiv.org/abs/2404.02147v1",target:"_blank",rel:"noopener noreferrer"},Wt=e("p",null,[e("strong",null,"Abstract"),t(": Human-centered AI (HCAI), rather than replacing the human, puts the human user in the driver's seat of so-called human-centered AI-infused tools (HCAI tools): interactive software tools that amplify, augment, empower, and enhance human performance using AI models; often novel generative or foundation AI ones. In this paper, we discuss how interactive visualization can be a key enabling technology for creating such human-centered AI tools. Visualization has already been shown to be a fundamental component in explainable AI models, and coupling this with data-driven, semantic, and unified interaction feedback loops will enable a human-centered approach to integrating AI models in the loop with human users. We present several examples of our past and current work on such HCAI tools, including for creative writing, temporal prediction, and user experience analysis. We then draw parallels between these tools to suggest common themes on how interactive visualization can support the design of future HCAI tools.")],-1),Pt=e("h4",{id:"the-effects-of-group-sanctions-on-participation-and-toxicity-quasi-experimental-evidence-from-the-fediverse",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-effects-of-group-sanctions-on-participation-and-toxicity-quasi-experimental-evidence-from-the-fediverse"},[e("span",null,"The Effects of Group Sanctions on Participation and Toxicity: Quasi-experimental Evidence from the Fediverse")])],-1),Rt=e("p",null,[e("strong",null,"Authors"),t(": Carl Colglazier, Nathan TeBlunthuis, Aaron Shaw")],-1),Ht=e("strong",null,"Link",-1),Et={href:"http://arxiv.org/abs/2404.02109v1",target:"_blank",rel:"noopener noreferrer"},Dt=e("p",null,[e("strong",null,"Abstract"),t(": Online communities often overlap and coexist, despite incongruent norms and approaches to content moderation. When communities diverge, decentralized and federated communities may pursue group-level sanctions, including defederation (disconnection) to block communication between members of specific communities. We investigate the effects of defederation in the context of the Fediverse, a set of decentralized, interconnected social networks with independent governance. Mastodon and Pleroma, the most popular software powering the Fediverse, allow administrators on one server to defederate from another. We use a difference-in-differences approach and matched controls to estimate the effects of defederation events on participation and message toxicity among affected members of the blocked and blocking servers. We find that defederation causes a drop in activity for accounts on the blocked servers, but not on the blocking servers. Also, we find no evidence of an effect of defederation on message toxicity.")],-1),qt=e("h4",{id:"explainability-in-jupyterlab-and-beyond-interactive-xai-systems-for-integrated-and-collaborative-workflows",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#explainability-in-jupyterlab-and-beyond-interactive-xai-systems-for-integrated-and-collaborative-workflows"},[e("span",null,"Explainability in JupyterLab and Beyond: Interactive XAI Systems for Integrated and Collaborative Workflows")])],-1),Bt=e("p",null,[e("strong",null,"Authors"),t(": Grace Guo, Dustin Arendt, Alex Endert")],-1),Gt=e("strong",null,"Link",-1),Nt={href:"http://arxiv.org/abs/2404.02081v1",target:"_blank",rel:"noopener noreferrer"},Vt=e("p",null,[e("strong",null,"Abstract"),t(": Explainable AI (XAI) tools represent a turn to more human-centered and human-in-the-loop AI approaches that emphasize user needs and perspectives in machine learning model development workflows. However, while the majority of ML resources available today are developed for Python computational environments such as JupyterLab and Jupyter Notebook, the same has not been true of interactive XAI systems, which are often still implemented as standalone interfaces. In this paper, we address this mismatch by identifying three design patterns for embedding front-end XAI interfaces into Jupyter, namely: 1) One-way communication from Python to JavaScript, 2) Two-way data synchronization, and 3) Bi-directional callbacks. We also provide an open-source toolkit, bonXAI, that demonstrates how each design pattern might be used to build interactive XAI tools for a Pytorch text classification workflow. Finally, we conclude with a discussion of best practices and open questions. Our aims for this paper are to discuss how interactive XAI tools might be developed for computational notebooks, and how they can better integrate into existing model development workflows to support more collaborative, human-centered AI.")],-1),Ot=e("h4",{id:"preuve-de-concept-d-un-bot-vocal-dialoguant-en-wolof",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#preuve-de-concept-d-un-bot-vocal-dialoguant-en-wolof"},[e("span",null,"Preuve de concept d'un bot vocal dialoguant en wolof")])],-1),jt=e("p",null,[e("strong",null,"Authors"),t(": Elodie Gauthier, Papa-Séga Wade, Thierry Moudenc, Patrice Collen, Emilie De Neef, Oumar Ba, Ndeye Khoyane Cama, Cheikh Ahmadou Bamba Kebe, Ndeye Aissatou Gningue, Thomas Mendo'o Aristide")],-1),Ft=e("strong",null,"Link",-1),Jt={href:"http://arxiv.org/abs/2404.02009v1",target:"_blank",rel:"noopener noreferrer"},Ut=e("p",null,[e("strong",null,"Abstract"),t(": This paper presents the proof-of-concept of the first automatic voice assistant ever built in Wolof language, the main vehicular language spoken in Senegal. This voicebot is the result of a collaborative research project between Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp, a small IT company based in Dakar, Senegal. The purpose of the voicebot is to provide information to Orange customers about the Sargal loyalty program of Orange Senegal by using the most natural mean to communicate: speech. The voicebot receives in input the customer's oral request that is then processed by a SLU system to reply to the customer's request using audio recordings. The first results of this proof-of-concept are encouraging as we achieved 22% of WER for the ASR task and 78% of F1-score on the NLU task.")],-1),Kt=e("h4",{id:"cash-or-non-cash-unveiling-ideators-incentive-preferences-in-crowdsourcing-contests",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#cash-or-non-cash-unveiling-ideators-incentive-preferences-in-crowdsourcing-contests"},[e("span",null,"Cash or Non-Cash? Unveiling Ideators' Incentive Preferences in Crowdsourcing Contests")])],-1),Zt=e("p",null,[e("strong",null,"Authors"),t(": Christoph Riedl, Johann Füller, Katja Hutter, Gerard J. Tellis")],-1),Yt=e("strong",null,"Link",-1),Xt={href:"http://arxiv.org/abs/2404.01997v1",target:"_blank",rel:"noopener noreferrer"},Qt=e("p",null,[e("strong",null,"Abstract"),t(": Even though research has repeatedly shown that non-cash incentives can be effective, cash incentives are the de facto standard in crowdsourcing contests. In this multi-study research, we quantify ideators' preferences for non-cash incentives and investigate how allowing ideators to self-select their preferred incentive -- offering ideators a choice between cash and non-cash incentives -- affects their creative performance. We further explore whether the market context of the organization hosting the contest -- social (non-profit) or monetary (for-profit) -- moderates incentive preferences and their effectiveness. We find that individuals exhibit heterogeneous incentive preferences and often prefer non-cash incentives, even in for-profit contexts. Offering ideators a choice of incentives can enhance creative performance. Market context moderates the effect of incentives, such that ideators who receive non-cash incentives in for-profit contexts tend to exert less effort. We show that heterogeneity of ideators' preferences (and the ability to satisfy diverse preferences with suitably diverse incentive options) is a critical boundary condition to realizing benefits from offering ideators a choice of incentives. We provide managers with guidance to design effective incentives by improving incentive-preference fit for ideators.")],-1),$t=e("h4",{id:"fast-and-adaptive-questionnaires-for-voting-advice-applications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#fast-and-adaptive-questionnaires-for-voting-advice-applications"},[e("span",null,"Fast and Adaptive Questionnaires for Voting Advice Applications")])],-1),en=e("p",null,[e("strong",null,"Authors"),t(": Fynn Bachmann, Cristina Sarasua, Abraham Bernstein")],-1),tn=e("strong",null,"Link",-1),nn={href:"http://arxiv.org/abs/2404.01872v1",target:"_blank",rel:"noopener noreferrer"},an=e("p",null,[e("strong",null,"Abstract"),t(": The effectiveness of Voting Advice Applications (VAA) is often compromised by the length of their questionnaires. To address user fatigue and incomplete responses, some applications (such as the Swiss Smartvote) offer a condensed version of their questionnaire. However, these condensed versions can not ensure the accuracy of recommended parties or candidates, which we show to remain below 40%. To tackle these limitations, this work introduces an adaptive questionnaire approach that selects subsequent questions based on users' previous answers, aiming to enhance recommendation accuracy while reducing the number of questions posed to the voters. Our method uses an encoder and decoder module to predict missing values at any completion stage, leveraging a two-dimensional latent space reflective of political science's traditional methods for visualizing political orientations. Additionally, a selector module is proposed to determine the most informative subsequent question based on the voter's current position in the latent space and the remaining unanswered questions. We validated our approach using the Smartvote dataset from the Swiss Federal elections in 2019, testing various spatial models and selection methods to optimize the system's predictive accuracy. Our findings indicate that employing the IDEAL model both as encoder and decoder, combined with a PosteriorRMSE method for question selection, significantly improves the accuracy of recommendations, achieving 74% accuracy after asking the same number of questions as in the condensed version.")],-1),on=e("h4",{id:"co-speech-gesture-video-generation-via-motion-decoupled-diffusion-model",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#co-speech-gesture-video-generation-via-motion-decoupled-diffusion-model"},[e("span",null,"Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model")])],-1),sn=e("p",null,[e("strong",null,"Authors"),t(": Xu He, Qiaochu Huang, Zhensong Zhang, Zhiwei Lin, Zhiyong Wu, Sicheng Yang, Minglei Li, Zhiyi Chen, Songcen Xu, Xiaofei Wu")],-1),rn=e("strong",null,"Link",-1),ln={href:"http://arxiv.org/abs/2404.01862v1",target:"_blank",rel:"noopener noreferrer"},cn=e("p",null,[e("strong",null,"Abstract"),t(": Co-speech gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-speech gesture videos in this work. There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information. 2) Gestures and speech exhibit inherent dependencies and should be temporally aligned even of arbitrary length. To solve these problems, we present a novel motion-decoupled framework to generate co-speech gesture videos. Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information. Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and speech, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos. For better visual perception, we further design a refinement network focusing on missing details of certain areas. Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations. Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion.")],-1),hn=e("h4",{id:"that-s-not-good-science-an-argument-for-the-thoughtful-use-of-formative-situations-in-research-through-design",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#that-s-not-good-science-an-argument-for-the-thoughtful-use-of-formative-situations-in-research-through-design"},[e("span",null,`"That's Not Good Science!": An Argument for the Thoughtful Use of Formative Situations in Research through Design`)])],-1),dn=e("p",null,[e("strong",null,"Authors"),t(": Raquel B Robinson, Anya Osborne, Chen Ji, James Collin Fey, Ella Dagan, Katherine Isbister")],-1),un=e("strong",null,"Link",-1),pn={href:"http://arxiv.org/abs/2404.01848v1",target:"_blank",rel:"noopener noreferrer"},gn=e("p",null,[e("strong",null,"Abstract"),t(": Most currently accepted approaches to evaluating Research through Design (RtD) presume that design prototypes are finalized and ready for robust testing in laboratory or in-the-wild settings. However, it is also valuable to assess designs at intermediate phases with mid-fidelity prototypes, not just to inform an ongoing design process, but also to glean knowledge of broader use to the research community. We propose 'formative situations' as a frame for examining mid-fidelity prototypes-in-process in this way. We articulate a set of criteria to help the community better assess the rigor of formative situations, in the service of opening conversation about establishing formative situations as a valuable contribution type within the RtD community.")],-1),mn=e("h4",{id:"unmasking-the-nuances-of-loneliness-using-digital-biomarkers-to-understand-social-and-emotional-loneliness-in-college-students",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unmasking-the-nuances-of-loneliness-using-digital-biomarkers-to-understand-social-and-emotional-loneliness-in-college-students"},[e("span",null,"Unmasking the Nuances of Loneliness: Using Digital Biomarkers to Understand Social and Emotional Loneliness in College Students")])],-1),fn=e("p",null,[e("strong",null,"Authors"),t(": Malik Muhammad Qirtas, Evi Zafeirid, Dirk Pesch, Eleanor Bantry White")],-1),vn=e("strong",null,"Link",-1),bn={href:"http://arxiv.org/abs/2404.01845v1",target:"_blank",rel:"noopener noreferrer"},yn=e("p",null,[e("strong",null,"Abstract"),t(": Background: Loneliness among students is increasing across the world, with potential consequences for mental health and academic success. To address this growing problem, accurate methods of detection are needed to identify loneliness and to differentiate social and emotional loneliness so that intervention can be personalized to individual need. Passive sensing technology provides a unique technique to capture behavioral patterns linked with distinct loneliness forms, allowing for more nuanced understanding and interventions for loneliness. Methods: To differentiate between social and emotional loneliness using digital biomarkers, our study included statistical tests, machine learning for predictive modeling, and SHAP values for feature importance analysis, revealing important factors in loneliness classification. Results: Our analysis revealed significant behavioral differences between socially and emotionally lonely groups, particularly in terms of phone usage and location-based features , with machine learning models demonstrating substantial predictive power in classifying loneliness levels. The XGBoost model, in particular, showed high accuracy and was effective in identifying key digital biomarkers, including phone usage duration and location-based features, as significant predictors of loneliness categories. Conclusion: This study underscores the potential of passive sensing data, combined with machine learning techniques, to provide insights into the behavioral manifestations of social and emotional loneliness among students. The identification of key digital biomarkers paves the way for targeted interventions aimed at mitigating loneliness in this population.")],-1),wn=e("h4",{id:"rethinking-annotator-simulation-realistic-evaluation-of-whole-body-pet-lesion-interactive-segmentation-methods",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#rethinking-annotator-simulation-realistic-evaluation-of-whole-body-pet-lesion-interactive-segmentation-methods"},[e("span",null,"Rethinking Annotator Simulation: Realistic Evaluation of Whole-Body PET Lesion Interactive Segmentation Methods")])],-1),_n=e("p",null,[e("strong",null,"Authors"),t(": Zdravko Marinov, Moon Kim, Jens Kleesiek, Rainer Stiefelhagen")],-1),kn=e("strong",null,"Link",-1),xn={href:"http://arxiv.org/abs/2404.01816v1",target:"_blank",rel:"noopener noreferrer"},An=e("p",null,[e("strong",null,"Abstract"),t(": Interactive segmentation plays a crucial role in accelerating the annotation, particularly in domains requiring specialized expertise such as nuclear medicine. For example, annotating lesions in whole-body Positron Emission Tomography (PET) images can require over an hour per volume. While previous works evaluate interactive segmentation models through either real user studies or simulated annotators, both approaches present challenges. Real user studies are expensive and often limited in scale, while simulated annotators, also known as robot users, tend to overestimate model performance due to their idealized nature. To address these limitations, we introduce four evaluation metrics that quantify the user shift between real and simulated annotators. In an initial user study involving four annotators, we assess existing robot users using our proposed metrics and find that robot users significantly deviate in performance and annotation behavior compared to real annotators. Based on these findings, we propose a more realistic robot user that reduces the user shift by incorporating human factors such as click variation and inter-annotator disagreement. We validate our robot user in a second user study, involving four other annotators, and show it consistently reduces the simulated-to-real user shift compared to traditional robot users. By employing our robot user, we can conduct more large-scale and cost-efficient evaluations of interactive segmentation models, while preserving the fidelity of real user studies. Our implementation is based on MONAI Label and will be made publicly available.")],-1),Ln=e("h4",{id:"generative-ai-for-immersive-communication-the-next-frontier-in-internet-of-senses-through-6g",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#generative-ai-for-immersive-communication-the-next-frontier-in-internet-of-senses-through-6g"},[e("span",null,"Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G")])],-1),Tn=e("p",null,[e("strong",null,"Authors"),t(": Nassim Sehad, Lina Bariah, Wassim Hamidouche, Hamed Hellaoui, Riku Jäntti, Mérouane Debbah")],-1),In=e("strong",null,"Link",-1),Mn={href:"http://arxiv.org/abs/2404.01713v1",target:"_blank",rel:"noopener noreferrer"},Sn=e("p",null,[e("strong",null,"Abstract"),t(": Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical applications of generative AI for immersive media while addressing the challenges and outlining future trajectories.")],-1),Cn=e("h4",{id:"tell-and-show-combining-multiple-modalities-to-communicate-manipulation-tasks-to-a-robot",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tell-and-show-combining-multiple-modalities-to-communicate-manipulation-tasks-to-a-robot"},[e("span",null,"Tell and show: Combining multiple modalities to communicate manipulation tasks to a robot")])],-1),zn=e("p",null,[e("strong",null,"Authors"),t(": Petr Vanc, Radoslav Skoviera, Karla Stepanova")],-1),Wn=e("strong",null,"Link",-1),Pn={href:"http://arxiv.org/abs/2404.01702v1",target:"_blank",rel:"noopener noreferrer"},Rn=e("p",null,[e("strong",null,"Abstract"),t(": As human-robot collaboration is becoming more widespread, there is a need for a more natural way of communicating with the robot. This includes combining data from several modalities together with the context of the situation and background knowledge. Current approaches to communication typically rely only on a single modality or are often very rigid and not robust to missing, misaligned, or noisy data. In this paper, we propose a novel method that takes inspiration from sensor fusion approaches to combine uncertain information from multiple modalities and enhance it with situational awareness (e.g., considering object properties or the scene setup). We first evaluate the proposed solution on simulated bimodal datasets (gestures and language) and show by several ablation experiments the importance of various components of the system and its robustness to noisy, missing, or misaligned observations. Then we implement and evaluate the model on the real setup. In human-robot interaction, we must also consider whether the selected action is probable enough to be executed or if we should better query humans for clarification. For these purposes, we enhance our model with adaptive entropy-based thresholding that detects the appropriate thresholds for different types of interaction showing similar performance as fine-tuned fixed thresholds.")],-1),Hn=e("h4",{id:"nlp-systems-that-can-t-tell-use-from-mention-censor-counterspeech-but-teaching-the-distinction-helps",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#nlp-systems-that-can-t-tell-use-from-mention-censor-counterspeech-but-teaching-the-distinction-helps"},[e("span",null,"NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps")])],-1),En=e("p",null,[e("strong",null,"Authors"),t(": Kristina Gligoric, Myra Cheng, Lucia Zheng, Esin Durmus, Dan Jurafsky")],-1),Dn=e("strong",null,"Link",-1),qn={href:"http://arxiv.org/abs/2404.01651v1",target:"_blank",rel:"noopener noreferrer"},Bn=e("p",null,[e("strong",null,"Abstract"),t(": The use of words to convey speaker's intent is traditionally distinguished from the `mention' of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech. We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers ways to address it.")],-1),Gn=e("h4",{id:"insightlens-discovering-and-exploring-insights-from-conversational-contexts-in-large-language-model-powered-data-analysis",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#insightlens-discovering-and-exploring-insights-from-conversational-contexts-in-large-language-model-powered-data-analysis"},[e("span",null,"InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis")])],-1),Nn=e("p",null,[e("strong",null,"Authors"),t(": Luoxuan Weng, Xingbo Wang, Junyu Lu, Yingchaojie Feng, Yihan Liu, Wei Chen")],-1),Vn=e("strong",null,"Link",-1),On={href:"http://arxiv.org/abs/2404.01644v1",target:"_blank",rel:"noopener noreferrer"},jn=e("p",null,[e("strong",null,"Abstract"),t(": The proliferation of large language models (LLMs) has revolutionized the capabilities of natural language interfaces (NLIs) for data analysis. LLMs can perform multi-step and complex reasoning to generate data insights based on users' analytic intents. However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and natural language explanations. This hinders efficient identification, verification, and interpretation of insights within the current chat-based interfaces of LLMs. In this paper, we first conduct a formative study with eight experienced data analysts to understand their general workflow and pain points during LLM-powered data analysis. Then, we propose an LLM-based multi-agent framework to automatically extract, associate, and organize insights along with the analysis process. Based on this, we introduce InsightLens, an interactive system that visualizes the intricate conversational contexts from multiple aspects to facilitate insight discovery and exploration. A user study with twelve data analysts demonstrates the effectiveness of InsightLens, showing that it significantly reduces users' manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience.")],-1),Fn=e("h4",{id:"gen4ds-workshop-on-data-storytelling-in-an-era-of-generative-ai",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#gen4ds-workshop-on-data-storytelling-in-an-era-of-generative-ai"},[e("span",null,"Gen4DS: Workshop on Data Storytelling in an Era of Generative AI")])],-1),Jn=e("p",null,[e("strong",null,"Authors"),t(": Xingyu Lan, Leni Yang, Zezhong Wang, Danqing Shi, Sheelagh Carpendale")],-1),Un=e("strong",null,"Link",-1),Kn={href:"http://arxiv.org/abs/2404.01622v1",target:"_blank",rel:"noopener noreferrer"},Zn=e("p",null,[e("strong",null,"Abstract"),t(": Storytelling is an ancient and precious human ability that has been rejuvenated in the digital age. Over the last decade, there has been a notable surge in the recognition and application of data storytelling, both in academia and industry. Recently, the rapid development of generative AI has brought new opportunities and challenges to this field, sparking numerous new questions. These questions may not necessarily be quickly transformed into papers, but we believe it is necessary to promptly discuss them to help the community better clarify important issues and research agendas for the future. We thus invite you to join our workshop (Gen4DS) to discuss questions such as: How can generative AI facilitate the creation of data stories? How might generative AI alter the workflow of data storytellers? What are the pitfalls and risks of incorporating AI in storytelling? We have designed both paper presentations and interactive activities (including hands-on creation, group discussion pods, and debates on controversial issues) for the workshop. We hope that participants will learn about the latest advances and pioneering work in data storytelling, engage in critical conversations with each other, and have an enjoyable, unforgettable, and meaningful experience at the event.")],-1),Yn=e("h4",{id:"collaborative-human-ai-trust-chai-t-a-process-framework-for-active-management-of-trust-in-human-ai-collaboration",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#collaborative-human-ai-trust-chai-t-a-process-framework-for-active-management-of-trust-in-human-ai-collaboration"},[e("span",null,"Collaborative human-AI trust (CHAI-T): A process framework for active management of trust in human-AI collaboration")])],-1),Xn=e("p",null,[e("strong",null,"Authors"),t(": Melanie J. McGrath, Andreas Duenser, Justine Lacey, Cecile Paris")],-1),Qn=e("strong",null,"Link",-1),$n={href:"http://arxiv.org/abs/2404.01615v1",target:"_blank",rel:"noopener noreferrer"},ei=e("p",null,[e("strong",null,"Abstract"),t(": Collaborative human-AI (HAI) teaming combines the unique skills and capabilities of humans and machines in sustained teaming interactions leveraging the strengths of each. In tasks involving regular exposure to novelty and uncertainty, collaboration between adaptive, creative humans and powerful, precise artificial intelligence (AI) promises new solutions and efficiencies. User trust is essential to creating and maintaining these collaborative relationships. Established models of trust in traditional forms of AI typically recognize the contribution of three primary categories of trust antecedents: characteristics of the human user, characteristics of the technology, and environmental factors. The emergence of HAI teams, however, requires an understanding of human trust that accounts for the specificity of task contexts and goals, integrates processes of interaction, and captures how trust evolves in a teaming environment over time. Drawing on both the psychological and computer science literature, the process framework of trust in collaborative HAI teams (CHAI-T) presented in this paper adopts the tripartite structure of antecedents established by earlier models, while incorporating team processes and performance phases to capture the dynamism inherent to trust in teaming contexts. These features enable active management of trust in collaborative AI systems, with practical implications for the design and deployment of collaborative HAI teams.")],-1),ti=e("h4",{id:"helmsman-of-the-masses-evaluate-the-opinion-leadership-of-large-language-models-in-the-werewolf-game",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#helmsman-of-the-masses-evaluate-the-opinion-leadership-of-large-language-models-in-the-werewolf-game"},[e("span",null,"Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game")])],-1),ni=e("p",null,[e("strong",null,"Authors"),t(": Silin Du, Xiaowei Zhang")],-1),ii=e("strong",null,"Link",-1),ai={href:"http://arxiv.org/abs/2404.01602v1",target:"_blank",rel:"noopener noreferrer"},oi=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players' decisions. We conduct extensive experiments to evaluate LLMs of different scales. In addition, we collect a Werewolf question-answering dataset (WWQA) to assess and enhance LLM's grasp of the game rules, and we also incorporate human participants for further analysis. The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of LLMs and few LLMs possess the capacity for opinion leadership.")],-1),si=e("h4",{id:"leveraging-digital-perceptual-technologies-for-remote-perception-and-analysis-of-human-biomechanical-processes-a-contactless-approach-for-workload-and-joint-force-assessment",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#leveraging-digital-perceptual-technologies-for-remote-perception-and-analysis-of-human-biomechanical-processes-a-contactless-approach-for-workload-and-joint-force-assessment"},[e("span",null,"Leveraging Digital Perceptual Technologies for Remote Perception and Analysis of Human Biomechanical Processes: A Contactless Approach for Workload and Joint Force Assessment")])],-1),ri=e("p",null,[e("strong",null,"Authors"),t(": Jesudara Omidokun, Darlington Egeonu, Bochen Jia, Liang Yang")],-1),li=e("strong",null,"Link",-1),ci={href:"http://arxiv.org/abs/2404.01576v1",target:"_blank",rel:"noopener noreferrer"},hi=e("p",null,[e("strong",null,"Abstract"),t(": This study presents an innovative computer vision framework designed to analyze human movements in industrial settings, aiming to enhance biomechanical analysis by integrating seamlessly with existing software. Through a combination of advanced imaging and modeling techniques, the framework allows for comprehensive scrutiny of human motion, providing valuable insights into kinematic patterns and kinetic data. Utilizing Convolutional Neural Networks (CNNs), Direct Linear Transform (DLT), and Long Short-Term Memory (LSTM) networks, the methodology accurately detects key body points, reconstructs 3D landmarks, and generates detailed 3D body meshes. Extensive evaluations across various movements validate the framework's effectiveness, demonstrating comparable results to traditional marker-based models with minor differences in joint angle estimations and precise estimations of weight and height. Statistical analyses consistently support the framework's reliability, with joint angle estimations showing less than a 5-degree difference for hip flexion, elbow flexion, and knee angle methods. Additionally, weight estimation exhibits an average error of less than 6 % for weight and less than 2 % for height when compared to ground-truth values from 10 subjects. The integration of the Biomech-57 landmark skeleton template further enhances the robustness and reinforces the framework's credibility. This framework shows significant promise for meticulous biomechanical analysis in industrial contexts, eliminating the need for cumbersome markers and extending its utility to diverse research domains, including the study of specific exoskeleton devices' impact on facilitating the prompt return of injured workers to their tasks.")],-1),di=e("h2",{id:"_2024-04-01",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-01"},[e("span",null,"2024-04-01")])],-1),ui=e("h4",{id:"playfutures-imagining-civic-futures-with-ai-and-puppets",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#playfutures-imagining-civic-futures-with-ai-and-puppets"},[e("span",null,"PlayFutures: Imagining Civic Futures with AI and Puppets")])],-1),pi=e("p",null,[e("strong",null,"Authors"),t(": Supratim Pait, Sumita Sharma, Ashley Frith, Michael Nitsche, Noura Howell")],-1),gi=e("strong",null,"Link",-1),mi={href:"http://arxiv.org/abs/2404.01527v1",target:"_blank",rel:"noopener noreferrer"},fi=e("p",null,[e("strong",null,"Abstract"),t(": Children are the builders of the future and crucial to how the technologies around us develop. They are not voters but are participants in how the public spaces in a city are used. Through a workshop designed around kids of age 9-12, we investigate if novel technologies like artificial intelligence can be integrated in existing ways of play and performance to 1) re-imagine the future of civic spaces, 2) reflect on these novel technologies in the process and 3) build ways of civic engagement through play. We do this using a blend AI image generation and Puppet making to ultimately build future scenarios, perform debate and discussion around the futures and reflect on AI, its role and potential in their process. We present our findings of how AI helped envision these futures, aid performances, and report some initial reflections from children about the technology.")],-1),vi=e("h4",{id:"delve-into-earth-s-past-a-visualization-based-exhibit-deployed-across-multiple-museum-contexts",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#delve-into-earth-s-past-a-visualization-based-exhibit-deployed-across-multiple-museum-contexts"},[e("span",null,"DeLVE into Earth's Past: A Visualization-Based Exhibit Deployed Across Multiple Museum Contexts")])],-1),bi=e("p",null,[e("strong",null,"Authors"),t(": Mara Solen, Nigar Sultana, Laura Lukes, Tamara Munzner")],-1),yi=e("strong",null,"Link",-1),wi={href:"http://arxiv.org/abs/2404.01488v1",target:"_blank",rel:"noopener noreferrer"},_i=e("p",null,[e("strong",null,"Abstract"),t(": While previous work has found success in deploying visualizations as museum exhibits, differences in visitor behaviour across varying museum contexts are understudied. We present an interactive Deep-time Literacy Visualization Exhibit (DeLVE) to help museum visitors understand deep time (lengths of extremely long geological processes) by improving proportional reasoning skills through comparison of different time periods. DeLVE uses a new visualization idiom, Connected Multi-Tier Ranges, to visualize curated datasets of past events across multiple scales of time, relating extreme scales with concrete scales that have more familiar magnitudes and units. Museum staff at three separate museums approved the deployment of DeLVE as a digital kiosk, and devoted time to curating a unique dataset in each of them. We collect data from two sources, an observational study and system trace logs, yielding evidence of successfully meeting our requirements. We discuss the importance of context: similar museum exhibits in different contexts were received very differently by visitors. We additionally discuss differences in our process from standard design study methodology which is focused on design studies for data analysis purposes, rather than for presentation. Supplemental materials are available at: https://osf.io/z53dq/?view_only=4df33aad207144aca149982412125541")],-1),ki=e("h4",{id:"a-design-space-for-visualization-with-large-scale-item-ratios",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-design-space-for-visualization-with-large-scale-item-ratios"},[e("span",null,"A Design Space for Visualization with Large Scale-Item Ratios")])],-1),xi=e("p",null,[e("strong",null,"Authors"),t(": Mara Solen, Tamara Munzner")],-1),Ai=e("strong",null,"Link",-1),Li={href:"http://arxiv.org/abs/2404.01485v1",target:"_blank",rel:"noopener noreferrer"},Ti=e("p",null,[e("strong",null,"Abstract"),t(": The scale-item ratio is the relationship between the largest scale and the smallest item in a visualization. Designing visualizations when this ratio is large can be challenging, and designers have developed many approaches to overcome this challenge. We present a design space for visualization with large scale-item ratios. The design space includes three dimensions, with eight total subdimensions. We demonstrate its descriptive power by using it to code approaches from a corpus we compiled of 54 examples, created by a mix of academics and practitioners. We then partition these examples into five strategies, which are shared approaches with respect to design space dimension choices. We demonstrate generative power by analyzing missed opportunities within the corpus of examples, identified through analysis of the design space, where we note how certain examples could have benefited from different choices. Supplemental materials: https://osf.io/wbrdm/?view_only=04389a2101a04e71a2c208a93bf2f7f2")],-1),Ii=e("h4",{id:"will-the-real-linda-please-stand-up-to-large-language-models-examining-the-representativeness-heuristic-in-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#will-the-real-linda-please-stand-up-to-large-language-models-examining-the-representativeness-heuristic-in-llms"},[e("span",null,"Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs")])],-1),Mi=e("p",null,[e("strong",null,"Authors"),t(": Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald")],-1),Si=e("strong",null,"Link",-1),Ci={href:"http://arxiv.org/abs/2404.01461v1",target:"_blank",rel:"noopener noreferrer"},zi=e("p",null,[e("strong",null,"Abstract"),t(": Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model of using its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while failing in a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.")],-1),Wi=e("h4",{id:"a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs"},[e("span",null,"A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs")])],-1),Pi=e("p",null,[e("strong",null,"Authors"),t(": Harry Li, Gabriel Appleby, Ashley Suh")],-1),Ri=e("strong",null,"Link",-1),Hi={href:"http://arxiv.org/abs/2404.01425v1",target:"_blank",rel:"noopener noreferrer"},Ei=e("p",null,[e("strong",null,"Abstract"),t(": We present a mixed-methods study to explore how large language models (LLMs) can assist users in the visual exploration and analysis of knowledge graphs (KGs). We surveyed and interviewed 20 professionals from industry, government laboratories, and academia who regularly work with KGs and LLMs, either collaboratively or concurrently. Our findings show that participants overwhelmingly want an LLM to facilitate data retrieval from KGs through joint query construction, to identify interesting relationships in the KG through multi-turn conversation, and to create on-demand visualizations from the KG that enhance their trust in the LLM's outputs. To interact with an LLM, participants strongly prefer a chat-based 'widget,' built on top of their regular analysis workflows, with the ability to guide the LLM using their interactions with a visualization. When viewing an LLM's outputs, participants similarly prefer a combination of annotated visuals (e.g., subgraphs or tables extracted from the KG) alongside summarizing text. However, participants also expressed concerns about an LLM's ability to maintain semantic intent when translating natural language questions into KG queries, the risk of an LLM 'hallucinating' false data from the KG, and the difficulties of engineering a 'perfect prompt.' From the analysis of our interviews, we contribute a preliminary roadmap for the design of LLM-driven knowledge graph exploration systems and outline future opportunities in this emergent design space.")],-1),Di=e("h4",{id:"towards-a-potential-paradigm-shift-in-health-data-collection-and-analysis",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-a-potential-paradigm-shift-in-health-data-collection-and-analysis"},[e("span",null,"Towards a potential paradigm shift in health data collection and analysis")])],-1),qi=e("p",null,[e("strong",null,"Authors"),t(": David Josef Herzog, Nitsa Judith Herzog")],-1),Bi=e("strong",null,"Link",-1),Gi={href:"http://arxiv.org/abs/2404.01403v1",target:"_blank",rel:"noopener noreferrer"},Ni=e("p",null,[e("strong",null,"Abstract"),t(": Industrial Revolution 4.0 transforms healthcare systems. The first three technological revolutions changed the relationship between human and machine interaction due to the exponential growth of machine numbers. The fourth revolution put humans into a situation where heterogeneous data is produced with unmatched quantity and quality not only by traditional methods, enforced by digitization, but also by ubiquitous computing, machine-to-machine interactions and smart environment. The modern cyber-physical space underlines the role of the person in the expanding context of computerization and big data processing. In healthcare, where data collection and analysis particularly depend on human efforts, the disruptive nature of these developments is evident. Adaptation to this process requires deep scrutiny of the trends and recognition of future medical data technologies` evolution. Significant difficulties arise from discrepancies in requirements by healthcare, administrative and technology stakeholders. Black box and grey box decisions made in medical imaging and diagnostic Decision Support Software are often not transparent enough for the professional, social and medico-legal requirements. While Explainable AI proposes a partial solution for AI applications in medicine, the approach has to be wider and multiplex. LLM potential and limitations are also discussed. This paper lists the most significant issues in these topics and describes possible solutions.")],-1),Vi=e("h4",{id:"evaluating-privacy-perceptions-experience-and-behavior-of-software-development-teams",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evaluating-privacy-perceptions-experience-and-behavior-of-software-development-teams"},[e("span",null,"Evaluating Privacy Perceptions, Experience, and Behavior of Software Development Teams")])],-1),Oi=e("p",null,[e("strong",null,"Authors"),t(": Maxwell Prybylo, Sara Haghighi, Sai Teja Peddinti, Sepideh Ghanavati")],-1),ji=e("strong",null,"Link",-1),Fi={href:"http://arxiv.org/abs/2404.01283v1",target:"_blank",rel:"noopener noreferrer"},Ji=e("p",null,[e("strong",null,"Abstract"),t(": With the increase in the number of privacy regulations, small development teams are forced to make privacy decisions on their own. In this paper, we conduct a mixed-method survey study, including statistical and qualitative analysis, to evaluate the privacy perceptions, practices, and knowledge of members involved in various phases of software development (SDLC). Our survey includes 362 participants from 23 countries, encompassing roles such as product managers, developers, and testers. Our results show diverse definitions of privacy across SDLC roles, emphasizing the need for a holistic privacy approach throughout SDLC. We find that software teams, regardless of their region, are less familiar with privacy concepts (such as anonymization), relying on self-teaching and forums. Most participants are more familiar with GDPR and HIPAA than other regulations, with multi-jurisdictional compliance being their primary concern. Our results advocate the need for role-dependent solutions to address the privacy challenges, and we highlight research directions and educational takeaways to help improve privacy-aware software development.")],-1),Ui=e("h4",{id:"information-plane-analysis-visualization-in-deep-learning-via-transfer-entropy",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#information-plane-analysis-visualization-in-deep-learning-via-transfer-entropy"},[e("span",null,"Information Plane Analysis Visualization in Deep Learning via Transfer Entropy")])],-1),Ki=e("p",null,[e("strong",null,"Authors"),t(": Adrian Moldovan, Angel Cataron, Razvan Andonie")],-1),Zi=e("strong",null,"Link",-1),Yi={href:"http://arxiv.org/abs/2404.01364v1",target:"_blank",rel:"noopener noreferrer"},Xi=e("p",null,[e("strong",null,"Abstract"),t(": In a feedforward network, Transfer Entropy (TE) can be used to measure the influence that one layer has on another by quantifying the information transfer between them during training. According to the Information Bottleneck principle, a neural model's internal representation should compress the input data as much as possible while still retaining sufficient information about the output. Information Plane analysis is a visualization technique used to understand the trade-off between compression and information preservation in the context of the Information Bottleneck method by plotting the amount of information in the input data against the compressed representation. The claim that there is a causal link between information-theoretic compression and generalization, measured by mutual information, is plausible, but results from different studies are conflicting. In contrast to mutual information, TE can capture temporal relationships between variables. To explore such links, in our novel approach we use TE to quantify information transfer between neural layers and perform Information Plane analysis. We obtained encouraging experimental results, opening the possibility for further investigations.")],-1),Qi=e("h4",{id:"image-reconstruction-from-electroencephalography-using-latent-diffusion",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#image-reconstruction-from-electroencephalography-using-latent-diffusion"},[e("span",null,"Image Reconstruction from Electroencephalography Using Latent Diffusion")])],-1),$i=e("p",null,[e("strong",null,"Authors"),t(": Teng Fei, Virginia de Sa")],-1),ea=e("strong",null,"Link",-1),ta={href:"http://arxiv.org/abs/2404.01250v1",target:"_blank",rel:"noopener noreferrer"},na=e("p",null,[e("strong",null,"Abstract"),t(": In this work, we have adopted the diffusion-based image reconstruction pipeline previously used for fMRI image reconstruction and applied it to Electroencephalography (EEG). The EEG encoding method is very simple, and forms a baseline from which more sophisticated EEG encoding methods can be compared. We have also evaluated the fidelity of the generated image using the same metrics used in the previous functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) works. Our results show that while the reconstruction from EEG recorded to rapidly presented images is not as good as reconstructions from fMRI to slower presented images, it holds a surprising amount of information that could be applied in specific use cases. Also, EEG-based image reconstruction works better in some categories-such as land animals and food-than others, shedding new light on previous findings of EEG's sensitivity to those categories and revealing potential for these methods to further understand EEG responses to human visual coding. More investigation should use longer-duration image stimulations to elucidate the later components that might be salient to the different image categories.")],-1),ia=e("h4",{id:"aurora-navigating-ui-tarpits-via-automated-neural-screen-understanding",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#aurora-navigating-ui-tarpits-via-automated-neural-screen-understanding"},[e("span",null,"AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding")])],-1),aa=e("p",null,[e("strong",null,"Authors"),t(": Safwat Ali Khan, Wenyu Wang, Yiran Ren, Bin Zhu, Jiangfan Shi, Alyssa McGowan, Wing Lam, Kevin Moran")],-1),oa=e("strong",null,"Link",-1),sa={href:"http://arxiv.org/abs/2404.01240v1",target:"_blank",rel:"noopener noreferrer"},ra=e("p",null,[e("strong",null,"Abstract"),t(": Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform. Much of this work has come in the form of Automated Input Generation tools (AIG tools) that dynamically explore app screens. However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage - particularly on sophisticated proprietary apps. Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate. In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding. We introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect common screen designs and navigate them accordingly. The key idea of AURORA is that there are a finite number of mobile app screen designs, albeit with subtle variations, such that the general patterns of different categories of UI designs can be learned. As such, AURORA employs a multi-modal, neural screen classifier that is able to recognize the most common types of UI screen designs. After recognizing a given screen, it then applies a set of flexible and generalizable heuristics to properly navigate the screen. We evaluated AURORA both on a set of 12 apps with known tarpits from prior work, and on a new set of five of the most popular apps from the Google Play store. Our results indicate that AURORA is able to effectively navigate tarpit screens, outperforming prior approaches that avoid tarpits by 19.6% in terms of method coverage. The improvements can be attributed to AURORA's UI design classification and heuristic navigation techniques.")],-1),la=e("h4",{id:"llm-attributor-interactive-visual-attribution-for-llm-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-attributor-interactive-visual-attribution-for-llm-generation"},[e("span",null,"LLM Attributor: Interactive Visual Attribution for LLM Generation")])],-1),ca=e("p",null,[e("strong",null,"Authors"),t(": Seongmin Lee, Zijie J. Wang, Aishwarya Chakravarthy, Alec Helbling, ShengYun Peng, Mansi Phute, Duen Horng Chau, Minsuk Kahng")],-1),ha=e("strong",null,"Link",-1),da={href:"http://arxiv.org/abs/2404.01361v1",target:"_blank",rel:"noopener noreferrer"},ua=e("p",null,[e("strong",null,"Abstract"),t(": While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.")],-1),pa=e("h4",{id:"chat-modeling-natural-language-based-procedural-modeling-of-biological-structures-without-training",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#chat-modeling-natural-language-based-procedural-modeling-of-biological-structures-without-training"},[e("span",null,"Chat Modeling: Natural Language-based Procedural Modeling of Biological Structures without Training")])],-1),ga=e("p",null,[e("strong",null,"Authors"),t(": Donggang Jia, Yunhai Wang, Ivan Viola")],-1),ma=e("strong",null,"Link",-1),fa={href:"http://arxiv.org/abs/2404.01063v1",target:"_blank",rel:"noopener noreferrer"},va=e("p",null,[e("strong",null,"Abstract"),t(": 3D modeling of biological structures is an inherently complex process, necessitating both biological and geometric understanding. Additionally, the complexity of user interfaces of 3D modeling tools and the associated steep learning curve further exacerbate the difficulty of authoring a 3D model. In this paper, we introduce a novel framework to address the challenge of using 3D modeling software by converting users' textual inputs into modeling actions within an interactive procedural modeling system. The framework incorporates a code generator of a novel code format and a corresponding code interpreter. The major technical innovation includes the user-refinement mechanism that captures the degree of user dissatisfaction with the modeling outcome, offers an interactive revision, and leverages this feedback for future improved 3D modeling. This entire framework is powered by large language models and eliminates the need for a traditional training process. We develop a prototype tool named Chat Modeling, offering both automatic and step-by-step 3D modeling approaches. Our evaluation of the framework with structural biologists highlights the potential of our approach being utilized in their scientific workflows. All supplemental materials are available at https://osf.io/x4qb7/.")],-1),ba=e("h4",{id:"drag-your-noise-interactive-point-based-editing-via-diffusion-semantic-propagation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#drag-your-noise-interactive-point-based-editing-via-diffusion-semantic-propagation"},[e("span",null,"Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation")])],-1),ya=e("p",null,[e("strong",null,"Authors"),t(": Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, Shengfeng He")],-1),wa=e("strong",null,"Link",-1),_a={href:"http://arxiv.org/abs/2404.01050v1",target:"_blank",rel:"noopener noreferrer"},ka=e("p",null,[e("strong",null,"Abstract"),t(": Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion semantics in a single denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing. Comparative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion. Our codes are available at https://github.com/haofengl/DragNoise.")],-1),xa=e("h4",{id:"how-can-large-language-models-enable-better-socially-assistive-human-robot-interaction-a-brief-survey",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#how-can-large-language-models-enable-better-socially-assistive-human-robot-interaction-a-brief-survey"},[e("span",null,"How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey")])],-1),Aa=e("p",null,[e("strong",null,"Authors"),t(": Zhonghao Shi, Ellen Landrum, Amy O' Connell, Mina Kian, Leticia Pinto-Alva, Kaleen Shrestha, Xiaoyuan Zhu, Maja J Matarić")],-1),La=e("strong",null,"Link",-1),Ta={href:"http://arxiv.org/abs/2404.00938v1",target:"_blank",rel:"noopener noreferrer"},Ia=e("p",null,[e("strong",null,"Abstract"),t(": Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that have not yet been encountered, and must be carefully be addressed to safely deploy these more advanced systems. In this work, we aim to conduct a brief survey on the use of LLMs in SAR technologies, and discuss the potentials and risks of applying LLMs to the following three major technical challenges of SAR: 1) natural language dialog; 2) multimodal understanding; 3) LLMs as robot policies.")],-1),Ma=e("h2",{id:"_2024-03-31",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-31"},[e("span",null,"2024-03-31")])],-1),Sa=e("h4",{id:"designing-human-ai-systems-anthropomorphism-and-framing-bias-on-human-ai-collaboration",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#designing-human-ai-systems-anthropomorphism-and-framing-bias-on-human-ai-collaboration"},[e("span",null,"Designing Human-AI Systems: Anthropomorphism and Framing Bias on Human-AI Collaboration")])],-1),Ca=e("p",null,[e("strong",null,"Authors"),t(": Samuel Aleksander Sánchez Olszewski")],-1),za=e("strong",null,"Link",-1),Wa={href:"http://arxiv.org/abs/2404.00634v1",target:"_blank",rel:"noopener noreferrer"},Pa=e("p",null,[e("strong",null,"Abstract"),t(": AI is redefining how humans interact with technology, leading to a synergetic collaboration between the two. Nevertheless, the effects of human cognition on this collaboration remain unclear. This study investigates the implications of two cognitive biases, anthropomorphism and framing effect, on human-AI collaboration within a hiring setting. Subjects were asked to select job candidates with the help of an AI-powered recommendation tool. The tool was manipulated to have either human-like or robot-like characteristics and presented its recommendations in either positive or negative frames. The results revealed that the framing of AI's recommendations had no significant influence on subjects' decisions. In contrast, anthropomorphism significantly affected subjects' agreement with AI recommendations. Contrary to expectations, subjects were less likely to agree with the AI if it had human-like characteristics. These findings demonstrate that cognitive biases can impact human-AI collaboration and highlight the need for tailored approaches to AI product design, rather than a single, universal solution.")],-1),Ra=e("h4",{id:"my-agent-understands-me-better-integrating-dynamic-human-like-memory-recall-and-consolidation-in-llm-based-agents",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#my-agent-understands-me-better-integrating-dynamic-human-like-memory-recall-and-consolidation-in-llm-based-agents"},[e("span",null,'"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents')])],-1),Ha=e("p",null,[e("strong",null,"Authors"),t(": Yuki Hou, Haruki Tamoto, Homei Miyashita")],-1),Ea=e("strong",null,"Link",-1),Da={href:"http://arxiv.org/abs/2404.00573v1",target:"_blank",rel:"noopener noreferrer"},qa=e("p",null,[e("strong",null,"Abstract"),t(": In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of large language model based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of LLMs. We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user's interaction history in a database that encapsulates each memory's content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences.")],-1),Ba=e("h4",{id:"the-emotional-impact-of-game-duration-a-framework-for-understanding-player-emotions-in-extended-gameplay-sessions",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-emotional-impact-of-game-duration-a-framework-for-understanding-player-emotions-in-extended-gameplay-sessions"},[e("span",null,"The Emotional Impact of Game Duration: A Framework for Understanding Player Emotions in Extended Gameplay Sessions")])],-1),Ga=e("p",null,[e("strong",null,"Authors"),t(": Anoop Kumar, Suresh Dodda, Navin Kamuni, Venkata Sai Mahesh Vuppalapati")],-1),Na=e("strong",null,"Link",-1),Va={href:"http://arxiv.org/abs/2404.00526v1",target:"_blank",rel:"noopener noreferrer"},Oa=e("p",null,[e("strong",null,"Abstract"),t(": Video games have played a crucial role in entertainment since their development in the 1970s, becoming even more prominent during the lockdown period when people were looking for ways to entertain them. However, at that time, players were unaware of the significant impact that playtime could have on their feelings. This has made it challenging for designers and developers to create new games since they have to control the emotional impact that these games will take on players. Thus, the purpose of this study is to look at how a player's emotions are affected by the duration of the game. In order to achieve this goal, a framework for emotion detection is created. According to the experiment's results, the volunteers' general ability to express emotions increased from 20 to 60 minutes. In comparison to shorter gameplay sessions, the experiment found that extended gameplay sessions did significantly affect the player's emotions. According to the results, it was recommended that in order to lessen the potential emotional impact that playing computer and video games may have in the future, game producers should think about creating shorter, entertaining games.")],-1),ja=e("h4",{id:"humane-speech-synthesis-through-zero-shot-emotion-and-disfluency-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#humane-speech-synthesis-through-zero-shot-emotion-and-disfluency-generation"},[e("span",null,"Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation")])],-1),Fa=e("p",null,[e("strong",null,"Authors"),t(": Rohan Chaudhury, Mihir Godbole, Aakash Garg, Jinsil Hwaryoung Seo")],-1),Ja=e("strong",null,"Link",-1),Ua={href:"http://arxiv.org/abs/2404.01339v1",target:"_blank",rel:"noopener noreferrer"},Ka=e("p",null,[e("strong",null,"Abstract"),t(": Contemporary conversational systems often present a significant limitation: their responses lack the emotional depth and disfluent characteristic of human interactions. This absence becomes particularly noticeable when users seek more personalized and empathetic interactions. Consequently, this makes them seem mechanical and less relatable to human users. Recognizing this gap, we embarked on a journey to humanize machine communication, to ensure AI systems not only comprehend but also resonate. To address this shortcoming, we have designed an innovative speech synthesis pipeline. Within this framework, a cutting-edge language model introduces both human-like emotion and disfluencies in a zero-shot setting. These intricacies are seamlessly integrated into the generated text by the language model during text generation, allowing the system to mirror human speech patterns better, promoting more intuitive and natural user interactions. These generated elements are then adeptly transformed into corresponding speech patterns and emotive sounds using a rule-based approach during the text-to-speech phase. Based on our experiments, our novel system produces synthesized speech that's almost indistinguishable from genuine human communication, making each interaction feel more personal and authentic.")],-1),Za=e("h2",{id:"_2024-03-30",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-30"},[e("span",null,"2024-03-30")])],-1),Ya=e("h4",{id:"contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app"},[e("span",null,"Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App")])],-1),Xa=e("p",null,[e("strong",null,"Authors"),t(": Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Eunsol Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Colin Depp, Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell")],-1),Qa=e("strong",null,"Link",-1),$a={href:"http://arxiv.org/abs/2404.00487v1",target:"_blank",rel:"noopener noreferrer"},eo=e("p",null,[e("strong",null,"Abstract"),t(": MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.")],-1),to=e("h4",{id:"interactive-multi-robot-flocking-with-gesture-responsiveness-and-musical-accompaniment",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#interactive-multi-robot-flocking-with-gesture-responsiveness-and-musical-accompaniment"},[e("span",null,"Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical Accompaniment")])],-1),no=e("p",null,[e("strong",null,"Authors"),t(": Catie Cuan, Kyle Jeffrey, Kim Kleiven, Adrian Li, Emre Fisher, Matt Harrison, Benjie Holson, Allison Okamura, Matt Bennice")],-1),io=e("strong",null,"Link",-1),ao={href:"http://arxiv.org/abs/2404.00442v1",target:"_blank",rel:"noopener noreferrer"},oo=e("p",null,[e("strong",null,"Abstract"),t(": For decades, robotics researchers have pursued various tasks for multi-robot systems, from cooperative manipulation to search and rescue. These tasks are multi-robot extensions of classical robotic tasks and often optimized on dimensions such as speed or efficiency. As robots transition from commercial and research settings into everyday environments, social task aims such as engagement or entertainment become increasingly relevant. This work presents a compelling multi-robot task, in which the main aim is to enthrall and interest. In this task, the goal is for a human to be drawn to move alongside and participate in a dynamic, expressive robot flock. Towards this aim, the research team created algorithms for robot movements and engaging interaction modes such as gestures and sound. The contributions are as follows: (1) a novel group navigation algorithm involving human and robot agents, (2) a gesture responsive algorithm for real-time, human-robot flocking interaction, (3) a weight mode characterization system for modifying flocking behavior, and (4) a method of encoding a choreographer's preferences inside a dynamic, adaptive, learned system. An experiment was performed to understand individual human behavior while interacting with the flock under three conditions: weight modes selected by a human choreographer, a learned model, or subset list. Results from the experiment showed that the perception of the experience was not influenced by the weight mode selection. This work elucidates how differing task aims such as engagement manifest in multi-robot system design and execution, and broadens the domain of multi-robot tasks.")],-1),so=e("h4",{id:"visualizing-routes-with-ai-discovered-street-view-patterns",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#visualizing-routes-with-ai-discovered-street-view-patterns"},[e("span",null,"Visualizing Routes with AI-Discovered Street-View Patterns")])],-1),ro=e("p",null,[e("strong",null,"Authors"),t(": Tsung Heng Wu, Md Amiruzzaman, Ye Zhao, Deepshikha Bhati, Jing Yang")],-1),lo=e("strong",null,"Link",-1),co={href:"http://arxiv.org/abs/2404.00431v1",target:"_blank",rel:"noopener noreferrer"},ho=e("p",null,[e("strong",null,"Abstract"),t(": Street-level visual appearances play an important role in studying social systems, such as understanding the built environment, driving routes, and associated social and economic factors. It has not been integrated into a typical geographical visualization interface (e.g., map services) for planning driving routes. In this paper, we study this new visualization task with several new contributions. First, we experiment with a set of AI techniques and propose a solution of using semantic latent vectors for quantifying visual appearance features. Second, we calculate image similarities among a large set of street-view images and then discover spatial imagery patterns. Third, we integrate these discovered patterns into driving route planners with new visualization techniques. Finally, we present VivaRoutes, an interactive visualization prototype, to show how visualizations leveraged with these discovered patterns can help users effectively and interactively explore multiple routes. Furthermore, we conducted a user study to assess the usefulness and utility of VivaRoutes.")],-1),uo=e("h4",{id:"a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration"},[e("span",null,"A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration")])],-1),po=e("p",null,[e("strong",null,"Authors"),t(": Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, Thomas W. Malone")],-1),go=e("strong",null,"Link",-1),mo={href:"http://arxiv.org/abs/2404.00405v1",target:"_blank",rel:"noopener noreferrer"},fo=e("p",null,[e("strong",null,"Abstract"),t(`: With ChatGPT's release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the "5W1H" guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.`)],-1),vo=e("h4",{id:"designing-a-user-centric-framework-for-information-quality-ranking-of-large-scale-street-view-images",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#designing-a-user-centric-framework-for-information-quality-ranking-of-large-scale-street-view-images"},[e("span",null,"Designing a User-centric Framework for Information Quality Ranking of Large-scale Street View Images")])],-1),bo=e("p",null,[e("strong",null,"Authors"),t(": Tahiya Chowdhury, Ilan Mandel, Jorge Ortiz, Wendy Ju")],-1),yo=e("strong",null,"Link",-1),wo={href:"http://arxiv.org/abs/2404.00392v1",target:"_blank",rel:"noopener noreferrer"},_o=e("p",null,[e("strong",null,"Abstract"),t(": Street view imagery (SVI), largely captured via outfitted fleets or mounted dashcams in consumer vehicles is a rapidly growing source of geospatial data used in urban sensing and development. These datasets are often collected opportunistically, are massive in size, and vary in quality which limits the scope and extent of their use in urban planning. Thus far there has not been much work to identify the obstacles experienced and tools needed by the users of such datasets. This severely limits the opportunities of using emerging street view images in supporting novel research questions that can improve the quality of urban life. This work includes a formative interview study with 5 expert users of large-scale street view datasets from academia, urban planning, and related professions which identifies novel use cases, challenges, and opportunities to increase the utility of these datasets. Based on the user findings, we present a framework to evaluate the quality of information for street images across three attributes (spatial, temporal, and content) that stakeholders can utilize for estimating the value of a dataset, and to improve it over time for their respective use case. We then present a case study using novel street view images where we evaluate our framework and present practical use cases for users. We discuss the implications for designing future systems to support the collection and use of street view data to assist in sensing and planning the urban environment.")],-1),ko=e("h4",{id:"on-task-and-in-sync-examining-the-relationship-between-gaze-synchrony-and-self-reported-attention-during-video-lecture-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#on-task-and-in-sync-examining-the-relationship-between-gaze-synchrony-and-self-reported-attention-during-video-lecture-learning"},[e("span",null,"On Task and in Sync: Examining the Relationship between Gaze Synchrony and Self-Reported Attention During Video Lecture Learning")])],-1),xo=e("p",null,[e("strong",null,"Authors"),t(": Babette Bühler, Efe Bozkir, Hannah Deininger, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci")],-1),Ao=e("strong",null,"Link",-1),Lo={href:"http://arxiv.org/abs/2404.00333v1",target:"_blank",rel:"noopener noreferrer"},To=e("p",null,[e("strong",null,"Abstract"),t(": Successful learning depends on learners' ability to sustain attention, which is particularly challenging in online education due to limited teacher interaction. A potential indicator for attention is gaze synchrony, demonstrating predictive power for learning achievements in video-based learning in controlled experiments focusing on manipulating attention. This study (N=84) examines the relationship between gaze synchronization and self-reported attention of learners, using experience sampling, during realistic online video learning. Gaze synchrony was assessed through Kullback-Leibler Divergence of gaze density maps and MultiMatch algorithm scanpath comparisons. Results indicated significantly higher gaze synchronization in attentive participants for both measures and self-reported attention significantly predicted post-test scores. In contrast, synchrony measures did not correlate with learning outcomes. While supporting the hypothesis that attentive learners exhibit similar eye movements, the direct use of synchrony as an attention indicator poses challenges, requiring further research on the interplay of attention, gaze synchrony, and video content type.")],-1),Io=e("h4",{id:"enhancing-empathy-in-virtual-reality-an-embodied-approach-to-mindset-modulation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-empathy-in-virtual-reality-an-embodied-approach-to-mindset-modulation"},[e("span",null,"Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset Modulation")])],-1),Mo=e("p",null,[e("strong",null,"Authors"),t(": Seoyeon Bae, Yoon Kyung Lee, Jungcheol Lee, Jaeheon Kim, Haeseong Jeon, Seung-Hwan Lim, Byung-Cheol Kim, Sowon Hahn")],-1),So=e("strong",null,"Link",-1),Co={href:"http://arxiv.org/abs/2404.00300v1",target:"_blank",rel:"noopener noreferrer"},zo=e("p",null,[e("strong",null,"Abstract"),t(": A growth mindset has shown promising outcomes for increasing empathy ability. However, stimulating a growth mindset in VR-based empathy interventions is under-explored. In the present study, we implemented prosocial VR content, Our Neighbor Hero, focusing on embodying a virtual character to modulate players' mindsets. The virtual body served as a stepping stone, enabling players to identify with the character and cultivate a growth mindset as they followed mission instructions. We considered several implementation factors to assist players in positioning within the VR experience, including positive feedback, content difficulty, background lighting, and multimodal feedback. We conducted an experiment to investigate the intervention's effectiveness in increasing empathy. Our findings revealed that the VR content and mindset training encouraged participants to improve their growth mindsets and empathic motives. This VR content was developed for college students to enhance their empathy and teamwork skills. It has the potential to improve collaboration in organizational and community environments.")],-1),Wo=e("h4",{id:"your-co-workers-matter-evaluating-collaborative-capabilities-of-language-models-in-blocks-world",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#your-co-workers-matter-evaluating-collaborative-capabilities-of-language-models-in-blocks-world"},[e("span",null,"Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World")])],-1),Po=e("p",null,[e("strong",null,"Authors"),t(": Guande Wu, Chen Zhao, Claudio Silva, He He")],-1),Ro=e("strong",null,"Link",-1),Ho={href:"http://arxiv.org/abs/2404.00246v1",target:"_blank",rel:"noopener noreferrer"},Eo=e("p",null,[e("strong",null,"Abstract"),t(": Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state and identify and correct execution errors. Both human-machine and machine-machine experiments show that LLM agents have strong grounding capacities, and our approach significantly improves the evaluation metric.")],-1),Do=e("h2",{id:"_2024-03-29",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-29"},[e("span",null,"2024-03-29")])],-1),qo=e("h4",{id:"tools-and-tasks-in-sensemaking-a-visual-accessibility-perspective",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tools-and-tasks-in-sensemaking-a-visual-accessibility-perspective"},[e("span",null,"Tools and Tasks in Sensemaking: A Visual Accessibility Perspective")])],-1),Bo=e("p",null,[e("strong",null,"Authors"),t(": Yichun Zhao, Miguel A. Nacenta")],-1),Go=e("strong",null,"Link",-1),No={href:"http://arxiv.org/abs/2404.00192v1",target:"_blank",rel:"noopener noreferrer"},Vo=e("p",null,[e("strong",null,"Abstract"),t(": Our previous interview study explores the needs and uses of diagrammatic information by the Blind and Low Vision (BLV) community, resulting in a framework called the Ladder of Diagram Access. The framework outlines five levels of information access when interacting with a diagram. In this paper, we connect this framework to include the global activity of sensemaking and discuss its (in)accessibility to the BLV demographic. We also discuss the integration of this framework into the sensemaking process and explore the current sensemaking practices and strategies employed by the BLV community, the challenges they face at different levels of the ladder, and potential solutions to enhance inclusivity towards a data-driven workforce.")],-1),Oo=e("h4",{id:"no-risk-no-reward-towards-an-automated-measure-of-psychological-safety-from-online-communication",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#no-risk-no-reward-towards-an-automated-measure-of-psychological-safety-from-online-communication"},[e("span",null,"No Risk, No Reward: Towards An Automated Measure of Psychological Safety from Online Communication")])],-1),jo=e("p",null,[e("strong",null,"Authors"),t(": Sharon Ferguson, Georgia Van de Zande, Alison Olechowski")],-1),Fo=e("strong",null,"Link",-1),Jo={href:"http://arxiv.org/abs/2404.00171v1",target:"_blank",rel:"noopener noreferrer"},Uo=e("p",null,[e("strong",null,"Abstract"),t(": The data created from virtual communication platforms presents the opportunity to explore automated measures for monitoring team performance. In this work, we explore one important characteristic of successful teams - Psychological Safety - or the belief that a team is safe for interpersonal risk-taking. To move towards an automated measure of this phenomenon, we derive virtual communication characteristics and message keywords related to elements of Psychological Safety from the literature. Using a mixed methods approach, we investigate whether these characteristics are present in the Slack messages from two design teams - one high in Psychological Safety, and one low. We find that some usage characteristics, such as replies, reactions, and user mentions, might be promising metrics to indicate higher levels of Psychological Safety, while simple keyword searches may not be nuanced enough. We present the first step towards the automated detection of this important, yet complex, team characteristic.")],-1),Ko=e("h4",{id:"circle-back-next-week-the-effect-of-meeting-free-weeks-on-distributed-workers-unstructured-time-and-attention-negotiation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#circle-back-next-week-the-effect-of-meeting-free-weeks-on-distributed-workers-unstructured-time-and-attention-negotiation"},[e("span",null,"Circle Back Next Week: The Effect of Meeting-Free Weeks on Distributed Workers' Unstructured Time and Attention Negotiation")])],-1),Zo=e("p",null,[e("strong",null,"Authors"),t(": Sharon Ferguson, Michael Massimi")],-1),Yo=e("strong",null,"Link",-1),Xo={href:"http://arxiv.org/abs/2404.00161v1",target:"_blank",rel:"noopener noreferrer"},Qo=e("p",null,[e("strong",null,"Abstract"),t(": While distributed workers rely on scheduled meetings for coordination and collaboration, these meetings can also challenge their ability to focus. Protecting worker focus has been addressed from a technical perspective, but companies are now attempting organizational interventions, such as meeting-free weeks. Recognizing distributed collaboration as a sociotechnical challenge, we first present an interview study with distributed workers participating in meeting-free weeks at an enterprise software company. We identify three orientations workers exhibit during these weeks: Focus, Collaborative, and Time-Bound, each with varying levels and use of unstructured time. These different orientations result in challenges in attention negotiation, which may be suited for technical interventions. This motivated a follow-up study investigating attention negotiation and the compensating mechanisms workers developed during meeting-free weeks. Our framework identified tensions between the attention-getting and attention-delegation strategies. We extend past work to show how workers adapt their virtual collaboration mechanisms in response to organizational interventions")],-1),$o=e("h4",{id:"give-text-a-chance-advocating-for-equal-consideration-for-language-and-visualization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#give-text-a-chance-advocating-for-equal-consideration-for-language-and-visualization"},[e("span",null,"Give Text A Chance: Advocating for Equal Consideration for Language and Visualization")])],-1),es=e("p",null,[e("strong",null,"Authors"),t(": Chase Stokes, Marti A. Hearst")],-1),ts=e("strong",null,"Link",-1),ns={href:"http://arxiv.org/abs/2404.00131v1",target:"_blank",rel:"noopener noreferrer"},is=e("p",null,[e("strong",null,"Abstract"),t(": Visualization research tends to de-emphasize consideration of the textual context in which its images are placed. We argue that visualization research should consider textual representations as a primary alternative to visual options when assessing designs, and when assessing designs, equal attention should be given to the construction of the language as to the visualizations. We also call for a consideration of readability when integrating visualizations with written text. In highlighting these points, visualization research would be elevated in efficacy and demonstrate thorough accounting for viewers' needs and responses.")],-1),as=e("h4",{id:"enhancing-dimension-reduced-scatter-plots-with-class-and-feature-centroids",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-dimension-reduced-scatter-plots-with-class-and-feature-centroids"},[e("span",null,"Enhancing Dimension-Reduced Scatter Plots with Class and Feature Centroids")])],-1),os=e("p",null,[e("strong",null,"Authors"),t(": Daniel B. Hier, Tayo Obafemi-Ajayi, Gayla R. Olbricht, Devin M. Burns, Sasha Petrenko, Donald C. Wunsch II")],-1),ss=e("strong",null,"Link",-1),rs={href:"http://arxiv.org/abs/2403.20246v1",target:"_blank",rel:"noopener noreferrer"},ls=e("p",null,[e("strong",null,"Abstract"),t(": Dimension reduction is increasingly applied to high-dimensional biomedical data to improve its interpretability. When datasets are reduced to two dimensions, each observation is assigned an x and y coordinates and is represented as a point on a scatter plot. A significant challenge lies in interpreting the meaning of the x and y axes due to the complexities inherent in dimension reduction. This study addresses this challenge by using the x and y coordinates derived from dimension reduction to calculate class and feature centroids, which can be overlaid onto the scatter plots. This method connects the low-dimension space to the original high-dimensional space. We illustrate the utility of this approach with data derived from the phenotypes of three neurogenetic diseases and demonstrate how the addition of class and feature centroids increases the interpretability of scatter plots.")],-1),cs=e("h4",{id:"entertainment-chatbot-for-the-digital-inclusion-of-elderly-people-without-abstraction-capabilities",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#entertainment-chatbot-for-the-digital-inclusion-of-elderly-people-without-abstraction-capabilities"},[e("span",null,"Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities")])],-1),hs=e("p",null,[e("strong",null,"Authors"),t(": Silvia García-Méndez, Francisco de Arriba-Pérez, Francisco J. González-Castaño, José A. Regueiro-Janeiro, Felipe Gil-Castiñeira")],-1),ds=e("strong",null,"Link",-1),us={href:"http://arxiv.org/abs/2404.01327v1",target:"_blank",rel:"noopener noreferrer"},ps=e("p",null,[e("strong",null,"Abstract"),t(`: Current language processing technologies allow the creation of conversational chatbot platforms. Even though artificial intelligence is still too immature to support satisfactory user experience in many mass market domains, conversational interfaces have found their way into ad hoc applications such as call centres and online shopping assistants. However, they have not been applied so far to social inclusion of elderly people, who are particularly vulnerable to the digital divide. Many of them relieve their loneliness with traditional media such as TV and radio, which are known to create a feeling of companionship. In this paper we present the EBER chatbot, designed to reduce the digital gap for the elderly. EBER reads news in the background and adapts its responses to the user's mood. Its novelty lies in the concept of "intelligent radio", according to which, instead of simplifying a digital information system to make it accessible to the elderly, a traditional channel they find familiar -- background news -- is augmented with interactions via voice dialogues. We make it possible by combining Artificial Intelligence Modelling Language, automatic Natural Language Generation and Sentiment Analysis. The system allows accessing digital content of interest by combining words extracted from user answers to chatbot questions with keywords extracted from the news items. This approach permits defining metrics of the abstraction capabilities of the users depending on a spatial representation of the word space. To prove the suitability of the proposed solution we present results of real experiments conducted with elderly people that provided valuable insights. Our approach was considered satisfactory during the tests and improved the information search capabilities of the participants.`)],-1),gs=e("h4",{id:"itcma-a-generative-agent-based-on-a-computational-consciousness-structure",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#itcma-a-generative-agent-based-on-a-computational-consciousness-structure"},[e("span",null,"ITCMA: A Generative Agent Based on a Computational Consciousness Structure")])],-1),ms=e("p",null,[e("strong",null,"Authors"),t(": Hanzhong Zhang, Jibin Yin, Haoyang Wang, Ziwei Xiang")],-1),fs=e("strong",null,"Link",-1),vs={href:"http://arxiv.org/abs/2403.20097v1",target:"_blank",rel:"noopener noreferrer"},bs=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure. We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and reasoning in open-world settings. ITCMA enhances LLMs' ability to understand implicit instructions and apply common-sense knowledge by considering agents' interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization. In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility in real-world settings.")],-1),ys=e("h4",{id:"mindarm-mechanized-intelligent-non-invasive-neuro-driven-prosthetic-arm-system",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#mindarm-mechanized-intelligent-non-invasive-neuro-driven-prosthetic-arm-system"},[e("span",null,"MindArm: Mechanized Intelligent Non-Invasive Neuro-Driven Prosthetic Arm System")])],-1),ws=e("p",null,[e("strong",null,"Authors"),t(": Maha Nawaz, Abdul Basit, Muhammad Shafique")],-1),_s=e("strong",null,"Link",-1),ks={href:"http://arxiv.org/abs/2403.19992v1",target:"_blank",rel:"noopener noreferrer"},xs=e("p",null,[e("strong",null,"Abstract"),t(': Currently, people with disability or difficulty to move their arms (referred to as "patients") have very limited technological solutions to efficiently address their physiological limitations. It is mainly due to two reasons: (1) the non-invasive solutions like mind-controlled prosthetic devices are typically very costly and require expensive maintenance; and (2) other solutions require costly invasive brain surgery, which is high risk to perform, expensive, and difficult to maintain. Therefore, current technological solutions are not accessible for all patients with different financial backgrounds. Toward this, we propose a low-cost technological solution called MindArm, a mechanized intelligent non-invasive neuro-driven prosthetic arm system. Our MindArm system employs a deep neural network (DNN) engine to translate brain signals into the intended prosthetic arm motion, thereby helping patients to perform many activities despite their physiological limitations. Here, our MindArm system utilizes widely accessible and low-cost surface electroencephalogram (EEG) electrodes coupled with an Open Brain Computer Interface and UDP networking for acquiring brain signals and transmitting them to the compute module for signal processing. In the compute module, we run a trained DNN model to interpret normalized micro-voltage of the brain signals, and then translate them into a prosthetic arm action via serial communication seamlessly. The experimental results on a fully working prototype demonstrate that, from the three defined actions, our MindArm system achieves positive success rates, i.e., 91% for idle/stationary, 85% for shake hand, and 84% for pick-up cup. This demonstrates that our MindArm provides a novel approach for an alternate low-cost mind-controlled prosthetic devices for all patients.')],-1),As=e("h2",{id:"_2024-03-28",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-28"},[e("span",null,"2024-03-28")])],-1),Ls=e("h4",{id:"i-m-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#i-m-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices"},[e("span",null,`"I'm categorizing LLM as a productivity tool": Examining ethics of LLM use in HCI research practices`)])],-1),Ts=e("p",null,[e("strong",null,"Authors"),t(": Shivani Kapania, Ruiyi Wang, Toby Jia-Jun Li, Tianshi Li, Hong Shen")],-1),Is=e("strong",null,"Link",-1),Ms={href:"http://arxiv.org/abs/2403.19876v1",target:"_blank",rel:"noopener noreferrer"},Ss=e("p",null,[e("strong",null,"Abstract"),t(": Large language models are increasingly applied in real-world scenarios, including research and education. These models, however, come with well-known ethical issues, which may manifest in unexpected ways in human-computer interaction research due to the extensive engagement with human subjects. This paper reports on research practices related to LLM use, drawing on 16 semi-structured interviews and a survey conducted with 50 HCI researchers. We discuss the ways in which LLMs are already being utilized throughout the entire HCI research pipeline, from ideation to system development and paper writing. While researchers described nuanced understandings of ethical issues, they were rarely or only partially able to identify and address those ethical concerns in their own projects. This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the LLM supply chain, the conditional nature of engaging with ethics, and competing priorities. Finally, we reflect on the implications of our findings and present opportunities to shape emerging norms of engaging with large language models in HCI research.")],-1),Cs=e("h4",{id:"creating-aesthetic-sonifications-on-the-web-with-siren",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#creating-aesthetic-sonifications-on-the-web-with-siren"},[e("span",null,"Creating Aesthetic Sonifications on the Web with SIREN")])],-1),zs=e("p",null,[e("strong",null,"Authors"),t(": Tristan Peng, Hongchan Choi, Jonathan Berger")],-1),Ws=e("strong",null,"Link",-1),Ps={href:"http://arxiv.org/abs/2403.19763v1",target:"_blank",rel:"noopener noreferrer"},Rs=e("p",null,[e("strong",null,"Abstract"),t(": SIREN is a flexible, extensible, and customizable web-based general-purpose interface for auditory data display (sonification). Designed as a digital audio workstation for sonification, synthesizers written in JavaScript using the Web Audio API facilitate intuitive mapping of data to auditory parameters for a wide range of purposes. This paper explores the breadth of sound synthesis techniques supported by SIREN, and details the structure and definition of a SIREN synthesizer module. The paper proposes further development that will increase SIREN's utility.")],-1),Hs=e("h4",{id:"leveraging-counterfactual-paths-for-contrastive-explanations-of-pomdp-policies",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#leveraging-counterfactual-paths-for-contrastive-explanations-of-pomdp-policies"},[e("span",null,"Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies")])],-1),Es=e("p",null,[e("strong",null,"Authors"),t(": Benjamin Kraske, Zakariya Laouar, Zachary Sunberg")],-1),Ds=e("strong",null,"Link",-1),qs={href:"http://arxiv.org/abs/2403.19760v1",target:"_blank",rel:"noopener noreferrer"},Bs=e("p",null,[e("strong",null,"Abstract"),t(": As humans come to rely on autonomous systems more, ensuring the transparency of such systems is important to their continued adoption. Explainable Artificial Intelligence (XAI) aims to reduce confusion and foster trust in systems by providing explanations of agent behavior. Partially observable Markov decision processes (POMDPs) provide a flexible framework capable of reasoning over transition and state uncertainty, while also being amenable to explanation. This work investigates the use of user-provided counterfactuals to generate contrastive explanations of POMDP policies. Feature expectations are used as a means of contrasting the performance of these policies. We demonstrate our approach in a Search and Rescue (SAR) setting. We analyze and discuss the associated challenges through two case studies.")],-1),Gs=e("h4",{id:"collaborative-interactive-evolution-of-art-in-the-latent-space-of-deep-generative-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#collaborative-interactive-evolution-of-art-in-the-latent-space-of-deep-generative-models"},[e("span",null,"Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models")])],-1),Ns=e("p",null,[e("strong",null,"Authors"),t(": Ole Hall, Anil Yaman")],-1),Vs=e("strong",null,"Link",-1),Os={href:"http://arxiv.org/abs/2403.19620v1",target:"_blank",rel:"noopener noreferrer"},js=e("p",null,[e("strong",null,"Abstract"),t(": Generative Adversarial Networks (GANs) have shown great success in generating high quality images and are thus used as one of the main approaches to generate art images. However, usually the image generation process involves sampling from the latent space of the learned art representations, allowing little control over the output. In this work, we first employ GANs that are trained to produce creative images using an architecture known as Creative Adversarial Networks (CANs), then, we employ an evolutionary approach to navigate within the latent space of the models to discover images. We use automatic aesthetic and collaborative interactive human evaluation metrics to assess the generated images. In the human interactive evaluation case, we propose a collaborative evaluation based on the assessments of several participants. Furthermore, we also experiment with an intelligent mutation operator that aims to improve the quality of the images through local search based on an aesthetic measure. We evaluate the effectiveness of this approach by comparing the results produced by the automatic and collaborative interactive evolution. The results show that the proposed approach can generate highly attractive art images when the evolution is guided by collaborative human feedback.")],-1),Fs=e("h4",{id:"exploring-communication-dynamics-eye-tracking-analysis-in-pair-programming-of-computer-science-education",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-communication-dynamics-eye-tracking-analysis-in-pair-programming-of-computer-science-education"},[e("span",null,"Exploring Communication Dynamics: Eye-tracking Analysis in Pair Programming of Computer Science Education")])],-1),Js=e("p",null,[e("strong",null,"Authors"),t(": Wunmin Jang, Hong Gao, Tilman Michaeli, Enkelejda Kasneci")],-1),Us=e("strong",null,"Link",-1),Ks={href:"http://arxiv.org/abs/2403.19560v1",target:"_blank",rel:"noopener noreferrer"},Zs=e("p",null,[e("strong",null,"Abstract"),t(": Pair programming is widely recognized as an effective educational tool in computer science that promotes collaborative learning and mirrors real-world work dynamics. However, communication breakdowns within pairs significantly challenge this learning process. In this study, we use eye-tracking data recorded during pair programming sessions to study communication dynamics between various pair programming roles across different student, expert, and mixed group cohorts containing 19 participants. By combining eye-tracking data analysis with focus group interviews and questionnaires, we provide insights into communication's multifaceted nature in pair programming. Our findings highlight distinct eye-tracking patterns indicating changes in communication skills across group compositions, with participants prioritizing code exploration over communication, especially during challenging tasks. Further, students showed a preference for pairing with experts, emphasizing the importance of understanding group formation in pair programming scenarios. These insights emphasize the importance of understanding group dynamics and enhancing communication skills through pair programming for successful outcomes in computer science education.")],-1),Ys=e("h4",{id:"llms-as-academic-reading-companions-extending-hci-through-synthetic-personae",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-as-academic-reading-companions-extending-hci-through-synthetic-personae"},[e("span",null,"LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae")])],-1),Xs=e("p",null,[e("strong",null,"Authors"),t(": Celia Chen, Alex Leitch")],-1),Qs=e("strong",null,"Link",-1),$s={href:"http://arxiv.org/abs/2403.19506v1",target:"_blank",rel:"noopener noreferrer"},er=e("p",null,[e("strong",null,"Abstract"),t(": This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning. We detail an exploratory study examining Claude.ai from Anthropic, an LLM-based interactive assistant that helps students comprehend complex qualitative literature content. The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging Claude.ai over a semester across two graduate courses. Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study. However, there is potential for overreliance and ethical considerations that warrant continued investigation. By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing.")],-1),tr=e("h4",{id:"a-theoretical-framework-for-the-design-and-analysis-of-computational-thinking-problems-in-education",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-theoretical-framework-for-the-design-and-analysis-of-computational-thinking-problems-in-education"},[e("span",null,"A theoretical framework for the design and analysis of computational thinking problems in education")])],-1),nr=e("p",null,[e("strong",null,"Authors"),t(": Giorgia Adorni, Alberto Piatti, Engin Bumbacher, Lucio Negrini, Francesco Mondada, Dorit Assaf, Francesca Mangili, Luca Gambardella")],-1),ir=e("strong",null,"Link",-1),ar={href:"http://arxiv.org/abs/2403.19475v1",target:"_blank",rel:"noopener noreferrer"},or=e("p",null,[e("strong",null,"Abstract"),t(": The field of computational thinking education has grown in recent years as researchers and educators have sought to develop and assess students' computational thinking abilities. While much of the research in this area has focused on defining computational thinking, the competencies it involves and how to assess them in teaching and learning contexts, this work takes a different approach. We provide a more situated perspective on computational thinking, focusing on the types of problems that require computational thinking skills to be solved and the features that support these processes. We develop a framework for analysing existing computational thinking problems in an educational context. We conduct a comprehensive literature review to identify prototypical activities from areas where computational thinking is typically pursued in education. We identify the main components and characteristics of these activities, along with their influence on activating computational thinking competencies. The framework provides a catalogue of computational thinking skills that can be used to understand the relationship between problem features and competencies activated. This study contributes to the field of computational thinking education by offering a tool for evaluating and revising existing problems to activate specific skills and for assisting in designing new problems that target the development of particular competencies. The results of this study may be of interest to researchers and educators working in computational thinking education.")],-1),sr=e("h4",{id:"at-the-end-of-the-day-i-am-accountable-gig-workers-self-tracking-for-multi-dimensional-accountability-management",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#at-the-end-of-the-day-i-am-accountable-gig-workers-self-tracking-for-multi-dimensional-accountability-management"},[e("span",null,`"At the end of the day, I am accountable": Gig Workers' Self-Tracking for Multi-Dimensional Accountability Management`)])],-1),rr=e("p",null,[e("strong",null,"Authors"),t(": Rie Helene, Hernandez, Qiurong Song, Yubo Kou, Xinning Gui")],-1),lr=e("strong",null,"Link",-1),cr={href:"http://arxiv.org/abs/2403.19436v1",target:"_blank",rel:"noopener noreferrer"},hr=e("p",null,[e("strong",null,"Abstract"),t(": Tracking is inherent in and central to the gig economy. Platforms track gig workers' performance through metrics such as acceptance rate and punctuality, while gig workers themselves engage in self-tracking. Although prior research has extensively examined how gig platforms track workers through metrics -- with some studies briefly acknowledging the phenomenon of self-tracking among workers -- there is a dearth of studies that explore how and why gig workers track themselves. To address this, we conducted 25 semi-structured interviews, revealing how gig workers self-tracking to manage accountabilities to themselves and external entities across three identities: the holistic self, the entrepreneurial self, and the platformized self. We connect our findings to neoliberalism, through which we contextualize gig workers' self-accountability and the invisible labor of self-tracking. We further discuss how self-tracking mitigates information and power asymmetries in gig work and offer design implications to support gig workers' multi-dimensional self-tracking.")],-1),dr=e("h4",{id:"an-interactive-human-machine-learning-interface-for-collecting-and-learning-from-complex-annotations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#an-interactive-human-machine-learning-interface-for-collecting-and-learning-from-complex-annotations"},[e("span",null,"An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations")])],-1),ur=e("p",null,[e("strong",null,"Authors"),t(": Jonathan Erskine, Matt Clifford, Alexander Hepburn, Raúl Santos-Rodríguez")],-1),pr=e("strong",null,"Link",-1),gr={href:"http://arxiv.org/abs/2403.19339v1",target:"_blank",rel:"noopener noreferrer"},mr=e("p",null,[e("strong",null,"Abstract"),t(": Human-Computer Interaction has been shown to lead to improvements in machine learning systems by boosting model performance, accelerating learning and building user confidence. In this work, we aim to alleviate the expectation that human annotators adapt to the constraints imposed by traditional labels by allowing for extra flexibility in the form that supervision information is collected. For this, we propose a human-machine learning interface for binary classification tasks which enables human annotators to utilise counterfactual examples to complement standard binary labels as annotations for a dataset. Finally we discuss the challenges in future extensions of this work.")],-1),fr=e("h4",{id:"cognidot-vasoactivity-based-cognitive-load-monitoring-with-a-miniature-on-skin-sensor",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#cognidot-vasoactivity-based-cognitive-load-monitoring-with-a-miniature-on-skin-sensor"},[e("span",null,"CogniDot: Vasoactivity-based Cognitive Load Monitoring with a Miniature On-skin Sensor")])],-1),vr=e("p",null,[e("strong",null,"Authors"),t(": Hongbo Lan, Yanrong Li, Shixuan Li, Xin Yi, Tengxiang Zhang")],-1),br=e("strong",null,"Link",-1),yr={href:"http://arxiv.org/abs/2403.19206v1",target:"_blank",rel:"noopener noreferrer"},wr=e("p",null,[e("strong",null,"Abstract"),t(": Vascular activities offer valuable signatures for psychological monitoring applications. We present CogniDot, an affordable, miniature skin sensor placed on the temporal area on the head that senses cognitive loads with a single-pixel color sensor. With its energy-efficient design, bio-compatible adhesive, and compact size (22mm diameter, 8.5mm thickness), it is ideal for long-term monitoring of mind status. We showed in detail the hardware design of our sensor. The user study results with 12 participants show that CogniDot can accurately differentiate between three levels of cognitive loads with a within-user accuracy of 97%. We also discuss its potential for broader applications.")],-1),_r=e("h4",{id:"algorithmic-ways-of-seeing-using-object-detection-to-facilitate-art-exploration",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#algorithmic-ways-of-seeing-using-object-detection-to-facilitate-art-exploration"},[e("span",null,"Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration")])],-1),kr=e("p",null,[e("strong",null,"Authors"),t(": Louie Søs Meyer, Johanne Engel Aaen, Anitamalina Regitse Tranberg, Peter Kun, Matthias Freiberger, Sebastian Risi, Anders Sundnes Løvlie")],-1),xr=e("strong",null,"Link",-1),Ar={href:"http://arxiv.org/abs/2403.19174v1",target:"_blank",rel:"noopener noreferrer"},Lr=e("p",null,[e("strong",null,"Abstract"),t(": This Research through Design paper explores how object detection may be applied to a large digital art museum collection to facilitate new ways of encountering and experiencing art. We present the design and evaluation of an interactive application called SMKExplore, which allows users to explore a museum's digital collection of paintings by browsing through objects detected in the images, as a novel form of open-ended exploration. We provide three contributions. First, we show how an object detection pipeline can be integrated into a design process for visual exploration. Second, we present the design and development of an app that enables exploration of an art museum's collection. Third, we offer reflections on future possibilities for museums and HCI researchers to incorporate object detection techniques into the digitalization of museums.")],-1),Tr=e("h4",{id:"exploring-holistic-hmi-design-for-automated-vehicles-insights-from-a-participatory-workshop-to-bridge-in-vehicle-and-external-communication",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-holistic-hmi-design-for-automated-vehicles-insights-from-a-participatory-workshop-to-bridge-in-vehicle-and-external-communication"},[e("span",null,"Exploring Holistic HMI Design for Automated Vehicles: Insights from a Participatory Workshop to Bridge In-Vehicle and External Communication")])],-1),Ir=e("p",null,[e("strong",null,"Authors"),t(": Haoyu Dong, Tram Thi Minh Tran, Rutger Verstegen, Silvia Cazacu, Ruolin Gao, Marius Hoggenmüller, Debargha Dey, Mervyn Franssen, Markus Sasalovici, Pavlo Bazilinskyy, Marieke Martens")],-1),Mr=e("strong",null,"Link",-1),Sr={href:"http://arxiv.org/abs/2403.19153v1",target:"_blank",rel:"noopener noreferrer"},Cr=e("p",null,[e("strong",null,"Abstract"),t(": Human-Machine Interfaces (HMIs) for automated vehicles (AVs) are typically divided into two categories: internal HMIs for interactions within the vehicle, and external HMIs for communication with other road users. In this work, we examine the prospects of bridging these two seemingly distinct domains. Through a participatory workshop with automotive user interface researchers and practitioners, we facilitated a critical exploration of holistic HMI design by having workshop participants collaboratively develop interaction scenarios involving AVs, in-vehicle users, and external road users. The discussion offers insights into the escalation of interface elements as an HMI design strategy, the direct interactions between different users, and an expanded understanding of holistic HMI design. This work reflects a collaborative effort to understand the practical aspects of this holistic design approach, offering new perspectives and encouraging further investigation into this underexplored aspect of automotive user interfaces.")],-1),zr=e("h4",{id:"real-time-accident-detection-and-physiological-signal-monitoring-to-enhance-motorbike-safety-and-emergency-response",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#real-time-accident-detection-and-physiological-signal-monitoring-to-enhance-motorbike-safety-and-emergency-response"},[e("span",null,"Real-time accident detection and physiological signal monitoring to enhance motorbike safety and emergency response")])],-1),Wr=e("p",null,[e("strong",null,"Authors"),t(": S. M. Kayser Mehbub Siam, Khadiza Islam Sumaiya, Md Rakib Al-Amin, Tamim Hasan Turjo, Ahsanul Islam, A. H. M. A. Rahim, Md Rakibul Hasan")],-1),Pr=e("strong",null,"Link",-1),Rr={href:"http://arxiv.org/abs/2403.19085v1",target:"_blank",rel:"noopener noreferrer"},Hr=e("p",null,[e("strong",null,"Abstract"),t(": Rapid urbanization and improved living standards have led to a substantial increase in the number of vehicles on the road, consequently resulting in a rise in the frequency of accidents. Among these accidents, motorbike accidents pose a particularly high risk, often resulting in serious injuries or deaths. A significant number of these fatalities occur due to delayed or inadequate medical attention. To this end, we propose a novel automatic detection and notification system specifically designed for motorbike accidents. The proposed system comprises two key components: a detection system and a physiological signal monitoring system. The detection system is integrated into the helmet and consists of a microcontroller, accelerometer, GPS, GSM, and Wi-Fi modules. The physio-monitoring system incorporates a sensor for monitoring pulse rate and SpO$"),e("em",null,"{2}$ saturation. All collected data are presented on an LCD display and wirelessly transmitted to the detection system through the microcontroller of the physiological signal monitoring system. If the accelerometer readings consistently deviate from the specified threshold decided through extensive experimentation, the system identifies the event as an accident and transmits the victim's information -- including the GPS location, pulse rate, and SpO$"),t("{2}$ saturation rate -- to the designated emergency contacts. Preliminary results demonstrate the efficacy of the proposed system in accurately detecting motorbike accidents and promptly alerting emergency contacts. We firmly believe that the proposed system has the potential to significantly mitigate the risks associated with motorbike accidents and save lives.")],-1),Er=e("h2",{id:"_2024-03-27",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-27"},[e("span",null,"2024-03-27")])],-1),Dr=e("h4",{id:"towards-human-centered-construction-robotics-an-rl-driven-companion-robot-for-contextually-assisting-carpentry-workers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-human-centered-construction-robotics-an-rl-driven-companion-robot-for-contextually-assisting-carpentry-workers"},[e("span",null,"Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers")])],-1),qr=e("p",null,[e("strong",null,"Authors"),t(": Yuning Wu, Jiaying Wei, Jean Oh, Daniel Cardoso Llach")],-1),Br=e("strong",null,"Link",-1),Gr={href:"http://arxiv.org/abs/2403.19060v1",target:"_blank",rel:"noopener noreferrer"},Nr=e("p",null,[e("strong",null,"Abstract"),t(": In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover\" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive and collaborative human-robot workforce.")],-1),Vr=e("h4",{id:"visualizing-high-dimensional-temporal-data-using-direction-aware-t-sne",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#visualizing-high-dimensional-temporal-data-using-direction-aware-t-sne"},[e("span",null,"Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE")])],-1),Or=e("p",null,[e("strong",null,"Authors"),t(": Pavlin G. Poličar, Blaž Zupan")],-1),jr=e("strong",null,"Link",-1),Fr={href:"http://arxiv.org/abs/2403.19040v1",target:"_blank",rel:"noopener noreferrer"},Jr=e("p",null,[e("strong",null,"Abstract"),t(": Many real-world data sets contain a temporal component or involve transitions from state to state. For exploratory data analysis, we can represent these high-dimensional data sets in two-dimensional maps, using embeddings of the data objects under exploration and representing their temporal relationships with directed edges. Most existing dimensionality reduction techniques, such as t-SNE and UMAP, do not take into account the temporal or relational nature of the data when constructing the embeddings, resulting in temporally cluttered visualizations that obscure potentially interesting patterns. To address this problem, we propose two complementary, direction-aware loss terms in the optimization function of t-SNE that emphasize the temporal aspects of the data, guiding the optimization and the resulting embedding to reveal temporal patterns that might otherwise go unnoticed. The Directional Coherence Loss (DCL) encourages nearby arrows connecting two adjacent time series points to point in the same direction, while the Edge Length Loss (ELL) penalizes arrows - which effectively represent time gaps in the visualized embedding - based on their length. Both loss terms are differentiable and can be easily incorporated into existing dimensionality reduction techniques. By promoting local directionality of the directed edges, our procedure produces more temporally meaningful and less cluttered visualizations. We demonstrate the effectiveness of our approach on a toy dataset and two real-world datasets.")],-1),Ur=e("h4",{id:"women-are-less-comfortable-expressing-opinions-online-than-men-and-report-heightened-fears-for-safety-surveying-gender-differences-in-experiences-of-online-harms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#women-are-less-comfortable-expressing-opinions-online-than-men-and-report-heightened-fears-for-safety-surveying-gender-differences-in-experiences-of-online-harms"},[e("span",null,"Women are less comfortable expressing opinions online than men and report heightened fears for safety: Surveying gender differences in experiences of online harms")])],-1),Kr=e("p",null,[e("strong",null,"Authors"),t(": Francesca Stevens, Florence E. Enock, Tvesha Sippy, Jonathan Bright, Miranda Cross, Pica Johansson, Judy Wajcman, Helen Z. Margetts")],-1),Zr=e("strong",null,"Link",-1),Yr={href:"http://arxiv.org/abs/2403.19037v1",target:"_blank",rel:"noopener noreferrer"},Xr=e("p",null,[e("strong",null,"Abstract"),t(": Online harms, such as hate speech, trolling and self-harm promotion, continue to be widespread. While some work suggests women are disproportionately affected, other studies find mixed evidence for gender differences in experiences with content of this kind. Using a nationally representative survey of UK adults (N=1992), we examine exposure to a variety of harms, fears surrounding being targeted, the psychological impact of online experiences, the use of safety tools to protect against harm, and comfort with various forms of online participation across men and women. We find that while men and women see harmful content online to a roughly similar extent, women are more at risk than men of being targeted by harms including online misogyny, cyberstalking and cyberflashing. Women are significantly more fearful of being targeted by harms overall, and report greater negative psychological impact as a result of particular experiences. Perhaps in an attempt to mitigate risk, women report higher use of a range of safety tools and less comfort with several forms of online participation, with just 23% of women comfortable expressing political views online compared to 40% of men. We also find direct associations between fears surrounding harms and comfort with online behaviours. For example, fear of being trolled significantly decreases comfort expressing opinions, and fear of being targeted by misogyny significantly decreases comfort sharing photos. Our results are important because with much public discourse happening online, we must ensure all members of society feel safe and able to participate in online spaces.")],-1),Qr=e("h4",{id:"should-i-help-a-delivery-robot-cultivating-prosocial-norms-through-observations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#should-i-help-a-delivery-robot-cultivating-prosocial-norms-through-observations"},[e("span",null,"Should I Help a Delivery Robot? Cultivating Prosocial Norms through Observations")])],-1),$r=e("p",null,[e("strong",null,"Authors"),t(": Vivienne Bihe Chi, Shashank Mehrotra, Teruhisa Misu, Kumar Akash")],-1),el=e("strong",null,"Link",-1),tl={href:"http://arxiv.org/abs/2403.19027v1",target:"_blank",rel:"noopener noreferrer"},nl=e("p",null,[e("strong",null,"Abstract"),t(": We propose leveraging prosocial observations to cultivate new social norms to encourage prosocial behaviors toward delivery robots. With an online experiment, we quantitatively assess updates in norm beliefs regarding human-robot prosocial behaviors through observational learning. Results demonstrate the initially perceived normativity of helping robots is influenced by familiarity with delivery robots and perceptions of robots' social intelligence. Observing human-robot prosocial interactions notably shifts peoples' normative beliefs about prosocial actions; thereby changing their perceived obligations to offer help to delivery robots. Additionally, we found that observing robots offering help to humans, rather than receiving help, more significantly increased participants' feelings of obligation to help robots. Our findings provide insights into prosocial design for future mobility systems. Improved familiarity with robot capabilities and portraying them as desirable social partners can help foster wider acceptance. Furthermore, robots need to be designed to exhibit higher levels of interactivity and reciprocal capabilities for prosocial behavior.")],-1),il=e("h4",{id:"the-correlations-of-scene-complexity-workload-presence-and-cybersickness-in-a-task-based-vr-game",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-correlations-of-scene-complexity-workload-presence-and-cybersickness-in-a-task-based-vr-game"},[e("span",null,"The Correlations of Scene Complexity, Workload, Presence, and Cybersickness in a Task-Based VR Game")])],-1),al=e("p",null,[e("strong",null,"Authors"),t(": Mohammadamin Sanaei, Stephen B. Gilbert, Nikoo Javadpour, Hila Sabouni, Michael C. Dorneich, Jonathan W. Kelly")],-1),ol=e("strong",null,"Link",-1),sl={href:"http://arxiv.org/abs/2403.19019v1",target:"_blank",rel:"noopener noreferrer"},rl=e("p",null,[e("strong",null,"Abstract"),t(": This investigation examined the relationships among scene complexity, workload, presence, and cybersickness in virtual reality (VR) environments. Numerous factors can influence the overall VR experience, and existing research on this matter is not yet conclusive, warranting further investigation. In this between-subjects experimental setup, 44 participants engaged in the Pendulum Chair game, with half exposed to a simple scene with lower optic flow and lower familiarity, and the remaining half to a complex scene characterized by higher optic flow and greater familiarity. The study measured the dependent variables workload, presence, and cybersickness and analyzed their correlations. Equivalence testing was also used to compare the simple and complex environments. Results revealed that despite the visible differences between the environments, within the 10% boundaries of the maximum possible value for workload and presence, and 13.6% of the maximum SSQ value, a statistically significant equivalence was observed between the simple and complex scenes. Additionally, a moderate, negative correlation emerged between workload and SSQ scores. The findings suggest two key points: (1) the nature of the task can mitigate the impact of scene complexity factors such as optic flow and familiarity, and (2) the correlation between workload and cybersickness may vary, showing either a positive or negative relationship.")],-1),ll=e("h4",{id:"thelxinoe-recognizing-human-emotions-using-pupillometry-and-machine-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#thelxinoe-recognizing-human-emotions-using-pupillometry-and-machine-learning"},[e("span",null,"Thelxinoë: Recognizing Human Emotions Using Pupillometry and Machine Learning")])],-1),cl=e("p",null,[e("strong",null,"Authors"),t(": Darlene Barker, Haim Levkowitz")],-1),hl=e("strong",null,"Link",-1),dl={href:"http://arxiv.org/abs/2403.19014v1",target:"_blank",rel:"noopener noreferrer"},ul=e("p",null,[e("strong",null,"Abstract"),t(': In this study, we present a method for emotion recognition in Virtual Reality (VR) using pupillometry. We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VR generated data. Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR). By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it. This research contributes significantly to the Thelxino"e framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions. Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for future advancements in virtual touch technology.')],-1),pl=e("h4",{id:"solderlesspcb-reusing-electronic-components-in-pcb-prototyping-through-detachable-3d-printed-housings",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#solderlesspcb-reusing-electronic-components-in-pcb-prototyping-through-detachable-3d-printed-housings"},[e("span",null,"SolderlessPCB: Reusing Electronic Components in PCB Prototyping through Detachable 3D Printed Housings")])],-1),gl=e("p",null,[e("strong",null,"Authors"),t(": Zeyu Yan, Jiasheng Li, Zining Zhang, Huaishu Peng")],-1),ml=e("strong",null,"Link",-1),fl={href:"http://arxiv.org/abs/2403.18797v1",target:"_blank",rel:"noopener noreferrer"},vl=e("p",null,[e("strong",null,"Abstract"),t(": The iterative prototyping process for printed circuit boards (PCBs) frequently employs surface-mounted device (SMD) components, which are often discarded rather than reused due to the challenges associated with desoldering, leading to unnecessary electronic waste. This paper introduces SolderlessPCB, a collection of techniques for solder-free PCB prototyping, specifically designed to promote the recycling and reuse of electronic components. Central to this approach are custom 3D-printable housings that allow SMD components to be mounted onto PCBs without soldering. We detail the design of SolderlessPCB and the experiments conducted to evaluate its design parameters, electrical performance, and durability. To illustrate the potential for reusing SMD components with SolderlessPCB, we discuss two scenarios: the reuse of components from earlier design iterations and from obsolete prototypes. We also provide examples demonstrating that SolderlessPCB can handle high-current applications and is suitable for high-speed data transmission. The paper concludes by discussing the limitations of our approach and suggesting future directions to overcome these challenges.")],-1),bl=e("h4",{id:"teaching-introductory-hri-uchicago-course-human-robot-interaction-research-and-practice",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#teaching-introductory-hri-uchicago-course-human-robot-interaction-research-and-practice"},[e("span",null,'Teaching Introductory HRI: UChicago Course "Human-Robot Interaction: Research and Practice"')])],-1),yl=e("p",null,[e("strong",null,"Authors"),t(": Sarah Sebo")],-1),wl=e("strong",null,"Link",-1),_l={href:"http://arxiv.org/abs/2403.18692v1",target:"_blank",rel:"noopener noreferrer"},kl=e("p",null,[e("strong",null,"Abstract"),t(": In 2020, I designed the course CMSC 20630/30630 Human-Robot Interaction: Research and Practice as a hands-on introduction to human-robot interaction (HRI) research for both undergraduate and graduate students at the University of Chicago. Since 2020, I have taught and refined this course each academic year. Human-Robot Interaction: Research and Practice focuses on the core concepts and cutting-edge research in the field of human-robot interaction (HRI), covering topics that include: nonverbal robot behavior, verbal robot behavior, social dynamics, norms & ethics, collaboration & learning, group interactions, applications, and future challenges of HRI. Course meetings involve students in the class leading discussions about cutting-edge peer-reviewed research HRI publications. Students also participate in a quarter-long collaborative research project, where they pursue an HRI research question that often involves conducing their own human-subjects research study where they recruit human subjects to interact with a robot. In this paper, I detail the structure of the course and its learning goals as well as my reflections and student feedback on the course.")],-1),xl=e("h4",{id:"an-exploratory-study-on-upper-level-computing-students-use-of-large-language-models-as-tools-in-a-semester-long-project",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#an-exploratory-study-on-upper-level-computing-students-use-of-large-language-models-as-tools-in-a-semester-long-project"},[e("span",null,"An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project")])],-1),Al=e("p",null,[e("strong",null,"Authors"),t(": Ben Arie Tanay, Lexy Arinze, Siddhant S. Joshi, Kirsten A. Davis, James C. Davis")],-1),Ll=e("strong",null,"Link",-1),Tl={href:"http://arxiv.org/abs/2403.18679v1",target:"_blank",rel:"noopener noreferrer"},Il=e("p",null,[e("strong",null,"Abstract"),t(": Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are influencing software engineering practice. Software engineering educators must teach future software engineers how to use such tools well. As of yet, there have been few studies that report on the use of LLMs in the classroom. It is, therefore, important to evaluate students' perception of LLMs and possible ways of adapting the computing curriculum to these shifting paradigms. Purpose: The purpose of this study is to explore computing students' experiences and approaches to using LLMs during a semester-long software engineering project. Design/Method: We collected data from a senior-level software engineering course at Purdue University. This course uses a project-based learning (PBL) design. The students used LLMs such as ChatGPT and Copilot in their projects. A sample of these student teams were interviewed to understand (1) how they used LLMs in their projects; and (2) whether and how their perspectives on LLMs changed over the course of the semester. We analyzed the data to identify themes related to students' usage patterns and learning outcomes. Results/Discussion: When computing students utilize LLMs within a project, their use cases cover both technical and professional applications. In addition, these students perceive LLMs to be efficient tools in obtaining information and completion of tasks. However, there were concerns about the responsible use of LLMs without being detrimental to their own learning outcomes. Based on our findings, we recommend future research to investigate the usage of LLM's in lower-level computer engineering courses to understand whether and how LLMs can be integrated as a learning aid without hurting the learning outcomes.")],-1),Ml=e("h4",{id:"aiming-for-relevance",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#aiming-for-relevance"},[e("span",null,"Aiming for Relevance")])],-1),Sl=e("p",null,[e("strong",null,"Authors"),t(": Bar Eini Porat, Danny Eytan, Uri Shalit")],-1),Cl=e("strong",null,"Link",-1),zl={href:"http://arxiv.org/abs/2403.18668v1",target:"_blank",rel:"noopener noreferrer"},Wl=e("p",null,[e("strong",null,"Abstract"),t(": Vital signs are crucial in intensive care units (ICUs). They are used to track the patient's state and to identify clinically significant changes. Predicting vital sign trajectories is valuable for early detection of adverse events. However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions. We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations. These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians. We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU). Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events. This research paves the way for clinically relevant machine learning model evaluation and optimization, promising to improve ICU patient care. 10 pages, 9 figures.")],-1);function Pl(Rl,Hl){const n=o("ExternalLinkIcon");return s(),r("div",null,[h,e("p",null,[d,t(": "),e("a",u,[t("http://arxiv.org/abs/2404.03612v1"),i(n)])]),p,g,m,e("p",null,[f,t(": "),e("a",v,[t("http://arxiv.org/abs/2404.03498v1"),i(n)])]),b,y,w,e("p",null,[_,t(": "),e("a",k,[t("http://arxiv.org/abs/2404.03356v1"),i(n)])]),x,A,L,e("p",null,[T,t(": "),e("a",I,[t("http://arxiv.org/abs/2404.03337v1"),i(n)])]),M,S,C,e("p",null,[z,t(": "),e("a",W,[t("http://arxiv.org/abs/2404.03239v1"),i(n)])]),P,R,H,e("p",null,[E,t(": "),e("a",D,[t("http://arxiv.org/abs/2404.03206v1"),i(n)])]),q,B,G,e("p",null,[N,t(": "),e("a",V,[t("http://arxiv.org/abs/2404.03165v1"),i(n)])]),O,j,F,e("p",null,[J,t(": "),e("a",U,[t("http://arxiv.org/abs/2404.03130v1"),i(n)])]),K,Z,Y,X,e("p",null,[Q,t(": "),e("a",$,[t("http://arxiv.org/abs/2404.03108v1"),i(n)])]),ee,te,ne,e("p",null,[ie,t(": "),e("a",ae,[t("http://arxiv.org/abs/2404.03085v1"),i(n)])]),oe,se,re,e("p",null,[le,t(": "),e("a",ce,[t("http://arxiv.org/abs/2404.03023v1"),i(n)])]),he,de,ue,e("p",null,[pe,t(": "),e("a",ge,[t("http://arxiv.org/abs/2404.02990v1"),i(n)])]),me,fe,ve,e("p",null,[be,t(": "),e("a",ye,[t("http://arxiv.org/abs/2404.02880v1"),i(n)])]),we,_e,ke,e("p",null,[xe,t(": "),e("a",Ae,[t("http://arxiv.org/abs/2404.02806v1"),i(n)])]),Le,Te,Ie,e("p",null,[Me,t(": "),e("a",Se,[t("http://arxiv.org/abs/2404.02798v1"),i(n)])]),Ce,ze,We,e("p",null,[Pe,t(": "),e("a",Re,[t("http://arxiv.org/abs/2404.02743v1"),i(n)])]),He,Ee,De,e("p",null,[qe,t(": "),e("a",Be,[t("http://arxiv.org/abs/2404.02718v1"),i(n)])]),Ge,Ne,Ve,e("p",null,[Oe,t(": "),e("a",je,[t("http://arxiv.org/abs/2404.02706v1"),i(n)])]),Fe,Je,Ue,e("p",null,[Ke,t(": "),e("a",Ze,[t("http://arxiv.org/abs/2404.02565v1"),i(n)])]),Ye,Xe,Qe,e("p",null,[$e,t(": "),e("a",et,[t("http://arxiv.org/abs/2404.02475v1"),i(n)])]),tt,nt,it,e("p",null,[at,t(": "),e("a",ot,[t("http://arxiv.org/abs/2404.02439v1"),i(n)])]),st,rt,lt,e("p",null,[ct,t(": "),e("a",ht,[t("http://arxiv.org/abs/2404.02411v1"),i(n)])]),dt,ut,pt,gt,e("p",null,[mt,t(": "),e("a",ft,[t("http://arxiv.org/abs/2404.02317v1"),i(n)])]),vt,bt,yt,e("p",null,[wt,t(": "),e("a",_t,[t("http://arxiv.org/abs/2404.02277v1"),i(n)])]),kt,xt,At,e("p",null,[Lt,t(": "),e("a",Tt,[t("http://arxiv.org/abs/2404.02213v1"),i(n)])]),It,Mt,St,e("p",null,[Ct,t(": "),e("a",zt,[t("http://arxiv.org/abs/2404.02147v1"),i(n)])]),Wt,Pt,Rt,e("p",null,[Ht,t(": "),e("a",Et,[t("http://arxiv.org/abs/2404.02109v1"),i(n)])]),Dt,qt,Bt,e("p",null,[Gt,t(": "),e("a",Nt,[t("http://arxiv.org/abs/2404.02081v1"),i(n)])]),Vt,Ot,jt,e("p",null,[Ft,t(": "),e("a",Jt,[t("http://arxiv.org/abs/2404.02009v1"),i(n)])]),Ut,Kt,Zt,e("p",null,[Yt,t(": "),e("a",Xt,[t("http://arxiv.org/abs/2404.01997v1"),i(n)])]),Qt,$t,en,e("p",null,[tn,t(": "),e("a",nn,[t("http://arxiv.org/abs/2404.01872v1"),i(n)])]),an,on,sn,e("p",null,[rn,t(": "),e("a",ln,[t("http://arxiv.org/abs/2404.01862v1"),i(n)])]),cn,hn,dn,e("p",null,[un,t(": "),e("a",pn,[t("http://arxiv.org/abs/2404.01848v1"),i(n)])]),gn,mn,fn,e("p",null,[vn,t(": "),e("a",bn,[t("http://arxiv.org/abs/2404.01845v1"),i(n)])]),yn,wn,_n,e("p",null,[kn,t(": "),e("a",xn,[t("http://arxiv.org/abs/2404.01816v1"),i(n)])]),An,Ln,Tn,e("p",null,[In,t(": "),e("a",Mn,[t("http://arxiv.org/abs/2404.01713v1"),i(n)])]),Sn,Cn,zn,e("p",null,[Wn,t(": "),e("a",Pn,[t("http://arxiv.org/abs/2404.01702v1"),i(n)])]),Rn,Hn,En,e("p",null,[Dn,t(": "),e("a",qn,[t("http://arxiv.org/abs/2404.01651v1"),i(n)])]),Bn,Gn,Nn,e("p",null,[Vn,t(": "),e("a",On,[t("http://arxiv.org/abs/2404.01644v1"),i(n)])]),jn,Fn,Jn,e("p",null,[Un,t(": "),e("a",Kn,[t("http://arxiv.org/abs/2404.01622v1"),i(n)])]),Zn,Yn,Xn,e("p",null,[Qn,t(": "),e("a",$n,[t("http://arxiv.org/abs/2404.01615v1"),i(n)])]),ei,ti,ni,e("p",null,[ii,t(": "),e("a",ai,[t("http://arxiv.org/abs/2404.01602v1"),i(n)])]),oi,si,ri,e("p",null,[li,t(": "),e("a",ci,[t("http://arxiv.org/abs/2404.01576v1"),i(n)])]),hi,di,ui,pi,e("p",null,[gi,t(": "),e("a",mi,[t("http://arxiv.org/abs/2404.01527v1"),i(n)])]),fi,vi,bi,e("p",null,[yi,t(": "),e("a",wi,[t("http://arxiv.org/abs/2404.01488v1"),i(n)])]),_i,ki,xi,e("p",null,[Ai,t(": "),e("a",Li,[t("http://arxiv.org/abs/2404.01485v1"),i(n)])]),Ti,Ii,Mi,e("p",null,[Si,t(": "),e("a",Ci,[t("http://arxiv.org/abs/2404.01461v1"),i(n)])]),zi,Wi,Pi,e("p",null,[Ri,t(": "),e("a",Hi,[t("http://arxiv.org/abs/2404.01425v1"),i(n)])]),Ei,Di,qi,e("p",null,[Bi,t(": "),e("a",Gi,[t("http://arxiv.org/abs/2404.01403v1"),i(n)])]),Ni,Vi,Oi,e("p",null,[ji,t(": "),e("a",Fi,[t("http://arxiv.org/abs/2404.01283v1"),i(n)])]),Ji,Ui,Ki,e("p",null,[Zi,t(": "),e("a",Yi,[t("http://arxiv.org/abs/2404.01364v1"),i(n)])]),Xi,Qi,$i,e("p",null,[ea,t(": "),e("a",ta,[t("http://arxiv.org/abs/2404.01250v1"),i(n)])]),na,ia,aa,e("p",null,[oa,t(": "),e("a",sa,[t("http://arxiv.org/abs/2404.01240v1"),i(n)])]),ra,la,ca,e("p",null,[ha,t(": "),e("a",da,[t("http://arxiv.org/abs/2404.01361v1"),i(n)])]),ua,pa,ga,e("p",null,[ma,t(": "),e("a",fa,[t("http://arxiv.org/abs/2404.01063v1"),i(n)])]),va,ba,ya,e("p",null,[wa,t(": "),e("a",_a,[t("http://arxiv.org/abs/2404.01050v1"),i(n)])]),ka,xa,Aa,e("p",null,[La,t(": "),e("a",Ta,[t("http://arxiv.org/abs/2404.00938v1"),i(n)])]),Ia,Ma,Sa,Ca,e("p",null,[za,t(": "),e("a",Wa,[t("http://arxiv.org/abs/2404.00634v1"),i(n)])]),Pa,Ra,Ha,e("p",null,[Ea,t(": "),e("a",Da,[t("http://arxiv.org/abs/2404.00573v1"),i(n)])]),qa,Ba,Ga,e("p",null,[Na,t(": "),e("a",Va,[t("http://arxiv.org/abs/2404.00526v1"),i(n)])]),Oa,ja,Fa,e("p",null,[Ja,t(": "),e("a",Ua,[t("http://arxiv.org/abs/2404.01339v1"),i(n)])]),Ka,Za,Ya,Xa,e("p",null,[Qa,t(": "),e("a",$a,[t("http://arxiv.org/abs/2404.00487v1"),i(n)])]),eo,to,no,e("p",null,[io,t(": "),e("a",ao,[t("http://arxiv.org/abs/2404.00442v1"),i(n)])]),oo,so,ro,e("p",null,[lo,t(": "),e("a",co,[t("http://arxiv.org/abs/2404.00431v1"),i(n)])]),ho,uo,po,e("p",null,[go,t(": "),e("a",mo,[t("http://arxiv.org/abs/2404.00405v1"),i(n)])]),fo,vo,bo,e("p",null,[yo,t(": "),e("a",wo,[t("http://arxiv.org/abs/2404.00392v1"),i(n)])]),_o,ko,xo,e("p",null,[Ao,t(": "),e("a",Lo,[t("http://arxiv.org/abs/2404.00333v1"),i(n)])]),To,Io,Mo,e("p",null,[So,t(": "),e("a",Co,[t("http://arxiv.org/abs/2404.00300v1"),i(n)])]),zo,Wo,Po,e("p",null,[Ro,t(": "),e("a",Ho,[t("http://arxiv.org/abs/2404.00246v1"),i(n)])]),Eo,Do,qo,Bo,e("p",null,[Go,t(": "),e("a",No,[t("http://arxiv.org/abs/2404.00192v1"),i(n)])]),Vo,Oo,jo,e("p",null,[Fo,t(": "),e("a",Jo,[t("http://arxiv.org/abs/2404.00171v1"),i(n)])]),Uo,Ko,Zo,e("p",null,[Yo,t(": "),e("a",Xo,[t("http://arxiv.org/abs/2404.00161v1"),i(n)])]),Qo,$o,es,e("p",null,[ts,t(": "),e("a",ns,[t("http://arxiv.org/abs/2404.00131v1"),i(n)])]),is,as,os,e("p",null,[ss,t(": "),e("a",rs,[t("http://arxiv.org/abs/2403.20246v1"),i(n)])]),ls,cs,hs,e("p",null,[ds,t(": "),e("a",us,[t("http://arxiv.org/abs/2404.01327v1"),i(n)])]),ps,gs,ms,e("p",null,[fs,t(": "),e("a",vs,[t("http://arxiv.org/abs/2403.20097v1"),i(n)])]),bs,ys,ws,e("p",null,[_s,t(": "),e("a",ks,[t("http://arxiv.org/abs/2403.19992v1"),i(n)])]),xs,As,Ls,Ts,e("p",null,[Is,t(": "),e("a",Ms,[t("http://arxiv.org/abs/2403.19876v1"),i(n)])]),Ss,Cs,zs,e("p",null,[Ws,t(": "),e("a",Ps,[t("http://arxiv.org/abs/2403.19763v1"),i(n)])]),Rs,Hs,Es,e("p",null,[Ds,t(": "),e("a",qs,[t("http://arxiv.org/abs/2403.19760v1"),i(n)])]),Bs,Gs,Ns,e("p",null,[Vs,t(": "),e("a",Os,[t("http://arxiv.org/abs/2403.19620v1"),i(n)])]),js,Fs,Js,e("p",null,[Us,t(": "),e("a",Ks,[t("http://arxiv.org/abs/2403.19560v1"),i(n)])]),Zs,Ys,Xs,e("p",null,[Qs,t(": "),e("a",$s,[t("http://arxiv.org/abs/2403.19506v1"),i(n)])]),er,tr,nr,e("p",null,[ir,t(": "),e("a",ar,[t("http://arxiv.org/abs/2403.19475v1"),i(n)])]),or,sr,rr,e("p",null,[lr,t(": "),e("a",cr,[t("http://arxiv.org/abs/2403.19436v1"),i(n)])]),hr,dr,ur,e("p",null,[pr,t(": "),e("a",gr,[t("http://arxiv.org/abs/2403.19339v1"),i(n)])]),mr,fr,vr,e("p",null,[br,t(": "),e("a",yr,[t("http://arxiv.org/abs/2403.19206v1"),i(n)])]),wr,_r,kr,e("p",null,[xr,t(": "),e("a",Ar,[t("http://arxiv.org/abs/2403.19174v1"),i(n)])]),Lr,Tr,Ir,e("p",null,[Mr,t(": "),e("a",Sr,[t("http://arxiv.org/abs/2403.19153v1"),i(n)])]),Cr,zr,Wr,e("p",null,[Pr,t(": "),e("a",Rr,[t("http://arxiv.org/abs/2403.19085v1"),i(n)])]),Hr,Er,Dr,qr,e("p",null,[Br,t(": "),e("a",Gr,[t("http://arxiv.org/abs/2403.19060v1"),i(n)])]),Nr,Vr,Or,e("p",null,[jr,t(": "),e("a",Fr,[t("http://arxiv.org/abs/2403.19040v1"),i(n)])]),Jr,Ur,Kr,e("p",null,[Zr,t(": "),e("a",Yr,[t("http://arxiv.org/abs/2403.19037v1"),i(n)])]),Xr,Qr,$r,e("p",null,[el,t(": "),e("a",tl,[t("http://arxiv.org/abs/2403.19027v1"),i(n)])]),nl,il,al,e("p",null,[ol,t(": "),e("a",sl,[t("http://arxiv.org/abs/2403.19019v1"),i(n)])]),rl,ll,cl,e("p",null,[hl,t(": "),e("a",dl,[t("http://arxiv.org/abs/2403.19014v1"),i(n)])]),ul,pl,gl,e("p",null,[ml,t(": "),e("a",fl,[t("http://arxiv.org/abs/2403.18797v1"),i(n)])]),vl,bl,yl,e("p",null,[wl,t(": "),e("a",_l,[t("http://arxiv.org/abs/2403.18692v1"),i(n)])]),kl,xl,Al,e("p",null,[Ll,t(": "),e("a",Tl,[t("http://arxiv.org/abs/2403.18679v1"),i(n)])]),Il,Ml,Sl,e("p",null,[Cl,t(": "),e("a",zl,[t("http://arxiv.org/abs/2403.18668v1"),i(n)])]),Wl])}const Dl=a(c,[["render",Pl],["__file","HCI.html.vue"]]),ql=JSON.parse('{"path":"/posts/paper/HCI.html","title":"HCI","lang":"en-US","frontmatter":{"description":"HCI 2024-04-04 Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior Authors: Frederick Choi, Charlotte Lambert, Vi...","head":[["meta",{"property":"og:url","content":"https://opendesign.world/posts/paper/HCI.html"}],["meta",{"property":"og:site_name","content":"OpenDesign"}],["meta",{"property":"og:title","content":"HCI"}],["meta",{"property":"og:description","content":"HCI 2024-04-04 Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior Authors: Frederick Choi, Charlotte Lambert, Vi..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-04-07T10:05:54.000Z"}],["meta",{"property":"article:modified_time","content":"2024-04-07T10:05:54.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"HCI\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-04-07T10:05:54.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":2,"title":"2024-04-04","slug":"_2024-04-04","link":"#_2024-04-04","children":[]},{"level":2,"title":"2024-04-03","slug":"_2024-04-03","link":"#_2024-04-03","children":[]},{"level":2,"title":"2024-04-02","slug":"_2024-04-02","link":"#_2024-04-02","children":[]},{"level":2,"title":"2024-04-01","slug":"_2024-04-01","link":"#_2024-04-01","children":[]},{"level":2,"title":"2024-03-31","slug":"_2024-03-31","link":"#_2024-03-31","children":[]},{"level":2,"title":"2024-03-30","slug":"_2024-03-30","link":"#_2024-03-30","children":[]},{"level":2,"title":"2024-03-29","slug":"_2024-03-29","link":"#_2024-03-29","children":[]},{"level":2,"title":"2024-03-28","slug":"_2024-03-28","link":"#_2024-03-28","children":[]},{"level":2,"title":"2024-03-27","slug":"_2024-03-27","link":"#_2024-03-27","children":[]}],"git":{"updatedTime":1712484354000,"contributors":[{"name":"xue160709","email":"xue160709@gmail.com","commits":1}]},"filePathRelative":"posts/paper/HCI.md","autoDesc":true,"excerpt":"\\n<h2>2024-04-04</h2>\\n<h4>Creator Hearts: Investigating the Impact Positive Signals from YouTube Creators in Shaping Comment Section Behavior</h4>\\n<p><strong>Authors</strong>: Frederick Choi, Charlotte Lambert, Vinay Koshy, Sowmya Pratipati, Tue Do, Eshwar Chandrasekharan</p>\\n<p><strong>Link</strong>: <a href=\\"http://arxiv.org/abs/2404.03612v1\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">http://arxiv.org/abs/2404.03612v1</a></p>"}');export{Dl as comp,ql as data};
