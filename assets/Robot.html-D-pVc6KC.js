import{_ as a,r as i,o as r,c as s,a as e,b as t,d as n,e as l}from"./app-b0nPsb4c.js";const c={},h=l('<h1 id="robot" tabindex="-1"><a class="header-anchor" href="#robot"><span>Robot</span></a></h1><h2 id="_2024-04-11" tabindex="-1"><a class="header-anchor" href="#_2024-04-11"><span>2024-04-11</span></a></h2><h4 id="multi-robot-target-tracking-with-sensing-and-communication-danger-zones" tabindex="-1"><a class="header-anchor" href="#multi-robot-target-tracking-with-sensing-and-communication-danger-zones"><span>Multi-Robot Target Tracking with Sensing and Communication Danger Zones</span></a></h4><p><strong>Authors</strong>: Jiazhen Li, Peihan Li, Yuwei Wu, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou</p>',4),d=e("strong",null,"Link",-1),u={href:"http://arxiv.org/abs/2404.07880v1",target:"_blank",rel:"noopener noreferrer"},p=e("p",null,[e("strong",null,"Abstract"),t(": Multi-robot target tracking finds extensive applications in different scenarios, such as environmental surveillance and wildfire management, which require the robustness of the practical deployment of multi-robot systems in uncertain and dangerous environments. Traditional approaches often focus on the performance of tracking accuracy with no modeling and assumption of the environments, neglecting potential environmental hazards which result in system failures in real-world deployments. To address this challenge, we investigate multi-robot target tracking in the adversarial environment considering sensing and communication attacks with uncertainty. We design specific strategies to avoid different danger zones and proposed a multi-agent tracking framework under the perilous environment. We approximate the probabilistic constraints and formulate practical optimization strategies to address computational challenges efficiently. We evaluate the performance of our proposed methods in simulations to demonstrate the ability of robots to adjust their risk-aware behaviors under different levels of environmental uncertainty and risk confidence. The proposed method is further validated via real-world robot experiments where a team of drones successfully track dynamic ground robots while being risk-aware of the sensing and/or communication danger zones.")],-1),g=e("h4",{id:"from-the-lab-to-the-theater-an-unconventional-field-robotics-journey",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#from-the-lab-to-the-theater-an-unconventional-field-robotics-journey"},[e("span",null,"From the Lab to the Theater: An Unconventional Field Robotics Journey")])],-1),m=e("p",null,[e("strong",null,"Authors"),t(": Ali Imran, Vivek Shankar Varadharajan, Rafael Gomes Braga, Yann Bouteiller, Abdalwhab Bakheet Mohamed Abdalwhab, Matthis Di-Giacomo, Alexandra Mercader, Giovanni Beltrame, David St-Onge")],-1),b=e("strong",null,"Link",-1),f={href:"http://arxiv.org/abs/2404.07795v1",target:"_blank",rel:"noopener noreferrer"},v=e("p",null,[e("strong",null,"Abstract"),t(": Artistic performances involving robotic systems present unique technical challenges akin to those encountered in other field deployments. In this paper, we delve into the orchestration of robotic artistic performances, focusing on the complexities inherent in communication protocols and localization methods. Through our case studies and experimental insights, we demonstrate the breadth of technical requirements for this type of deployment, and, most importantly, the significant contributions of working closely with non-experts.")],-1),y=e("h4",{id:"sketch-plan-generalize-continual-few-shot-learning-of-inductively-generalizable-spatial-concepts-for-language-guided-robot-manipulation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sketch-plan-generalize-continual-few-shot-learning-of-inductively-generalizable-spatial-concepts-for-language-guided-robot-manipulation"},[e("span",null,"Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively Generalizable Spatial Concepts for Language-Guided Robot Manipulation")])],-1),w=e("p",null,[e("strong",null,"Authors"),t(": Namasivayam Kalithasan, Sachit Sachdeva, Himanshu Gaurav Singh, Divyanshu Aggarwal, Gurarmaan Singh Panjeta, Vishal Bindal, Arnav Tuli, Rohan Paul, Parag Singla")],-1),_=e("strong",null,"Link",-1),k={href:"http://arxiv.org/abs/2404.07774v1",target:"_blank",rel:"noopener noreferrer"},x=e("p",null,[e("strong",null,"Abstract"),t(": Our goal is to build embodied agents that can learn inductively generalizable spatial concepts in a continual manner, e.g, constructing a tower of a given height. Existing work suffers from certain limitations (a) (Liang et al., 2023) and their multi-modal extensions, rely heavily on prior knowledge and are not grounded in the demonstrations (b) (Liu et al., 2023) lack the ability to generalize due to their purely neural approach. A key challenge is to achieve a fine balance between symbolic representations which have the capability to generalize, and neural representations that are physically grounded. In response, we propose a neuro-symbolic approach by expressing inductive concepts as symbolic compositions over grounded neural concepts. Our key insight is to decompose the concept learning problem into the following steps 1) Sketch: Getting a programmatic representation for the given instruction 2) Plan: Perform Model-Based RL over the sequence of grounded neural action concepts to learn a grounded plan 3) Generalize: Abstract out a generic (lifted) Python program to facilitate generalizability. Continual learning is achieved by interspersing learning of grounded neural concepts with higher level symbolic constructs. Our experiments demonstrate that our approach significantly outperforms existing baselines in terms of its ability to learn novel concepts and generalize inductively.")],-1),A=e("h4",{id:"diffusing-in-someone-else-s-shoes-robotic-perspective-taking-with-diffusion",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#diffusing-in-someone-else-s-shoes-robotic-perspective-taking-with-diffusion"},[e("span",null,"Diffusing in Someone Else's Shoes: Robotic Perspective Taking with Diffusion")])],-1),L=e("p",null,[e("strong",null,"Authors"),t(": Josua Spisak, Matthias Kerzel, Stefan Wermter")],-1),T=e("strong",null,"Link",-1),R={href:"http://arxiv.org/abs/2404.07735v1",target:"_blank",rel:"noopener noreferrer"},S=e("p",null,[e("strong",null,"Abstract"),t(": Humanoid robots can benefit from their similarity to the human shape by learning from humans. When humans teach other humans how to perform actions, they often demonstrate the actions and the learning human can try to imitate the demonstration. Being able to mentally transfer from a demonstration seen from a third-person perspective to how it should look from a first-person perspective is fundamental for this ability in humans. As this is a challenging task, it is often simplified for robots by creating a demonstration in the first-person perspective. Creating these demonstrations requires more effort but allows for an easier imitation. We introduce a novel diffusion model aimed at enabling the robot to directly learn from the third-person demonstrations. Our model is capable of learning and generating the first-person perspective from the third-person perspective by translating the size and rotations of objects and the environment between two perspectives. This allows us to utilise the benefits of easy-to-produce third-person demonstrations and easy-to-imitate first-person demonstrations. The model can either represent the first-person perspective in an RGB image or calculate the joint values. Our approach significantly outperforms other image-to-image models in this task.")],-1),M=e("h4",{id:"reflectance-estimation-for-proximity-sensing-by-vision-language-models-utilizing-distributional-semantics-for-low-level-cognition-in-robotics",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#reflectance-estimation-for-proximity-sensing-by-vision-language-models-utilizing-distributional-semantics-for-low-level-cognition-in-robotics"},[e("span",null,"Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics")])],-1),z=e("p",null,[e("strong",null,"Authors"),t(": Masashi Osada, Gustavo A. Garcia Ricardez, Yosuke Suzuki, Tadahiro Taniguchi")],-1),P=e("strong",null,"Link",-1),j={href:"http://arxiv.org/abs/2404.07717v1",target:"_blank",rel:"noopener noreferrer"},C=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) and vision-language models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored. In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors. We investigate whether LLMs can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation. In this paper, we verify that 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images. Our experiments show that GPT-4 can estimate an object's reflectance using only text input with a mean error of 14.7%, lower than the image-only ResNet. Moreover, CLIP achieved the lowest mean error of 11.8%, while GPT-3.5 obtained a competitive 19.9% compared to ResNet's 17.8%. These results suggest that the distributional semantics in LLMs and VLMs increases their generalization capabilities, and the knowledge acquired by VLMs benefits from the latent structure of language.")],-1),D=e("h4",{id:"safe-haptic-teleoperations-of-admittance-controlled-robots-with-virtualization-of-the-force-feedback",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#safe-haptic-teleoperations-of-admittance-controlled-robots-with-virtualization-of-the-force-feedback"},[e("span",null,"Safe haptic teleoperations of admittance controlled robots with virtualization of the force feedback")])],-1),I=e("p",null,[e("strong",null,"Authors"),t(": Lorenzo Pagliara, Enrico Ferrentino, Andrea Chiacchio, Giovanni Russo")],-1),W=e("strong",null,"Link",-1),B={href:"http://arxiv.org/abs/2404.07672v1",target:"_blank",rel:"noopener noreferrer"},O=e("p",null,[e("strong",null,"Abstract"),t(": Haptic teleoperations play a key role in extending human capabilities to perform complex tasks remotely, employing a robotic system. The impact of haptics is far-reaching and can improve the sensory awareness and motor accuracy of the operator. In this context, a key challenge is attaining a natural, stable and safe haptic human-robot interaction. Achieving these conflicting requirements is particularly crucial for complex procedures, e.g. medical ones. To address this challenge, in this work we develop a novel haptic bilateral teleoperation system (HBTS), featuring a virtualized force feedback, based on the motion error generated by an admittance controlled robot. This approach allows decoupling the force rendering system from the control of the interaction: the rendered force is assigned with the desired dynamics, while the admittance control parameters are separately tuned to maximize interaction performance. Furthermore, recognizing the necessity to limit the forces exerted by the robot on the environment, to ensure a safe interaction, we embed a saturation strategy of the motion references provided by the haptic device to admittance control. We validate the different aspects of the proposed HBTS, through a teleoperated blackboard writing experiment, against two other architectures. The results indicate that the proposed HBTS improves the naturalness of teleoperation, as well as safety and accuracy of the interaction.")],-1),q=e("h4",{id:"weakly-supervised-learning-via-multi-lateral-decoder-branching-for-guidewire-segmentation-in-robot-assisted-cardiovascular-catheterization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#weakly-supervised-learning-via-multi-lateral-decoder-branching-for-guidewire-segmentation-in-robot-assisted-cardiovascular-catheterization"},[e("span",null,"Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Guidewire Segmentation in Robot-Assisted Cardiovascular Catheterization")])],-1),F=e("p",null,[e("strong",null,"Authors"),t(": Olatunji Mumini Omisore, Toluwanimi Akinyemi, Anh Nguyen, Lei Wang")],-1),H=e("strong",null,"Link",-1),E={href:"http://arxiv.org/abs/2404.07594v1",target:"_blank",rel:"noopener noreferrer"},G=e("p",null,[e("strong",null,"Abstract"),t(": Although robot-assisted cardiovascular catheterization is commonly performed for intervention of cardiovascular diseases, more studies are needed to support the procedure with automated tool segmentation. This can aid surgeons on tool tracking and visualization during intervention. Learning-based segmentation has recently offered state-of-the-art segmentation performances however, generating ground-truth signals for fully-supervised methods is labor-intensive and time consuming for the interventionists. In this study, a weakly-supervised learning method with multi-lateral pseudo labeling is proposed for tool segmentation in cardiac angiograms. The method includes a modified U-Net model with one encoder and multiple lateral-branched decoders that produce pseudo labels as supervision signals under different perturbation. The pseudo labels are self-generated through a mixed loss function and shared consistency in the decoders. We trained the model end-to-end with weakly-annotated data obtained during robotic cardiac catheterization. Experiments with the proposed model shows weakly annotated data has closer performance to when fully annotated data is used. Compared to three existing weakly-supervised methods, our approach yielded higher segmentation performance across three different cardiac angiogram data. With ablation study, we showed consistent performance under different parameters. Thus, we offer a less expensive method for real-time tool segmentation and tracking during robot-assisted cardiac catheterization.")],-1),N=e("h4",{id:"differentiable-rendering-as-a-way-to-program-cable-driven-soft-robots",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#differentiable-rendering-as-a-way-to-program-cable-driven-soft-robots"},[e("span",null,"Differentiable Rendering as a Way to Program Cable-Driven Soft Robots")])],-1),K=e("p",null,[e("strong",null,"Authors"),t(": Kasra Arnavaz, Kenny Erleben")],-1),J=e("strong",null,"Link",-1),Y={href:"http://arxiv.org/abs/2404.07590v1",target:"_blank",rel:"noopener noreferrer"},V=e("p",null,[e("strong",null,"Abstract"),t(": Soft robots have gained increased popularity in recent years due to their adaptability and compliance. In this paper, we use a digital twin model of cable-driven soft robots to learn control parameters in simulation. In doing so, we take advantage of differentiable rendering as a way to instruct robots to complete tasks such as point reach, gripping an object, and obstacle avoidance. This approach simplifies the mathematical description of such complicated tasks and removes the need for landmark points and their tracking. Our experiments demonstrate the applicability of our method.")],-1),U=e("h4",{id:"socially-pertinent-robots-in-gerontological-healthcare",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#socially-pertinent-robots-in-gerontological-healthcare"},[e("span",null,"Socially Pertinent Robots in Gerontological Healthcare")])],-1),Z=e("p",null,[e("strong",null,"Authors"),t(": Xavier Alameda-Pineda, Angus Addlesee, Daniel Hernández García, Chris Reinke, Soraya Arias, Federica Arrigoni, Alex Auternaud, Lauriane Blavette, Cigdem Beyan, Luis Gomez Camara, Ohad Cohen, Alessandro Conti, Sébastien Dacunha, Christian Dondrup, Yoav Ellinson, Francesco Ferro, Sharon Gannot, Florian Gras, Nancie Gunson, Radu Horaud, Moreno D'Incà, Imad Kimouche, Séverin Lemaignan, Oliver Lemon, Cyril Liotard, Luca Marchionni, Mordehay Moradi, Tomas Pajdla, Maribel Pino, Michal Polic, Matthieu Py, Ariel Rado, Bin Ren, Elisa Ricci, Anne-Sophie Rigaud, Paolo Rota, Marta Romeo, Nicu Sebe, Weronika Sieińska, Pinchas Tandeitnik, Francesco Tonini, Nicolas Turro, Timothée Wintz, Yanchao Yu")],-1),$=e("strong",null,"Link",-1),X={href:"http://arxiv.org/abs/2404.07560v1",target:"_blank",rel:"noopener noreferrer"},Q=e("p",null,[e("strong",null,"Abstract"),t(": Despite the many recent achievements in developing and deploying social robotics, there are still many underexplored environments and applications for which systematic evaluation of such systems by end-users is necessary. While several robotic platforms have been used in gerontological healthcare, the question of whether or not a social interactive robot with multi-modal conversational capabilities will be useful and accepted in real-life facilities is yet to be answered. This paper is an attempt to partially answer this question, via two waves of experiments with patients and companions in a day-care gerontological facility in Paris with a full-sized humanoid robot endowed with social and conversational interaction capabilities. The software architecture, developed during the H2020 SPRING project, together with the experimental protocol, allowed us to evaluate the acceptability (AES) and usability (SUS) with more than 60 end-users. Overall, the users are receptive to this technology, especially when the robot perception and action skills are robust to environmental clutter and flexible to handle a plethora of different interactions.")],-1),ee=e("h4",{id:"model-predictive-trajectory-planning-for-human-robot-handovers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#model-predictive-trajectory-planning-for-human-robot-handovers"},[e("span",null,"Model Predictive Trajectory Planning for Human-Robot Handovers")])],-1),te=e("p",null,[e("strong",null,"Authors"),t(": Thies Oelerich, Christian Hartl-Nesic, Andreas Kugi")],-1),oe=e("strong",null,"Link",-1),ne={href:"http://arxiv.org/abs/2404.07505v1",target:"_blank",rel:"noopener noreferrer"},ae=e("p",null,[e("strong",null,"Abstract"),t(": This work develops a novel trajectory planner for human-robot handovers. The handover requirements can naturally be handled by a path-following-based model predictive controller, where the path progress serves as a progress measure of the handover. Moreover, the deviations from the path are used to follow human motion by adapting the path deviation bounds with a handover location prediction. A Gaussian process regression model, which is trained on known handover trajectories, is employed for this prediction. Experiments with a collaborative 7-DoF robotic manipulator show the effectiveness and versatility of the proposed approach.")],-1),ie=e("h4",{id:"adademo-data-efficient-demonstration-expansion-for-generalist-robotic-agent",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#adademo-data-efficient-demonstration-expansion-for-generalist-robotic-agent"},[e("span",null,"AdaDemo: Data-Efficient Demonstration Expansion for Generalist Robotic Agent")])],-1),re=e("p",null,[e("strong",null,"Authors"),t(": Tongzhou Mu, Yijie Guo, Jie Xu, Ankit Goyal, Hao Su, Dieter Fox, Animesh Garg")],-1),se=e("strong",null,"Link",-1),le={href:"http://arxiv.org/abs/2404.07428v1",target:"_blank",rel:"noopener noreferrer"},ce=e("p",null,[e("strong",null,"Abstract"),t(": Encouraged by the remarkable achievements of language and vision foundation models, developing generalist robotic agents through imitation learning, using large demonstration datasets, has become a prominent area of interest in robot learning. The efficacy of imitation learning is heavily reliant on the quantity and quality of the demonstration datasets. In this study, we aim to scale up demonstrations in a data-efficient way to facilitate the learning of generalist robotic agents. We introduce AdaDemo (Adaptive Online Demonstration Expansion), a general framework designed to improve multi-task policy learning by actively and continually expanding the demonstration dataset. AdaDemo strategically collects new demonstrations to address the identified weakness in the existing policy, ensuring data efficiency is maximized. Through a comprehensive evaluation on a total of 22 tasks across two robotic manipulation benchmarks (RLBench and Adroit), we demonstrate AdaDemo's capability to progressively improve policy performance by guiding the generation of high-quality demonstration datasets in a data-efficient manner.")],-1),he=e("h4",{id:"too-good-to-be-true-people-reject-free-gifts-from-robots-because-they-infer-bad-intentions",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#too-good-to-be-true-people-reject-free-gifts-from-robots-because-they-infer-bad-intentions"},[e("span",null,"Too good to be true: People reject free gifts from robots because they infer bad intentions")])],-1),de=e("p",null,[e("strong",null,"Authors"),t(": Benjamin Lebrun, Andrew Vonasch, Christoph Bartneck")],-1),ue=e("strong",null,"Link",-1),pe={href:"http://arxiv.org/abs/2404.07409v1",target:"_blank",rel:"noopener noreferrer"},ge=e("p",null,[e("strong",null,"Abstract"),t(": A recent psychology study found that people sometimes reject overly generous offers from people because they imagine hidden ''phantom costs'' must be part of the transaction. Phantom costs occur when a person seems overly generous for no apparent reason. This study aims to explore whether people can imagine phantom costs when interacting with a robot. To this end, screen or physically embodied agents (human or robot) offered to people either a cookie or a cookie + $2. Participants were then asked to make a choice whether they would accept or decline the offer. Results showed that people did perceive phantom costs in the offer + $2 conditions when interacting with a human, but also with a robot, across both embodiment levels, leading to the characteristic behavioral effect that offering more money made people less likely to accept the offer. While people were more likely to accept offers from a robot than from a human, people more often accepted offers from humans when they were physically compared to screen embodied but were equally likely to accept the offer from a robot whether it was screen or physically embodied. This suggests that people can treat robots (and humans) as social agents with hidden intentions and knowledge, and that this influences their behavior toward them. This provides not only new insights on how people make decisions when interacting with a robot but also how robot embodiment impacts HRI research.")],-1),me=e("h2",{id:"_2024-04-10",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-10"},[e("span",null,"2024-04-10")])],-1),be=e("h4",{id:"enhancing-accessibility-in-soft-robotics-exploring-magnet-embedded-paper-based-interactions",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-accessibility-in-soft-robotics-exploring-magnet-embedded-paper-based-interactions"},[e("span",null,"Enhancing Accessibility in Soft Robotics: Exploring Magnet-Embedded Paper-Based Interactions")])],-1),fe=e("p",null,[e("strong",null,"Authors"),t(": Ruhan Yang, Ellen Yi-Luen Do")],-1),ve=e("strong",null,"Link",-1),ye={href:"http://arxiv.org/abs/2404.07360v1",target:"_blank",rel:"noopener noreferrer"},we=e("p",null,[e("strong",null,"Abstract"),t(": This paper explores the implementation of embedded magnets to enhance paper-based interactions. The integration of magnets in paper-based interactions simplifies the fabrication process, making it more accessible for building soft robotics systems. We discuss various interaction patterns achievable through this approach and highlight their potential applications.")],-1),_e=e("h4",{id:"interactive-learning-of-physical-object-properties-through-robot-manipulation-and-database-of-object-measurements",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#interactive-learning-of-physical-object-properties-through-robot-manipulation-and-database-of-object-measurements"},[e("span",null,"Interactive Learning of Physical Object Properties Through Robot Manipulation and Database of Object Measurements")])],-1),ke=e("p",null,[e("strong",null,"Authors"),t(": Andrej Kruzliak, Jiri Hartvich, Shubhan P. Patni, Lukas Rustler, Jan Kristof Behrens, Fares J. Abu-Dakka, Krystian Mikolajczyk, Ville Kyrki, Matej Hoffmann")],-1),xe=e("strong",null,"Link",-1),Ae={href:"http://arxiv.org/abs/2404.07344v1",target:"_blank",rel:"noopener noreferrer"},Le=e("p",null,[e("strong",null,"Abstract"),t(": This work presents a framework for automatically extracting physical object properties, such as material composition, mass, volume, and stiffness, through robot manipulation and a database of object measurements. The framework involves exploratory action selection to maximize learning about objects on a table. A Bayesian network models conditional dependencies between object properties, incorporating prior probability distributions and uncertainty associated with measurement actions. The algorithm selects optimal exploratory actions based on expected information gain and updates object properties through Bayesian inference. Experimental evaluation demonstrates effective action selection compared to a baseline and correct termination of the experiments if there is nothing more to be learned. The algorithm proved to behave intelligently when presented with trick objects with material properties in conflict with their appearance. The robot pipeline integrates with a logging module and an online database of objects, containing over 24,000 measurements of 63 objects with different grippers. All code and data are publicly available, facilitating automatic digitization of objects and their physical properties through exploratory manipulations.")],-1),Te=e("h4",{id:"using-neural-networks-to-model-hysteretic-kinematics-in-tendon-actuated-continuum-robots",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#using-neural-networks-to-model-hysteretic-kinematics-in-tendon-actuated-continuum-robots"},[e("span",null,"Using Neural Networks to Model Hysteretic Kinematics in Tendon-Actuated Continuum Robots")])],-1),Re=e("p",null,[e("strong",null,"Authors"),t(": Yuan Wang, Max McCandless, Abdulhamit Donder, Giovanni Pittiglio, Behnam Moradkhani, Yash Chitalia, Pierre E. Dupont")],-1),Se=e("strong",null,"Link",-1),Me={href:"http://arxiv.org/abs/2404.07168v1",target:"_blank",rel:"noopener noreferrer"},ze=e("p",null,[e("strong",null,"Abstract"),t(": The ability to accurately model mechanical hysteretic behavior in tendon-actuated continuum robots using deep learning approaches is a growing area of interest. In this paper, we investigate the hysteretic response of two types of tendon-actuated continuum robots and, ultimately, compare three types of neural network modeling approaches with both forward and inverse kinematic mappings: feedforward neural network (FNN), FNN with a history input buffer, and long short-term memory (LSTM) network. We seek to determine which model best captures temporal dependent behavior. We find that, depending on the robot's design, choosing different kinematic inputs can alter whether hysteresis is exhibited by the system. Furthermore, we present the results of the model fittings, revealing that, in contrast to the standard FNN, both FNN with a history input buffer and the LSTM model exhibit the capacity to model historical dependence with comparable performance in capturing rate-dependent hysteresis.")],-1),Pe=e("h4",{id:"cbfkit-a-control-barrier-function-toolbox-for-robotics-applications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#cbfkit-a-control-barrier-function-toolbox-for-robotics-applications"},[e("span",null,"CBFKIT: A Control Barrier Function Toolbox for Robotics Applications")])],-1),je=e("p",null,[e("strong",null,"Authors"),t(": Mitchell Black, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, Danil Prokhorov")],-1),Ce=e("strong",null,"Link",-1),De={href:"http://arxiv.org/abs/2404.07158v1",target:"_blank",rel:"noopener noreferrer"},Ie=e("p",null,[e("strong",null,"Abstract"),t(": This paper introduces CBFKit, a Python/ROS toolbox for safe robotics planning and control under uncertainty. The toolbox provides a general framework for designing control barrier functions for mobility systems within both deterministic and stochastic environments. It can be connected to the ROS open-source robotics middleware, allowing for the setup of multi-robot applications, encoding of environments and maps, and integrations with predictive motion planning algorithms. Additionally, it offers multiple CBF variations and algorithms for robot control. The CBFKit is demonstrated on the Toyota Human Support Robot (HSR) in both simulation and in physical experiments.")],-1),We=e("h4",{id:"deep-reinforcement-learning-for-mobile-robot-path-planning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#deep-reinforcement-learning-for-mobile-robot-path-planning"},[e("span",null,"Deep Reinforcement Learning for Mobile Robot Path Planning")])],-1),Be=e("p",null,[e("strong",null,"Authors"),t(": Hao Liu, Yi Shen, Shuangjiang Yu, Zijun Gao, Tong Wu")],-1),Oe=e("strong",null,"Link",-1),qe={href:"http://arxiv.org/abs/2404.06974v1",target:"_blank",rel:"noopener noreferrer"},Fe=e("p",null,[e("strong",null,"Abstract"),t(": Path planning is an important problem with the the applications in many aspects, such as video games, robotics etc. This paper proposes a novel method to address the problem of Deep Reinforcement Learning (DRL) based path planning for a mobile robot. We design DRL-based algorithms, including reward functions, and parameter optimization, to avoid time-consuming work in a 2D environment. We also designed an Two-way search hybrid A* algorithm to improve the quality of local path planning. We transferred the designed algorithm to a simple embedded environment to test the computational load of the algorithm when running on a mobile robot. Experiments show that when deployed on a robot platform, the DRL-based algorithm in this article can achieve better planning results and consume less computing resources.")],-1),He=e("h4",{id:"robotic-learning-for-adaptive-informative-path-planning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robotic-learning-for-adaptive-informative-path-planning"},[e("span",null,"Robotic Learning for Adaptive Informative Path Planning")])],-1),Ee=e("p",null,[e("strong",null,"Authors"),t(": Marija Popovic, Joshua Ott, Julius Rückin, Mykel J. Kochendorfer")],-1),Ge=e("strong",null,"Link",-1),Ne={href:"http://arxiv.org/abs/2404.06940v1",target:"_blank",rel:"noopener noreferrer"},Ke=e("p",null,[e("strong",null,"Abstract"),t(": Adaptive informative path planning (AIPP) is important to many robotics applications, enabling mobile robots to efficiently collect useful data about initially unknown environments. In addition, learning-based methods are increasingly used in robotics to enhance adaptability, versatility, and robustness across diverse and complex tasks. Our survey explores research on applying robotic learning to AIPP, bridging the gap between these two research fields. We begin by providing a unified mathematical framework for general AIPP problems. Next, we establish two complementary taxonomies of current work from the perspectives of (i) learning algorithms and (ii) robotic applications. We explore synergies, recent trends, and highlight the benefits of learning-based methods in AIPP frameworks. Finally, we discuss key challenges and promising future directions to enable more generally applicable and robust robotic data-gathering systems through learning. We provide a comprehensive catalogue of papers reviewed in our survey, including publicly available repositories, to facilitate future studies in the field.")],-1),Je=e("h4",{id:"vision-language-model-based-physical-reasoning-for-robot-liquid-perception",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#vision-language-model-based-physical-reasoning-for-robot-liquid-perception"},[e("span",null,"Vision-Language Model-based Physical Reasoning for Robot Liquid Perception")])],-1),Ye=e("p",null,[e("strong",null,"Authors"),t(": Wenqiang Lai, Yuan Gao, Tin Lun Lam")],-1),Ve=e("strong",null,"Link",-1),Ue={href:"http://arxiv.org/abs/2404.06904v1",target:"_blank",rel:"noopener noreferrer"},Ze=e("p",null,[e("strong",null,"Abstract"),t(": There is a growing interest in applying large language models (LLMs) in robotic tasks, due to their remarkable reasoning ability and extensive knowledge learned from vast training corpora. Grounding LLMs in the physical world remains an open challenge as they can only process textual input. Recent advancements in large vision-language models (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone. In this work, we proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback. Specifically, we exploited the physical understanding of GPT-4V to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling multimodal perception beyond vision and language using images as proxies. We evaluated our method using 10 common household liquids with containers of various geometry and material. Without any training or fine-tuning, we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity. We also showed that by jointly reasoning over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0% -- achieved by the best-performing vision-only variant -- to 86.0%.")],-1),$e=e("h4",{id:"sound-matters-auditory-detectability-of-mobile-robots",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sound-matters-auditory-detectability-of-mobile-robots"},[e("span",null,"Sound Matters: Auditory Detectability of Mobile Robots")])],-1),Xe=e("p",null,[e("strong",null,"Authors"),t(": Subham Agrawal, Marlene Wessels, Jorge de Heuvel, Johannes Kraus, Maren Bennewitz")],-1),Qe=e("strong",null,"Link",-1),et={href:"http://arxiv.org/abs/2404.06807v1",target:"_blank",rel:"noopener noreferrer"},tt=e("p",null,[e("strong",null,"Abstract"),t(": Mobile robots are increasingly being used in noisy environments for social purposes, e.g. to provide support in healthcare or public spaces. Since these robots also operate beyond human sight, the question arises as to how different robot types, ambient noise or cognitive engagement impacts the detection of the robots by their sound. To address this research gap, we conducted a user study measuring auditory detection distances for a wheeled (Turtlebot 2i) and quadruped robot (Unitree Go 1), which emit different consequential sounds when moving. Additionally, we also manipulated background noise levels and participants' engagement in a secondary task during the study. Our results showed that the quadruped robot sound was detected significantly better (i.e., at a larger distance) than the wheeled one, which demonstrates that the movement mechanism has a meaningful impact on the auditory detectability. The detectability for both robots diminished significantly as background noise increased. But even in high background noise, participants detected the quadruped robot at a significantly larger distance. The engagement in a secondary task had hardly any impact. In essence, these findings highlight the critical role of distinguishing auditory characteristics of different robots to improve the smooth human-centered navigation of mobile robots in noisy environments.")],-1),ot=e("h4",{id:"designing-fluid-exuding-cartilage-for-biomimetic-robots-mimicking-human-joint-lubrication-function",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#designing-fluid-exuding-cartilage-for-biomimetic-robots-mimicking-human-joint-lubrication-function"},[e("span",null,"Designing Fluid-Exuding Cartilage for Biomimetic Robots Mimicking Human Joint Lubrication Function")])],-1),nt=e("p",null,[e("strong",null,"Authors"),t(": Akihiro Miki, Yuta Sahara, Kazuhiro Miyama, Shunnosuke Yoshimura, Yoshimoto Ribayashi, Shun Hasegawa, Kento Kawaharazuka, Kei Okada, Masayuki Inaba")],-1),at=e("strong",null,"Link",-1),it={href:"http://arxiv.org/abs/2404.06740v1",target:"_blank",rel:"noopener noreferrer"},rt=e("p",null,[e("strong",null,"Abstract"),t(": The human joint is an open-type joint composed of bones, cartilage, ligaments, synovial fluid, and joint capsule, having advantages of flexibility and impact resistance. However, replicating this structure in robots introduces friction challenges due to the absence of bearings. To address this, our study focuses on mimicking the fluid-exuding function of human cartilage. We employ a rubber-based 3D printing technique combined with absorbent materials to create a versatile and easily designed cartilage sheet for biomimetic robots. We evaluate both the fluid-exuding function and friction coefficient of the fabricated flat cartilage sheet. Furthermore, we practically create a piece of curved cartilage and an open-type biomimetic ball joint in combination with bones, ligaments, synovial fluid, and joint capsule to demonstrate the utility of the proposed cartilage sheet in the construction of such joints.")],-1),st=e("h4",{id:"fast-and-accurate-relative-motion-tracking-for-two-industrial-robots",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#fast-and-accurate-relative-motion-tracking-for-two-industrial-robots"},[e("span",null,"Fast and Accurate Relative Motion Tracking for Two Industrial Robots")])],-1),lt=e("p",null,[e("strong",null,"Authors"),t(": Honglu He, Chen-lung Lu, Glenn Saunders, Pinghai Yang, Jeffrey Schoonover, John Wason, Santiago Paternain, Agung Julius, John T. Wen")],-1),ct=e("strong",null,"Link",-1),ht={href:"http://arxiv.org/abs/2404.06687v1",target:"_blank",rel:"noopener noreferrer"},dt=e("p",null,[e("strong",null,"Abstract"),t(": Industrial robotic applications such as spraying, welding, and additive manufacturing frequently require fast, accurate, and uniform motion along a 3D spatial curve. To increase process throughput, some manufacturers propose a dual-robot setup to overcome the speed limitation of a single robot. Industrial robot motion is programmed through waypoints connected by motion primitives (Cartesian linear and circular paths and linear joint paths at constant Cartesian speed). The actual robot motion is affected by the blending between these motion primitives and the pose of the robot (an outstretched/close to singularity pose tends to have larger path-tracking errors). Choosing the waypoints and the speed along each motion segment to achieve the performance requirement is challenging. At present, there is no automated solution, and laborious manual tuning by robot experts is needed to approach the desired performance. In this paper, we present a systematic three-step approach to designing and programming a dual-robot system to optimize system performance. The first step is to select the relative placement between the two robots based on the specified relative motion path. The second step is to select the relative waypoints and the motion primitives. The final step is to update the waypoints iteratively based on the actual relative motion. Waypoint iteration is first executed in simulation and then completed using the actual robots. For performance measures, we use the mean path speed subject to the relative position and orientation constraints and the path speed uniformity constraint. We have demonstrated the effectiveness of this method with ABB and FANUC robots on two challenging test curves. The performance improvement over the current industrial practice baseline is over 300%. Compared to the optimized single-arm case that we have previously reported, the improvement is over 14%.")],-1),ut=e("h2",{id:"_2024-04-09",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-09"},[e("span",null,"2024-04-09")])],-1),pt=e("h4",{id:"genchip-generating-robot-policy-code-for-high-precision-and-contact-rich-manipulation-tasks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#genchip-generating-robot-policy-code-for-high-precision-and-contact-rich-manipulation-tasks"},[e("span",null,"GenCHiP: Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks")])],-1),gt=e("p",null,[e("strong",null,"Authors"),t(": Kaylee Burns, Ajinkya Jain, Keegan Go, Fei Xia, Michael Stark, Stefan Schaal, Karol Hausman")],-1),mt=e("strong",null,"Link",-1),bt={href:"http://arxiv.org/abs/2404.06645v1",target:"_blank",rel:"noopener noreferrer"},ft=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have been successful at generating robot policy code, but so far these results have been limited to high-level tasks that do not require precise movement. It is an open question how well such approaches work for tasks that require reasoning over contact forces and working within tight success tolerances. We find that, with the right action space, LLMs are capable of successfully generating policies for a variety of contact-rich and high-precision manipulation tasks, even under noisy conditions, such as perceptual errors or grasping inaccuracies. Specifically, we reparameterize the action space to include compliance with constraints on the interaction forces and stiffnesses involved in reaching a target pose. We validate this approach on subtasks derived from the Functional Manipulation Benchmark (FMB) and NIST Task Board Benchmarks. Exposing this action space alongside methods for estimating object poses improves policy generation with an LLM by greater than 3x and 4x when compared to non-compliant action spaces")],-1),vt=e("h4",{id:"counting-objects-in-a-robotic-hand",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#counting-objects-in-a-robotic-hand"},[e("span",null,"Counting Objects in a Robotic Hand")])],-1),yt=e("p",null,[e("strong",null,"Authors"),t(": Francis Tsow, Tianze Chen, Yu Sun")],-1),wt=e("strong",null,"Link",-1),_t={href:"http://arxiv.org/abs/2404.06631v1",target:"_blank",rel:"noopener noreferrer"},kt=e("p",null,[e("strong",null,"Abstract"),t(": A robot performing multi-object grasping needs to sense the number of objects in the hand after grasping. The count plays an important role in determining the robot's next move and the outcome and efficiency of the whole pick-place process. This paper presents a data-driven contrastive learning-based counting classifier with a modified loss function as a simple and effective approach for object counting despite significant occlusion challenges caused by robotic fingers and objects. The model was validated against other models with three different common shapes (spheres, cylinders, and cubes) in simulation and in a real setup. The proposed contrastive learning-based counting approach achieved above 96% accuracy for all three objects in the real setup.")],-1),xt=e("h4",{id:"morpheus-a-multimodal-one-armed-robot-assisted-peeling-system-with-human-users-in-the-loop",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#morpheus-a-multimodal-one-armed-robot-assisted-peeling-system-with-human-users-in-the-loop"},[e("span",null,"MORPHeus: a Multimodal One-armed Robot-assisted Peeling System with Human Users In-the-loop")])],-1),At=e("p",null,[e("strong",null,"Authors"),t(": Ruolin Ye, Yifei Hu, Yuhan, Bian, Luke Kulm, Tapomayukh Bhattacharjee")],-1),Lt=e("strong",null,"Link",-1),Tt={href:"http://arxiv.org/abs/2404.06570v1",target:"_blank",rel:"noopener noreferrer"},Rt=e("p",null,[e("strong",null,"Abstract"),t(": Meal preparation is an important instrumental activity of daily living~(IADL). While existing research has explored robotic assistance in meal preparation tasks such as cutting and cooking, the crucial task of peeling has received less attention. Robot-assisted peeling, conventionally a bimanual task, is challenging to deploy in the homes of care recipients using two wheelchair-mounted robot arms due to ergonomic and transferring challenges. This paper introduces a robot-assisted peeling system utilizing a single robotic arm and an assistive cutting board, inspired by the way individuals with one functional hand prepare meals. Our system incorporates a multimodal active perception module to determine whether an area on the food is peeled, a human-in-the-loop long-horizon planner to perform task planning while catering to a user's preference for peeling coverage, and a compliant controller to peel the food items. We demonstrate the system on 12 food items representing the extremes of different shapes, sizes, skin thickness, surface textures, skin vs flesh colors, and deformability.")],-1),St=e("h4",{id:"large-language-models-to-the-rescue-deadlock-resolution-in-multi-robot-systems",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#large-language-models-to-the-rescue-deadlock-resolution-in-multi-robot-systems"},[e("span",null,"Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems")])],-1),Mt=e("p",null,[e("strong",null,"Authors"),t(": Kunal Garg, Jacob Arkin, Songyuan Zhang, Nicholas Roy, Chuchu Fan")],-1),zt=e("strong",null,"Link",-1),Pt={href:"http://arxiv.org/abs/2404.06413v1",target:"_blank",rel:"noopener noreferrer"},jt=e("p",null,[e("strong",null,"Abstract"),t(": Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks. Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution. We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along. A graph neural network (GNN) based low-level distributed control policy executes the assigned plan. We systematically study various prompting techniques to improve LLM's performance in resolving deadlocks. In particular, as part of prompt engineering, we provide in-context examples for LLMs. We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles. Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS.")],-1),Ct=e("h4",{id:"learning-embeddings-with-centroid-triplet-loss-for-object-identification-in-robotic-grasping",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learning-embeddings-with-centroid-triplet-loss-for-object-identification-in-robotic-grasping"},[e("span",null,"Learning Embeddings with Centroid Triplet Loss for Object Identification in Robotic Grasping")])],-1),Dt=e("p",null,[e("strong",null,"Authors"),t(": Anas Gouda, Max Schwarz, Christopher Reining, Sven Behnke, Alice Kirchheim")],-1),It=e("strong",null,"Link",-1),Wt={href:"http://arxiv.org/abs/2404.06277v1",target:"_blank",rel:"noopener noreferrer"},Bt=e("p",null,[e("strong",null,"Abstract"),t(": Foundation models are a strong trend in deep learning and computer vision. These models serve as a base for applications as they require minor or no further fine-tuning by developers to integrate into their applications. Foundation models for zero-shot object segmentation such as Segment Anything (SAM) output segmentation masks from images without any further object information. When they are followed in a pipeline by an object identification model, they can perform object detection without training. Here, we focus on training such an object identification model. A crucial practical aspect for an object identification model is to be flexible in input size. As object identification is an image retrieval problem, a suitable method should handle multi-query multi-gallery situations without constraining the number of input images (e.g. by having fixed-size aggregation layers). The key solution to train such a model is the centroid triplet loss (CTL), which aggregates image features to their centroids. CTL yields high accuracy, avoids misleading training signals and keeps the model input size flexible. In our experiments, we establish a new state of the art on the ArmBench object identification task, which shows general applicability of our model. We furthermore demonstrate an integrated unseen object detection pipeline on the challenging HOPE dataset, which requires fine-grained detection. There, our pipeline matches and surpasses related methods which have been trained on dataset-specific data.")],-1),Ot=e("h4",{id:"resilient-movement-planning-for-continuum-robots",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#resilient-movement-planning-for-continuum-robots"},[e("span",null,"Resilient Movement Planning for Continuum Robots")])],-1),qt=e("p",null,[e("strong",null,"Authors"),t(": Oxana Shamilyan, Ievgen Kabin, Zoya Dyka, Peter Langendoerfer")],-1),Ft=e("strong",null,"Link",-1),Ht={href:"http://arxiv.org/abs/2404.06178v1",target:"_blank",rel:"noopener noreferrer"},Et=e("p",null,[e("strong",null,"Abstract"),t(": The paper presents an experimental study of resilient path planning for con-tinuum robots taking into account the multi-objective optimisation problem. To do this, we used two well-known algorithms, namely Genetic algorithm and A* algorithm, for path planning and the Analytical Hierarchy Process al-gorithm for paths evaluation. In our experiment Analytical Hierarchy Process algorithm considers four different criteria, i.e. distance, motors damage, me-chanical damage and accuracy each considered to contribute to the resilience of a continuum robot. The use of different criteria is necessary to increasing the time to maintenance operations of the robot. The experiment shows that on the one hand both algorithms can be used in combination with Analytical Hierarchy Process algorithm for multi criteria path-planning, while Genetic algorithm shows superior performance in the comparison of the two algo-rithms.")],-1),Gt=e("h4",{id:"intelligence-and-motion-models-of-continuum-robots-an-overview",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#intelligence-and-motion-models-of-continuum-robots-an-overview"},[e("span",null,"Intelligence and Motion Models of Continuum Robots: an Overview")])],-1),Nt=e("p",null,[e("strong",null,"Authors"),t(": Oxana Shamilyan, Ievgen Kabin, Zoya Dyka, Oleksandr Sudakov, Andrii Cherninskyi, Marcin Brzozowski, Peter Langendoerfer")],-1),Kt=e("strong",null,"Link",-1),Jt={href:"http://arxiv.org/abs/2404.06171v1",target:"_blank",rel:"noopener noreferrer"},Yt=e("p",null,[e("strong",null,"Abstract"),t(": Many technical solutions are bio-inspired. Octopus-inspired robotic arms belong to continuum robots which are used in minimally invasive surgery or for technical system restoration in areas difficult-toaccess. Continuum robot missions are bounded with their motions, whereby the motion of the robots is controlled by humans via wireless communication. In case of a lost connection, robot autonomy is required. Distributed control and distributed decision-making mechanisms based on artificial intelligence approaches can be a promising solution to achieve autonomy of technical systems and to increase their resilience. However these methods are not well investigated yet. Octopuses are the living example of natural distributed intelligence but their learning and decision-making mechanisms are also not fully investigated and understood yet. Our major interest is investigating mechanisms of Distributed Artificial Intelligence as a basis for improving resilience of complex systems. We decided to use a physical continuum robot prototype that is able to perform some basic movements for our research. The idea is to research how a technical system can be empowered to combine movements into sequences of motions by itself. For the experimental investigations a suitable physical prototype has to be selected, its motion control has to be implemented and automated. In this paper, we give an overview combining different fields of research, such as Distributed Artificial Intelligence and continuum robots based on 98 publications. We provide a detailed description of the basic motion control models of continuum robots based on the literature reviewed, discuss different aspects of autonomy and give an overview of physical prototypes of continuum robots.")],-1),Vt=e("h4",{id:"adaptable-recovery-behaviors-in-robotics-a-behavior-trees-and-motion-generators-btmg-approach-for-failure-management",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#adaptable-recovery-behaviors-in-robotics-a-behavior-trees-and-motion-generators-btmg-approach-for-failure-management"},[e("span",null,"Adaptable Recovery Behaviors in Robotics: A Behavior Trees and Motion Generators(BTMG) Approach for Failure Management")])],-1),Ut=e("p",null,[e("strong",null,"Authors"),t(": Faseeh Ahmad, Matthias Mayr, Sulthan Suresh-Fazeela, Volker Kreuger")],-1),Zt=e("strong",null,"Link",-1),$t={href:"http://arxiv.org/abs/2404.06129v1",target:"_blank",rel:"noopener noreferrer"},Xt=e("p",null,[e("strong",null,"Abstract"),t(": In dynamic operational environments, particularly in collaborative robotics, the inevitability of failures necessitates robust and adaptable recovery strategies. Traditional automated recovery strategies, while effective for predefined scenarios, often lack the flexibility required for on-the-fly task management and adaptation to expected failures. Addressing this gap, we propose a novel approach that models recovery behaviors as adaptable robotic skills, leveraging the Behavior Trees and Motion Generators~(BTMG) framework for policy representation. This approach distinguishes itself by employing reinforcement learning~(RL) to dynamically refine recovery behavior parameters, enabling a tailored response to a wide array of failure scenarios with minimal human intervention. We assess our methodology through a series of progressively challenging scenarios within a peg-in-a-hole task, demonstrating the approach's effectiveness in enhancing operational efficiency and task success rates in collaborative robotics settings. We validate our approach using a dual-arm KUKA robot.")],-1),Qt=e("h4",{id:"eve-enabling-anyone-to-train-robot-using-augmented-reality",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#eve-enabling-anyone-to-train-robot-using-augmented-reality"},[e("span",null,"EVE: Enabling Anyone to Train Robot using Augmented Reality")])],-1),eo=e("p",null,[e("strong",null,"Authors"),t(": Jun Wang, Chun-Cheng Chang, Jiafei Duan, Dieter Fox, Ranjay Krishna")],-1),to=e("strong",null,"Link",-1),oo={href:"http://arxiv.org/abs/2404.06089v1",target:"_blank",rel:"noopener noreferrer"},no=e("p",null,[e("strong",null,"Abstract"),t(": The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators. Consequently, only those with access to physical robots produce demonstrations to train robots. To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$). We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.")],-1),ao=e("h4",{id:"_3d-branch-point-cloud-completion-for-robotic-pruning-in-apple-orchards",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_3d-branch-point-cloud-completion-for-robotic-pruning-in-apple-orchards"},[e("span",null,"3D Branch Point Cloud Completion for Robotic Pruning in Apple Orchards")])],-1),io=e("p",null,[e("strong",null,"Authors"),t(": Tian Qiu, Alan Zoubi, Nikolai Spine, Lailiang Cheng, Yu Jiang")],-1),ro=e("strong",null,"Link",-1),so={href:"http://arxiv.org/abs/2404.05953v1",target:"_blank",rel:"noopener noreferrer"},lo=e("p",null,[e("strong",null,"Abstract"),t(": Robotic branch pruning is a significantly growing research area to cope with the shortage of labor force in the context of agriculture. One fundamental requirement in robotic pruning is the perception of detailed geometry and topology of branches. However, the point clouds obtained in agricultural settings often exhibit incompleteness due to several constraints, thereby restricting the accuracy of downstream robotic pruning. In this work, we addressed the issue of point cloud quality through a simulation-based deep neural network, leveraging a Real-to-Simulation (Real2Sim) data generation pipeline that not only eliminates the need for manual parameterization but also guarantees the realism of simulated data. The simulation-based neural network was applied to jointly perform point cloud completion and skeletonization on real-world partial branches, without additional real-world training. The Sim2Real qualitative completion and skeletonization results showed the model's remarkable capability for geometry reconstruction and topology prediction. Additionally, we quantitatively evaluated the Sim2Real performance by comparing branch-level trait characterization errors using raw incomplete data and complete data. The Mean Absolute Error (MAE) reduced by 75% and 8% for branch diameter and branch angle estimation, respectively, using the best complete data, which indicates the effectiveness of the Real2Sim data in a zero-shot generalization setting. The characterization improvements contributed to the precision and efficacy of robotic branch pruning.")],-1),co=e("h4",{id:"robot-safe-planning-in-dynamic-environments-based-on-model-predictive-control-using-control-barrier-function",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robot-safe-planning-in-dynamic-environments-based-on-model-predictive-control-using-control-barrier-function"},[e("span",null,"Robot Safe Planning In Dynamic Environments Based On Model Predictive Control Using Control Barrier Function")])],-1),ho=e("p",null,[e("strong",null,"Authors"),t(": Zetao Lu, Kaijun Feng, Jun Xu, Haoyao Chen, Yunjiang Lou")],-1),uo=e("strong",null,"Link",-1),po={href:"http://arxiv.org/abs/2404.05952v1",target:"_blank",rel:"noopener noreferrer"},go=e("p",null,[e("strong",null,"Abstract"),t(": Implementing obstacle avoidance in dynamic environments is a challenging problem for robots. Model predictive control (MPC) is a popular strategy for dealing with this type of problem, and recent work mainly uses control barrier function (CBF) as hard constraints to ensure that the system state remains in the safe set. However, in crowded scenarios, effective solutions may not be obtained due to infeasibility problems, resulting in degraded controller performance. We propose a new MPC framework that integrates CBF to tackle the issue of obstacle avoidance in dynamic environments, in which the infeasibility problem induced by hard constraints operating over the whole prediction horizon is solved by softening the constraints and introducing exact penalty, prompting the robot to actively seek out new paths. At the same time, generalized CBF is extended as a single-step safety constraint of the controller to enhance the safety of the robot during navigation. The efficacy of the proposed method is first shown through simulation experiments, in which a double-integrator system and a unicycle system are employed, and the proposed method outperforms other controllers in terms of safety, feasibility, and navigation efficiency. Furthermore, real-world experiment on an MR1000 robot is implemented to demonstrate the effectiveness of the proposed method.")],-1),mo=e("h4",{id:"body-design-and-gait-generation-of-chair-type-asymmetrical-tripedal-low-rigidity-robot",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#body-design-and-gait-generation-of-chair-type-asymmetrical-tripedal-low-rigidity-robot"},[e("span",null,"Body Design and Gait Generation of Chair-Type Asymmetrical Tripedal Low-rigidity Robot")])],-1),bo=e("p",null,[e("strong",null,"Authors"),t(": Shintaro Inoue, Kento Kawaharazuka, Kei Okada, Masayuki Inaba")],-1),fo=e("strong",null,"Link",-1),vo={href:"http://arxiv.org/abs/2404.05932v1",target:"_blank",rel:"noopener noreferrer"},yo=e("p",null,[e("strong",null,"Abstract"),t(`: In this study, a chair-type asymmetric tripedal low-rigidity robot was designed based on the three-legged chair character in the movie "Suzume" and its gait was generated. Its body structure consists of three legs that are asymmetric to the body, so it cannot be easily balanced. In addition, the actuator is a servo motor that can only feed-forward rotational angle commands and the sensor can only sense the robot's posture quaternion. In such an asymmetric and imperfect body structure, we analyzed how gait is generated in walking and stand-up motions by generating gaits with two different methods: a method using linear completion to connect the postures necessary for the gait discovered through trial and error using the actual robot, and a method using the gait generated by reinforcement learning in the simulator and reflecting it to the actual robot. Both methods were able to generate gait that realized walking and stand-up motions, and interesting gait patterns were observed, which differed depending on the method, and were confirmed on the actual robot. Our code and demonstration videos are available here: https://github.com/shin0805/Chair-TypeAsymmetricalTripedalRobot.git`)],-1),wo=e("h2",{id:"_2024-04-08",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-08"},[e("span",null,"2024-04-08")])],-1),_o=e("h4",{id:"on-the-fly-robotic-assisted-medical-instrument-planning-and-execution-using-mixed-reality",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#on-the-fly-robotic-assisted-medical-instrument-planning-and-execution-using-mixed-reality"},[e("span",null,"On the Fly Robotic-Assisted Medical Instrument Planning and Execution Using Mixed Reality")])],-1),ko=e("p",null,[e("strong",null,"Authors"),t(": Letian Ai, Yihao Liu, Mehran Armand, Amir Kheradmand, Alejandro Martin-Gomez")],-1),xo=e("strong",null,"Link",-1),Ao={href:"http://arxiv.org/abs/2404.05887v1",target:"_blank",rel:"noopener noreferrer"},Lo=e("p",null,[e("strong",null,"Abstract"),t(": Robotic-assisted medical systems (RAMS) have gained significant attention for their advantages in alleviating surgeons' fatigue and improving patients' outcomes. These systems comprise a range of human-computer interactions, including medical scene monitoring, anatomical target planning, and robot manipulation. However, despite its versatility and effectiveness, RAMS demands expertise in robotics, leading to a high learning cost for the operator. In this work, we introduce a novel framework using mixed reality technologies to ease the use of RAMS. The proposed framework achieves real-time planning and execution of medical instruments by providing 3D anatomical image overlay, human-robot collision detection, and robot programming interface. These features, integrated with an easy-to-use calibration method for head-mounted display, improve the effectiveness of human-robot interactions. To assess the feasibility of the framework, two medical applications are presented in this work: 1) coil placement during transcranial magnetic stimulation and 2) drill and injector device positioning during femoroplasty. Results from these use cases demonstrate its potential to extend to a wider range of medical scenarios.")],-1),To=e("h4",{id:"cobt-collaborative-programming-of-behaviour-trees-from-one-demonstration-for-robot-manipulation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#cobt-collaborative-programming-of-behaviour-trees-from-one-demonstration-for-robot-manipulation"},[e("span",null,"CoBT: Collaborative Programming of Behaviour Trees from One Demonstration for Robot Manipulation")])],-1),Ro=e("p",null,[e("strong",null,"Authors"),t(": Aayush Jain, Philip Long, Valeria Villani, John D. Kelleher, Maria Chiara Leva")],-1),So=e("strong",null,"Link",-1),Mo={href:"http://arxiv.org/abs/2404.05870v2",target:"_blank",rel:"noopener noreferrer"},zo=e("p",null,[e("strong",null,"Abstract"),t(": Mass customization and shorter manufacturing cycles are becoming more important among small and medium-sized companies. However, classical industrial robots struggle to cope with product variation and dynamic environments. In this paper, we present CoBT, a collaborative programming by demonstration framework for generating reactive and modular behavior trees. CoBT relies on a single demonstration and a combination of data-driven machine learning methods with logic-based declarative learning to learn a task, thus eliminating the need for programming expertise or long development times. The proposed framework is experimentally validated on 7 manipulation tasks and we show that CoBT achieves approx. 93% success rate overall with an average of 7.5s programming time. We conduct a pilot study with non-expert users to provide feedback regarding the usability of CoBT.")],-1),Po=e("h4",{id:"a-neuromorphic-approach-to-obstacle-avoidance-in-robot-manipulation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-neuromorphic-approach-to-obstacle-avoidance-in-robot-manipulation"},[e("span",null,"A Neuromorphic Approach to Obstacle Avoidance in Robot Manipulation")])],-1),jo=e("p",null,[e("strong",null,"Authors"),t(": Ahmed Faisal Abdelrahman, Matias Valdenegro-Toro, Maren Bennewitz, Paul G. Plöger")],-1),Co=e("strong",null,"Link",-1),Do={href:"http://arxiv.org/abs/2404.05858v1",target:"_blank",rel:"noopener noreferrer"},Io=e("p",null,[e("strong",null,"Abstract"),t(": Neuromorphic computing mimics computational principles of the brain in $\\textit{silico}$ and motivates research into event-based vision and spiking neural networks (SNNs). Event cameras (ECs) exclusively capture local intensity changes and offer superior power consumption, response latencies, and dynamic ranges. SNNs replicate biological neuronal dynamics and have demonstrated potential as alternatives to conventional artificial neural networks (ANNs), such as in reducing energy expenditure and inference time in visual classification. Nevertheless, these novel paradigms remain scarcely explored outside the domain of aerial robots. To investigate the utility of brain-inspired sensing and data processing, we developed a neuromorphic approach to obstacle avoidance on a camera-equipped manipulator. Our approach adapts high-level trajectory plans with reactive maneuvers by processing emulated event data in a convolutional SNN, decoding neural activations into avoidance motions, and adjusting plans using a dynamic motion primitive. We conducted experiments with a Kinova Gen3 arm performing simple reaching tasks that involve obstacles in sets of distinct task scenarios and in comparison to a non-adaptive baseline. Our neuromorphic approach facilitated reliable avoidance of imminent collisions in simulated and real-world experiments, where the baseline consistently failed. Trajectory adaptations had low impacts on safety and predictability criteria. Among the notable SNN properties were the correlation of computations with the magnitude of perceived motions and a robustness to different event emulation methods. Tests with a DAVIS346 EC showed similar performance, validating our experimental event emulation. Our results motivate incorporating SNN learning, utilizing neuromorphic processors, and further exploring the potential of neuromorphic methods.")],-1),Wo=e("h4",{id:"unveiling-latent-topics-in-robotic-process-automation-an-approach-based-on-latent-dirichlet-allocation-smart-review",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unveiling-latent-topics-in-robotic-process-automation-an-approach-based-on-latent-dirichlet-allocation-smart-review"},[e("span",null,"Unveiling Latent Topics in Robotic Process Automation -- an Approach based on Latent Dirichlet Allocation Smart Review")])],-1),Bo=e("p",null,[e("strong",null,"Authors"),t(": Petr Prucha, Peter Madzik, Lukas Falat, Hajo A. Reijers")],-1),Oo=e("strong",null,"Link",-1),qo={href:"http://arxiv.org/abs/2404.05836v1",target:"_blank",rel:"noopener noreferrer"},Fo=e("p",null,[e("strong",null,"Abstract"),t(": Robotic process automation (RPA) is a software technology that in recent years has gained a lot of attention and popularity. By now, research on RPA has spread into multiple research streams. This study aims to create a science map of RPA and its aspects by revealing latent topics related to RPA, their research interest, impact, and time development. We provide a systematic framework that is helpful to develop further research into this technology. By using an unsupervised machine learning method based on Latent Dirichlet Allocation, we were able to analyse over 2000 paper abstracts. Among these, we found 100 distinct study topics, 15 of which have been included in the science map we provide.")],-1),Ho=e("h4",{id:"humanoid-gym-reinforcement-learning-for-humanoid-robot-with-zero-shot-sim2real-transfer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#humanoid-gym-reinforcement-learning-for-humanoid-robot-with-zero-shot-sim2real-transfer"},[e("span",null,"Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer")])],-1),Eo=e("p",null,[e("strong",null,"Authors"),t(": Xinyang Gu, Yen-Jen Wang, Jianyu Chen")],-1),Go=e("strong",null,"Link",-1),No={href:"http://arxiv.org/abs/2404.05695v1",target:"_blank",rel:"noopener noreferrer"},Ko=e("p",null,[e("strong",null,"Abstract"),t(": Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environment with zero-shot sim-to-real transfer. The project website and source code can be found at: https://sites.google.com/view/humanoid-gym/.")],-1),Jo=e("h4",{id:"otterros-picking-and-programming-an-uncrewed-surface-vessel-for-experimental-field-robotics-research-with-ros-2",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#otterros-picking-and-programming-an-uncrewed-surface-vessel-for-experimental-field-robotics-research-with-ros-2"},[e("span",null,"OtterROS: Picking and Programming an Uncrewed Surface Vessel for Experimental Field Robotics Research with ROS 2")])],-1),Yo=e("p",null,[e("strong",null,"Authors"),t(": Thomas M. C. Sears, M. Riley Cooper, Sabrina R. Button, Joshua A. Marshall")],-1),Vo=e("strong",null,"Link",-1),Uo={href:"http://arxiv.org/abs/2404.05627v1",target:"_blank",rel:"noopener noreferrer"},Zo=e("p",null,[e("strong",null,"Abstract"),t(': There exist a wide range of options for field robotics research using ground and aerial mobile robots, but there are comparatively few robust and research-ready uncrewed surface vessels (USVs). This workshop paper starts with a snapshot of USVs currently available to the research community and then describes "OtterROS", an open source ROS 2 solution for the Otter USV. Field experiments using OtterROS are described, which highlight the utility of the Otter USV and the benefits of using ROS 2 in aquatic robotics research. For those interested in USV research, the paper details recommended hardware to run OtterROS and includes an example ROS 2 package using OtterROS, removing unnecessary non-recurring engineering from field robotics research activities.')],-1),$o=e("h4",{id:"stochastic-online-optimization-for-cyber-physical-and-robotic-systems",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#stochastic-online-optimization-for-cyber-physical-and-robotic-systems"},[e("span",null,"Stochastic Online Optimization for Cyber-Physical and Robotic Systems")])],-1),Xo=e("p",null,[e("strong",null,"Authors"),t(": Hao Ma, Melanie Zeilinger, Michael Muehlebach")],-1),Qo=e("strong",null,"Link",-1),en={href:"http://arxiv.org/abs/2404.05318v1",target:"_blank",rel:"noopener noreferrer"},tn=e("p",null,[e("strong",null,"Abstract"),t(": We propose a novel gradient-based online optimization framework for solving stochastic programming problems that frequently arise in the context of cyber-physical and robotic systems. Our problem formulation accommodates constraints that model the evolution of a cyber-physical system, which has, in general, a continuous state and action space, is nonlinear, and where the state is only partially observed. We also incorporate an approximate model of the dynamics as prior knowledge into the learning process and show that even rough estimates of the dynamics can significantly improve the convergence of our algorithms. Our online optimization framework encompasses both gradient descent and quasi-Newton methods, and we provide a unified convergence analysis of our algorithms in a non-convex setting. We also characterize the impact of modeling errors in the system dynamics on the convergence rate of the algorithms. Finally, we evaluate our algorithms in simulations of a flexible beam, a four-legged walking robot, and in real-world experiments with a ping-pong playing robot.")],-1),on=e("h4",{id:"long-horizon-locomotion-and-manipulation-on-a-quadrupedal-robot-with-large-language-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#long-horizon-locomotion-and-manipulation-on-a-quadrupedal-robot-with-large-language-models"},[e("span",null,"Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models")])],-1),nn=e("p",null,[e("strong",null,"Authors"),t(": Yutao Ouyang, Jinhan Li, Yunfei Li, Zhongyu Li, Chao Yu, Koushil Sreenath, Yi Wu")],-1),an=e("strong",null,"Link",-1),rn={href:"http://arxiv.org/abs/2404.05291v1",target:"_blank",rel:"noopener noreferrer"},sn=e("p",null,[e("strong",null,"Abstract"),t(": We present a large language model (LLM) based system to empower quadrupedal robots with problem-solving abilities for long-horizon tasks beyond short-term motions. Long-horizon tasks for quadrupeds are challenging since they require both a high-level understanding of the semantics of the problem for task planning and a broad range of locomotion and manipulation skills to interact with the environment. Our system builds a high-level reasoning layer with large language models, which generates hybrid discrete-continuous plans as robot code from task descriptions. It comprises multiple LLM agents: a semantic planner for sketching a plan, a parameter calculator for predicting arguments in the plan, and a code generator to convert the plan into executable robot code. At the low level, we adopt reinforcement learning to train a set of motion planning and control skills to unleash the flexibility of quadrupeds for rich environment interactions. Our system is tested on long-horizon tasks that are infeasible to complete with one single skill. Simulation and real-world experiments show that it successfully figures out multi-step strategies and demonstrates non-trivial behaviors, including building tools or notifying a human for help.")],-1),ln=e("h4",{id:"robust-anthropomorphic-robotic-manipulation-through-biomimetic-distributed-compliance",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robust-anthropomorphic-robotic-manipulation-through-biomimetic-distributed-compliance"},[e("span",null,"Robust Anthropomorphic Robotic Manipulation through Biomimetic Distributed Compliance")])],-1),cn=e("p",null,[e("strong",null,"Authors"),t(": Kai Junge, Josie Hughes")],-1),hn=e("strong",null,"Link",-1),dn={href:"http://arxiv.org/abs/2404.05262v1",target:"_blank",rel:"noopener noreferrer"},un=e("p",null,[e("strong",null,"Abstract"),t(": The impressive capabilities of humans to robustly perform manipulation relies on compliant interactions, enabled through the structure and materials spatially distributed in our hands. We propose by mimicking this distributed compliance in an anthropomorphic robotic hand, the open-loop manipulation robustness increases and observe the emergence of human-like behaviours. To achieve this, we introduce the ADAPT Hand equipped with tunable compliance throughout the skin, fingers, and the wrist. Through extensive automated pick-and-place tests, we show the grasping robustness closely mirrors an estimated geometric theoretical limit, while `stress-testing' the robot hand to perform 800+ grasps. Finally, 24 items with largely varying geometries are grasped in a constrained environment with a success rate of 93%. We demonstrate the hand-object self-organization behavior underlines this extreme robustness, where the hand automatically exhibits different grasp types depending on object geometries. Furthermore, the robot grasp type mimics a natural human grasp with a direct similarity of 68%.")],-1),pn=e("h4",{id:"mesa-drl-memory-enhanced-deep-reinforcement-learning-for-advanced-socially-aware-robot-navigation-in-crowded-environments",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#mesa-drl-memory-enhanced-deep-reinforcement-learning-for-advanced-socially-aware-robot-navigation-in-crowded-environments"},[e("span",null,"MeSA-DRL: Memory-Enhanced Deep Reinforcement Learning for Advanced Socially Aware Robot Navigation in Crowded Environments")])],-1),gn=e("p",null,[e("strong",null,"Authors"),t(": Mannan Saeed Muhammad, Estrella Montero")],-1),mn=e("strong",null,"Link",-1),bn={href:"http://arxiv.org/abs/2404.05203v1",target:"_blank",rel:"noopener noreferrer"},fn=e("p",null,[e("strong",null,"Abstract"),t(": Autonomous navigation capabilities play a critical role in service robots operating in environments where human interactions are pivotal, due to the dynamic and unpredictable nature of these environments. However, the variability in human behavior presents a substantial challenge for robots in predicting and anticipating movements, particularly in crowded scenarios. To address this issue, a memory-enabled deep reinforcement learning framework is proposed for autonomous robot navigation in diverse pedestrian scenarios. The proposed framework leverages long-term memory to retain essential information about the surroundings and model sequential dependencies effectively. The importance of human-robot interactions is also encoded to assign higher attention to these interactions. A global planning mechanism is incorporated into the memory-enabled architecture. Additionally, a multi-term reward system is designed to prioritize and encourage long-sighted robot behaviors by incorporating dynamic warning zones. Simultaneously, it promotes smooth trajectories and minimizes the time taken to reach the robot's desired goal. Extensive simulation experiments show that the suggested approach outperforms representative state-of-the-art methods, showcasing its ability to a navigation efficiency and safety in real-world scenarios.")],-1),vn=e("h4",{id:"llm-bt-performing-robotic-adaptive-tasks-based-on-large-language-models-and-behavior-trees",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-bt-performing-robotic-adaptive-tasks-based-on-large-language-models-and-behavior-trees"},[e("span",null,"LLM-BT: Performing Robotic Adaptive Tasks based on Large Language Models and Behavior Trees")])],-1),yn=e("p",null,[e("strong",null,"Authors"),t(": Haotian Zhou, Yunhan Lin, Longwu Yan, Jihong Zhu, Huasong Min")],-1),wn=e("strong",null,"Link",-1),_n={href:"http://arxiv.org/abs/2404.05134v1",target:"_blank",rel:"noopener noreferrer"},kn=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have been widely utilized to perform complex robotic tasks. However, handling external disturbances during tasks is still an open challenge. This paper proposes a novel method to achieve robotic adaptive tasks based on LLMs and Behavior Trees (BTs). It utilizes ChatGPT to reason the descriptive steps of tasks. In order to enable ChatGPT to understand the environment, semantic maps are constructed by an object recognition algorithm. Then, we design a Parser module based on Bidirectional Encoder Representations from Transformers (BERT) to parse these steps into initial BTs. Subsequently, a BTs Update algorithm is proposed to expand the initial BTs dynamically to control robots to perform adaptive tasks. Different from other LLM-based methods for complex robotic tasks, our method outputs variable BTs that can add and execute new actions according to environmental changes, which is robust to external disturbances. Our method is validated with simulation in different practical scenarios.")],-1),xn=e("h4",{id:"rollbot-a-spherical-robot-driven-by-a-single-actuator",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#rollbot-a-spherical-robot-driven-by-a-single-actuator"},[e("span",null,"Rollbot: a Spherical Robot Driven by a Single Actuator")])],-1),An=e("p",null,[e("strong",null,"Authors"),t(": Jingxian Wang, Michael Rubenstein")],-1),Ln=e("strong",null,"Link",-1),Tn={href:"http://arxiv.org/abs/2404.05120v1",target:"_blank",rel:"noopener noreferrer"},Rn=e("p",null,[e("strong",null,"Abstract"),t(": Here we present Rollbot, the first spherical robot capable of controllably maneuvering on 2D plane with a single actuator. Rollbot rolls on the ground in circular pattern and controls its motion by changing the curvature of the trajectory through accelerating and decelerating its single motor and attached mass. We present the theoretical analysis, design, and control of Rollbot, and demonstrate its ability to move in a controllable circular pattern and follow waypoints.")],-1),Sn=e("h2",{id:"_2024-04-07",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-07"},[e("span",null,"2024-04-07")])],-1),Mn=e("h4",{id:"legibot-generating-legible-motions-for-service-robots-using-cost-based-local-planners",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#legibot-generating-legible-motions-for-service-robots-using-cost-based-local-planners"},[e("span",null,"Legibot: Generating Legible Motions for Service Robots Using Cost-Based Local Planners")])],-1),zn=e("p",null,[e("strong",null,"Authors"),t(": Javad Amirian, Mouad Abrini, Mohamed Chetouani")],-1),Pn=e("strong",null,"Link",-1),jn={href:"http://arxiv.org/abs/2404.05100v1",target:"_blank",rel:"noopener noreferrer"},Cn=e("p",null,[e("strong",null,"Abstract"),t(": With the increasing presence of social robots in various environments and applications, there is an increasing need for these robots to exhibit socially-compliant behaviors. Legible motion, characterized by the ability of a robot to clearly and quickly convey intentions and goals to the individuals in its vicinity, through its motion, holds significant importance in this context. This will improve the overall user experience and acceptance of robots in human environments. In this paper, we introduce a novel approach to incorporate legibility into local motion planning for mobile robots. This can enable robots to generate legible motions in real-time and dynamic environments. To demonstrate the effectiveness of our proposed methodology, we also provide a robotic stack designed for deploying legibility-aware motion planning in a social robot, by integrating perception and localization components.")],-1),Dn=e("h4",{id:"pcbot-a-minimalist-robot-designed-for-swarm-applications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#pcbot-a-minimalist-robot-designed-for-swarm-applications"},[e("span",null,"PCBot: a Minimalist Robot Designed for Swarm Applications")])],-1),In=e("p",null,[e("strong",null,"Authors"),t(": Jingxian Wang, Michael Rubenstein")],-1),Wn=e("strong",null,"Link",-1),Bn={href:"http://arxiv.org/abs/2404.05087v1",target:"_blank",rel:"noopener noreferrer"},On=e("p",null,[e("strong",null,"Abstract"),t(": Complexity, cost, and power requirements for the actuation of individual robots can play a large factor in limiting the size of robotic swarms. Here we present PCBot, a minimalist robot that can precisely move on an orbital shake table using a bi-stable solenoid actuator built directly into its PCB. This allows the actuator to be built as part of the automated PCB manufacturing process, greatly reducing the impact it has on manual assembly. Thanks to this novel actuator design, PCBot has merely five major components and can be assembled in under 20 seconds, potentially enabling them to be easily mass-manufactured. Here we present the electro-magnetic and mechanical design of PCBot. Additionally, a prototype robot is used to demonstrate its ability to move in a straight line as well as follow given paths.")],-1),qn=e("h4",{id:"adaptive-anchor-pairs-selection-in-a-tdoa-based-system-through-robot-localization-error-minimization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#adaptive-anchor-pairs-selection-in-a-tdoa-based-system-through-robot-localization-error-minimization"},[e("span",null,"Adaptive Anchor Pairs Selection in a TDOA-based System Through Robot Localization Error Minimization")])],-1),Fn=e("p",null,[e("strong",null,"Authors"),t(": Marcin Kolakowski")],-1),Hn=e("strong",null,"Link",-1),En={href:"http://arxiv.org/abs/2404.05067v1",target:"_blank",rel:"noopener noreferrer"},Gn=e("p",null,[e("strong",null,"Abstract"),t(": The following paper presents an adaptive anchor pairs selection method for ultra-wideband (UWB) Time Difference of Arrival (TDOA) based positioning systems. The method divides the area covered by the system into several zones and assigns them anchor pair sets. The pair sets are determined during calibration based on localization root mean square error (RMSE). The calibration assumes driving a mobile platform equipped with a LiDAR sensor and a UWB tag through the specified zones. The robot is localized separately based on a large set of different TDOA pairs and using a LiDAR, which acts as the reference. For each zone, the TDOA pairs set for which the registered RMSE is lowest is selected and used for localization in the routine system work. The proposed method has been tested with simulations and experiments. The results for both simulated static and experimental dynamic scenarios have proven that the adaptive selection of the anchor nodes leads to an increase in localization accuracy. In the experiment, the median trajectory error for a moving person localization was at a level of 25 cm.")],-1),Nn=e("h4",{id:"co-design-accessible-public-robots-insights-from-people-with-mobility-disability-robotic-practitioners-and-their-collaborations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#co-design-accessible-public-robots-insights-from-people-with-mobility-disability-robotic-practitioners-and-their-collaborations"},[e("span",null,"Co-design Accessible Public Robots: Insights from People with Mobility Disability, Robotic Practitioners and Their Collaborations")])],-1),Kn=e("p",null,[e("strong",null,"Authors"),t(": Howard Ziyu Han, Franklin Mingzhe Li, Alesandra Baca Vazquez, Daragh Byrne, Nikolas Martelaro, Sarah E Fox")],-1),Jn=e("strong",null,"Link",-1),Yn={href:"http://arxiv.org/abs/2404.05050v1",target:"_blank",rel:"noopener noreferrer"},Vn=e("p",null,[e("strong",null,"Abstract"),t(": Sidewalk robots are increasingly common across the globe. Yet, their operation on public paths poses challenges for people with mobility disabilities (PwMD) who face barriers to accessibility, such as insufficient curb cuts. We interviewed 15 PwMD to understand how they perceive sidewalk robots. Findings indicated that PwMD feel they have to compete for space on the sidewalk when robots are introduced. We next interviewed eight robotics practitioners to learn about their attitudes towards accessibility. Practitioners described how issues often stem from robotic companies addressing accessibility only after problems arise. Both interview groups underscored the importance of integrating accessibility from the outset. Building on this finding, we held four co-design workshops with PwMD and practitioners in pairs. These convenings brought to bear accessibility needs around robots operating in public spaces and in the public interest. Our study aims to set the stage for a more inclusive future around public service robots.")],-1),Un=e("h4",{id:"staccatoe-a-single-leg-robot-that-mimics-the-human-leg-and-toe",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#staccatoe-a-single-leg-robot-that-mimics-the-human-leg-and-toe"},[e("span",null,"StaccaToe: A Single-Leg Robot that Mimics the Human Leg and Toe")])],-1),Zn=e("p",null,[e("strong",null,"Authors"),t(": Nisal Perera, Shangqun Yu, Daniel Marew, Mack Tang, Ken Suzuki, Aidan McCormack, Shifan Zhu, Yong-Jae Kim, Donghyun Kim")],-1),$n=e("strong",null,"Link",-1),Xn={href:"http://arxiv.org/abs/2404.05039v1",target:"_blank",rel:"noopener noreferrer"},Qn=e("p",null,[e("strong",null,"Abstract"),t(": We introduce StaccaToe, a human-scale, electric motor-powered single-leg robot designed to rival the agility of human locomotion through two distinctive attributes: an actuated toe and a co-actuation configuration inspired by the human leg. Leveraging the foundational design of HyperLeg's lower leg mechanism, we develop a stand-alone robot by incorporating new link designs, custom-designed power electronics, and a refined control system. Unlike previous jumping robots that rely on either special mechanisms (e.g., springs and clutches) or hydraulic/pneumatic actuators, StaccaToe employs electric motors without energy storage mechanisms. This choice underscores our ultimate goal of developing a practical, high-performance humanoid robot capable of human-like, stable walking as well as explosive dynamic movements. In this paper, we aim to empirically evaluate the balance capability and the exertion of explosive ground reaction forces of our toe and co-actuation mechanisms. Throughout extensive hardware and controller development, StaccaToe showcases its control fidelity by demonstrating a balanced tip-toe stance and dynamic jump. This study is significant for three key reasons: 1) StaccaToe represents the first human-scale, electric motor-driven single-leg robot to execute dynamic maneuvers without relying on specialized mechanisms; 2) our research provides empirical evidence of the benefits of replicating critical human leg attributes in robotic design; and 3) we explain the design process for creating agile legged robots, the details that have been scantily covered in academic literature.")],-1),ea=e("h4",{id:"pathfinder-attention-driven-dynamic-non-line-of-sight-tracking-with-a-mobile-robot",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#pathfinder-attention-driven-dynamic-non-line-of-sight-tracking-with-a-mobile-robot"},[e("span",null,"PathFinder: Attention-Driven Dynamic Non-Line-of-Sight Tracking with a Mobile Robot")])],-1),ta=e("p",null,[e("strong",null,"Authors"),t(": Shenbagaraj Kannapiran, Sreenithy Chandran, Suren Jayasuriya, Spring Berman")],-1),oa=e("strong",null,"Link",-1),na={href:"http://arxiv.org/abs/2404.05024v1",target:"_blank",rel:"noopener noreferrer"},aa=e("p",null,[e("strong",null,"Abstract"),t(": The study of non-line-of-sight (NLOS) imaging is growing due to its many potential applications, including rescue operations and pedestrian detection by self-driving cars. However, implementing NLOS imaging on a moving camera remains an open area of research. Existing NLOS imaging methods rely on time-resolved detectors and laser configurations that require precise optical alignment, making it difficult to deploy them in dynamic environments. This work proposes a data-driven approach to NLOS imaging, PathFinder, that can be used with a standard RGB camera mounted on a small, power-constrained mobile robot, such as an aerial drone. Our experimental pipeline is designed to accurately estimate the 2D trajectory of a person who moves in a Manhattan-world environment while remaining hidden from the camera's field-of-view. We introduce a novel approach to process a sequence of dynamic successive frames in a line-of-sight (LOS) video using an attention-based neural network that performs inference in real-time. The method also includes a preprocessing selection metric that analyzes images from a moving camera which contain multiple vertical planar surfaces, such as walls and building facades, and extracts planes that return maximum NLOS information. We validate the approach on in-the-wild scenes using a drone for video capture, thus demonstrating low-cost NLOS imaging in dynamic capture environments.")],-1),ia=e("h4",{id:"robomp-2-a-robotic-multimodal-perception-planning-framework-with-multimodal-large-language-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robomp-2-a-robotic-multimodal-perception-planning-framework-with-multimodal-large-language-models"},[e("span",null,"RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models")])],-1),ra=e("p",null,[e("strong",null,"Authors"),t(": Qi Lv, Hao Li, Xiang Deng, Rui Shao, Michael Yu Wang, Liqiang Nie")],-1),sa=e("strong",null,"Link",-1),la={href:"http://arxiv.org/abs/2404.04929v1",target:"_blank",rel:"noopener noreferrer"},ca=e("p",null,[e("strong",null,"Abstract"),t(": Multimodal Large Language Models (MLLMs) have shown impressive reasoning abilities and general intelligence in various domains. It inspires researchers to train end-to-end MLLMs or utilize large models to generate policies with human-selected prompts for embodied agents. However, these methods exhibit limited generalization capabilities on unseen tasks or scenarios, and overlook the multimodal environment information which is critical for robots to make decisions. In this paper, we introduce a novel Robotic Multimodal Perception-Planning (RoboMP$^2$) framework for robotic manipulation which consists of a Goal-Conditioned Multimodal Preceptor (GCMP) and a Retrieval-Augmented Multimodal Planner (RAMP). Specially, GCMP captures environment states by employing a tailored MLLMs for embodied agents with the abilities of semantic reasoning and localization. RAMP utilizes coarse-to-fine retrieval method to find the $k$ most-relevant policies as in-context demonstrations to enhance the planner. Extensive experiments demonstrate the superiority of RoboMP$^2$ on both VIMA benchmark and real-world tasks, with around 10% improvement over the baselines.")],-1),ha=e("h4",{id:"learning-adaptive-multi-objective-robot-navigation-with-demonstrations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learning-adaptive-multi-objective-robot-navigation-with-demonstrations"},[e("span",null,"Learning Adaptive Multi-Objective Robot Navigation with Demonstrations")])],-1),da=e("p",null,[e("strong",null,"Authors"),t(": Jorge de Heuvel, Tharun Sethuraman, Maren Bennewitz")],-1),ua=e("strong",null,"Link",-1),pa={href:"http://arxiv.org/abs/2404.04857v1",target:"_blank",rel:"noopener noreferrer"},ga=e("p",null,[e("strong",null,"Abstract"),t(": Preference-aligned robot navigation in human environments is typically achieved through learning-based approaches, utilizing demonstrations and user feedback for personalization. However, personal preferences are subject to change and might even be context-dependent. Yet traditional reinforcement learning (RL) approaches with a static reward function often fall short in adapting to these varying user preferences. This paper introduces a framework that combines multi-objective reinforcement learning (MORL) with demonstration-based learning. Our approach allows for dynamic adaptation to changing user preferences without retraining. Through rigorous evaluations, including sim-to-real and robot-to-robot transfers, we demonstrate our framework's capability to reflect user preferences accurately while achieving high navigational performance in terms of collision avoidance and goal pursuance.")],-1),ma=e("h4",{id:"enquery-ensemble-policies-for-diverse-query-generation-in-preference-alignment-of-robot-navigation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enquery-ensemble-policies-for-diverse-query-generation-in-preference-alignment-of-robot-navigation"},[e("span",null,"EnQuery: Ensemble Policies for Diverse Query-Generation in Preference Alignment of Robot Navigation")])],-1),ba=e("p",null,[e("strong",null,"Authors"),t(": Jorge de Heuvel, Florian Seiler, Maren Bennewitz")],-1),fa=e("strong",null,"Link",-1),va={href:"http://arxiv.org/abs/2404.04852v1",target:"_blank",rel:"noopener noreferrer"},ya=e("p",null,[e("strong",null,"Abstract"),t(": To align mobile robot navigation policies with user preferences through reinforcement learning from human feedback (RLHF), reliable and behavior-diverse user queries are required. However, deterministic policies fail to generate a variety of navigation trajectory suggestions for a given navigation task configuration. We introduce EnQuery, a query generation approach using an ensemble of policies that achieve behavioral diversity through a regularization term. For a given navigation task, EnQuery produces multiple navigation trajectory suggestions, thereby optimizing the efficiency of preference data collection with fewer queries. Our methodology demonstrates superior performance in aligning navigation policies with user preferences in low-query regimes, offering enhanced policy convergence from sparse preference queries. The evaluation is complemented with a novel explainability representation, capturing full scene navigation behavior of the mobile robot in a single plot.")],-1),wa=e("h4",{id:"robotic-sorting-systems-robot-management-and-layout-design-optimization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robotic-sorting-systems-robot-management-and-layout-design-optimization"},[e("span",null,"Robotic Sorting Systems: Robot Management and Layout Design Optimization")])],-1),_a=e("p",null,[e("strong",null,"Authors"),t(": Tong Zhao, Xi Lin, Fang He, Hanwen Dai")],-1),ka=e("strong",null,"Link",-1),xa={href:"http://arxiv.org/abs/2404.04832v1",target:"_blank",rel:"noopener noreferrer"},Aa=e("p",null,[e("strong",null,"Abstract"),t(": In the contemporary logistics industry, automation plays a pivotal role in enhancing production efficiency and expanding industrial scale. Autonomous mobile robots, in particular, have become integral to the modernization efforts in warehouses. One noteworthy application in robotic warehousing is the robotic sorting system (RSS), distinguished by its characteristics such as cost-effectiveness, simplicity, scalability, and adaptable throughput control. While previous research has focused on analyzing the efficiency of RSS, it often assumed an ideal robot management system ignoring potential queuing delays by assuming constant travel times. This study relaxes this assumption and explores the quantitative relationship between RSS configuration parameters and system throughput. We introduce a novel robot traffic management method, named the rhythmic control for sorting scenario (RC-S), for RSS operations, equipped with an estimation formula establishing the relationship between system performance and configurations. Simulations validate that RC-S reduces average service time by 10.3% compared to the classical cooperative A* algorithm, while also improving throughput and runtime. Based on the performance analysis of RC-S, we further develop a layout optimization model for RSS, considering RSS configuration, desired throughput, and costs, to minimize expenses and determine the best layout. Numerical studies show that at lower throughput levels, facility costs dominate, while at higher throughput levels, labor costs prevail. Additionally, due to traffic efficiency limitations, RSS is well-suited for small-scale operations like end-of-supply-chain distribution centers.")],-1),La=e("h4",{id:"efficient-reinforcement-learning-of-task-planners-for-robotic-palletization-through-iterative-action-masking-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#efficient-reinforcement-learning-of-task-planners-for-robotic-palletization-through-iterative-action-masking-learning"},[e("span",null,"Efficient Reinforcement Learning of Task Planners for Robotic Palletization through Iterative Action Masking Learning")])],-1),Ta=e("p",null,[e("strong",null,"Authors"),t(": Zheng Wu, Yichuan Li, Wei Zhan, Changliu Liu, Yun-Hui Liu, Masayoshi Tomizuka")],-1),Ra=e("strong",null,"Link",-1),Sa={href:"http://arxiv.org/abs/2404.04772v1",target:"_blank",rel:"noopener noreferrer"},Ma=e("p",null,[e("strong",null,"Abstract"),t(": The development of robotic systems for palletization in logistics scenarios is of paramount importance, addressing critical efficiency and precision demands in supply chain management. This paper investigates the application of Reinforcement Learning (RL) in enhancing task planning for such robotic systems. Confronted with the substantial challenge of a vast action space, which is a significant impediment to efficiently apply out-of-the-shelf RL methods, our study introduces a novel method of utilizing supervised learning to iteratively prune and manage the action space effectively. By reducing the complexity of the action space, our approach not only accelerates the learning phase but also ensures the effectiveness and reliability of the task planning in robotic palletization. The experimental results underscore the efficacy of this method, highlighting its potential in improving the performance of RL applications in complex and high-dimensional environments like logistics palletization.")],-1),za=e("h2",{id:"_2024-04-06",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-06"},[e("span",null,"2024-04-06")])],-1),Pa=e("h4",{id:"eagle-the-first-event-camera-dataset-gathered-by-an-agile-quadruped-robot",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#eagle-the-first-event-camera-dataset-gathered-by-an-agile-quadruped-robot"},[e("span",null,"EAGLE: The First Event Camera Dataset Gathered by an Agile Quadruped Robot")])],-1),ja=e("p",null,[e("strong",null,"Authors"),t(": Shifan Zhu, Zixun Xiong, Donghyun Kim")],-1),Ca=e("strong",null,"Link",-1),Da={href:"http://arxiv.org/abs/2404.04698v1",target:"_blank",rel:"noopener noreferrer"},Ia=e("p",null,[e("strong",null,"Abstract"),t(": When legged robots perform agile movements, traditional RGB cameras often produce blurred images, posing a challenge for accurate state estimation. Event cameras, inspired by biological vision mechanisms, have emerged as a promising solution for capturing high-speed movements and coping with challenging lighting conditions, owing to their significant advantages, such as low latency, high temporal resolution, and a high dynamic range. However, the integration of event cameras into agile-legged robots is still largely unexplored. Notably, no event camera-based dataset has yet been specifically developed for dynamic legged robots. To bridge this gap, we introduce EAGLE (Event dataset of an AGile LEgged robot), a new dataset comprising data from an event camera, an RGB-D camera, an IMU, a LiDAR, and joint angle encoders, all mounted on a quadruped robotic platform. This dataset features more than 100 sequences from real-world environments, encompassing various indoor and outdoor environments, different lighting conditions, a range of robot gaits (e.g., trotting, bounding, pronking), as well as acrobatic movements such as backflipping. To our knowledge, this is the first event camera dataset to include multi-sensory data collected by an agile quadruped robot.")],-1),Wa=e("h4",{id:"teleaware-robot-designing-awareness-augmented-telepresence-robot-for-remote-collaborative-locomotion",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#teleaware-robot-designing-awareness-augmented-telepresence-robot-for-remote-collaborative-locomotion"},[e("span",null,"TeleAware Robot: Designing Awareness-augmented Telepresence Robot for Remote Collaborative Locomotion")])],-1),Ba=e("p",null,[e("strong",null,"Authors"),t(": Ruyi Li, Yaxin Zhu, Min Liu, Yihang Zeng, Shanning Zhuang, Jiayi Fu, Yi Lu, Guyue Zhou, Can Liu, Jiangtao Gong")],-1),Oa=e("strong",null,"Link",-1),qa={href:"http://arxiv.org/abs/2404.04579v1",target:"_blank",rel:"noopener noreferrer"},Fa=e("p",null,[e("strong",null,"Abstract"),t(": Telepresence robots can be used to support users to navigate an environment remotely and share the visiting experience with their social partners. Although such systems allow users to see and hear the remote environment and communicate with their partners via live video feed, this does not provide enough awareness of the environment and their remote partner's activities. In this paper, we introduce an awareness framework for collaborative locomotion in scenarios of onsite and remote users visiting a place together. From an observational study of small groups of people visiting exhibitions, we derived four design goals for enhancing the environmental and social awareness between social partners, and developed a set of awareness-enhancing techniques to add to a standard telepresence robot - named TeleAware robot. Through a controlled experiment simulating a guided exhibition visiting task, TeleAware robot showed the ability to lower the workload, facilitate closer social proximity, and improve mutual awareness and social presence compared with the standard one. We discuss the impact of mobility and roles of local and remote users, and provide insights for the future design of awareness-enhancing telepresence robot systems that facilitate collaborative locomotion.")],-1),Ha=e("h4",{id:"jrdb-social-a-multifaceted-robotic-dataset-for-understanding-of-context-and-dynamics-of-human-interactions-within-social-groups",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#jrdb-social-a-multifaceted-robotic-dataset-for-understanding-of-context-and-dynamics-of-human-interactions-within-social-groups"},[e("span",null,"JRDB-Social: A Multifaceted Robotic Dataset for Understanding of Context and Dynamics of Human Interactions Within Social Groups")])],-1),Ea=e("p",null,[e("strong",null,"Authors"),t(": Simindokht Jahangard, Zhixi Cai, Shiki Wen, Hamid Rezatofighi")],-1),Ga=e("strong",null,"Link",-1),Na={href:"http://arxiv.org/abs/2404.04458v1",target:"_blank",rel:"noopener noreferrer"},Ka=e("p",null,[e("strong",null,"Abstract"),t(": Understanding human social behaviour is crucial in computer vision and robotics. Micro-level observations like individual actions fall short, necessitating a comprehensive approach that considers individual behaviour, intra-group dynamics, and social group levels for a thorough understanding. To address dataset limitations, this paper introduces JRDB-Social, an extension of JRDB. Designed to fill gaps in human understanding across diverse indoor and outdoor social contexts, JRDB-Social provides annotations at three levels: individual attributes, intra-group interactions, and social group context. This dataset aims to enhance our grasp of human social dynamics for robotic applications. Utilizing the recent cutting-edge multi-modal large language models, we evaluated our benchmark to explore their capacity to decipher social human behaviour.")],-1),Ja=e("h2",{id:"_2024-04-05",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-05"},[e("span",null,"2024-04-05")])],-1),Ya=e("h4",{id:"admittance-control-for-adaptive-remote-center-of-motion-in-robotic-laparoscopic-surgery",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#admittance-control-for-adaptive-remote-center-of-motion-in-robotic-laparoscopic-surgery"},[e("span",null,"Admittance Control for Adaptive Remote Center of Motion in Robotic Laparoscopic Surgery")])],-1),Va=e("p",null,[e("strong",null,"Authors"),t(": Ehsan Nasiri, Long Wang")],-1),Ua=e("strong",null,"Link",-1),Za={href:"http://arxiv.org/abs/2404.04416v1",target:"_blank",rel:"noopener noreferrer"},$a=e("p",null,[e("strong",null,"Abstract"),t(": In laparoscopic robot-assisted minimally invasive surgery, the kinematic control of the robot is subject to the remote center of motion (RCM) constraint at the port of entry (e.g., trocar) into the patient's body. During surgery, after the instrument is inserted through the trocar, intrinsic physiological movements such as the patient's heartbeat, breathing process, and/or other purposeful body repositioning may deviate the position of the port of entry. This can cause a conflict between the registered RCM and the moved port of entry. To mitigate this conflict, we seek to utilize the interaction forces at the RCM. We develop a novel framework that integrates admittance control into a redundancy resolution method for the RCM kinematic constraint. Using the force/torque sensory feedback at the base of the instrument driving mechanism (IDM), the proposed framework estimates the forces at RCM, rejects forces applied on other locations along the instrument, and uses them in the admittance controller. In this paper, we report analysis from kinematic simulations to validate the proposed framework. In addition, a hardware platform has been completed, and future work is planned for experimental validation.")],-1),Xa=e("h4",{id:"a-ground-mobile-robot-for-autonomous-terrestrial-laser-scanning-based-field-phenotyping",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-ground-mobile-robot-for-autonomous-terrestrial-laser-scanning-based-field-phenotyping"},[e("span",null,"A Ground Mobile Robot for Autonomous Terrestrial Laser Scanning-Based Field Phenotyping")])],-1),Qa=e("p",null,[e("strong",null,"Authors"),t(": Javier Rodriguez-Sanchez, Kyle Johnsen, Changying Li")],-1),ei=e("strong",null,"Link",-1),ti={href:"http://arxiv.org/abs/2404.04404v1",target:"_blank",rel:"noopener noreferrer"},oi=e("p",null,[e("strong",null,"Abstract"),t(": Traditional field phenotyping methods are often manual, time-consuming, and destructive, posing a challenge for breeding progress. To address this bottleneck, robotics and automation technologies offer efficient sensing tools to monitor field evolution and crop development throughout the season. This study aimed to develop an autonomous ground robotic system for LiDAR-based field phenotyping in plant breeding trials. A Husky platform was equipped with a high-resolution three-dimensional (3D) laser scanner to collect in-field terrestrial laser scanning (TLS) data without human intervention. To automate the TLS process, a 3D ray casting analysis was implemented for optimal TLS site planning, and a route optimization algorithm was utilized to minimize travel distance during data collection. The platform was deployed in two cotton breeding fields for evaluation, where it autonomously collected TLS data. The system provided accurate pose information through RTK-GNSS positioning and sensor fusion techniques, with average errors of less than 0.6 cm for location and 0.38$^{\\circ}$ for heading. The achieved localization accuracy allowed point cloud registration with mean point errors of approximately 2 cm, comparable to traditional TLS methods that rely on artificial targets and manual sensor deployment. This work presents an autonomous phenotyping platform that facilitates the quantitative assessment of plant traits under field conditions of both large agricultural fields and small breeding trials to contribute to the advancement of plant phenomics and breeding programs.")],-1),ni=e("h4",{id:"humanoid-robots-at-work-where-are-we",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#humanoid-robots-at-work-where-are-we"},[e("span",null,"Humanoid Robots at work: where are we ?")])],-1),ai=e("p",null,[e("strong",null,"Authors"),t(": Fabrice R. Noreils")],-1),ii=e("strong",null,"Link",-1),ri={href:"http://arxiv.org/abs/2404.04249v1",target:"_blank",rel:"noopener noreferrer"},si=e("p",null,[e("strong",null,"Abstract"),t(": Launched by Elon Musk and its Optimus, we are witnessing a new race in which many companies have already engaged. The objective it to put at work a new generation of humanoid robots in demanding industrial environments within 2 or 3 years. Is this objective realistic ? The aim of this document and its main contributions is to provide some hints by covering the following topics: First an analysis of 12 companies based on eight criteria that will help us to distinguish companies based on their maturity and approach to the market; second as these humanoids are very complex systems we will provide an overview of the technological challenges to be addressed; third when humanoids are deployed at scale, Operation and Maintenance become critical and the we will explore what is new with these complex machines; Finally Pilots are the last step to test the feasibility of a new system before mass deployment. This is an important step to test the maturity of a product and the strategy of the humanoid supplier to address a market and two pragmatic approaches will be discussed.")],-1),li=e("h4",{id:"modeling-kinematic-uncertainty-of-tendon-driven-continuum-robots-via-mixture-density-networks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#modeling-kinematic-uncertainty-of-tendon-driven-continuum-robots-via-mixture-density-networks"},[e("span",null,"Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via Mixture Density Networks")])],-1),ci=e("p",null,[e("strong",null,"Authors"),t(": Jordan Thompson, Brian Y. Cho, Daniel S. Brown, Alan Kuntz")],-1),hi=e("strong",null,"Link",-1),di={href:"http://arxiv.org/abs/2404.04241v1",target:"_blank",rel:"noopener noreferrer"},ui=e("p",null,[e("strong",null,"Abstract"),t(": Tendon-driven continuum robot kinematic models are frequently computationally expensive, inaccurate due to unmodeled effects, or both. In particular, unmodeled effects produce uncertainties that arise during the robot's operation that lead to variability in the resulting geometry. We propose a novel solution to these issues through the development of a Gaussian mixture kinematic model. We train a mixture density network to output a Gaussian mixture model representation of the robot geometry given the current tendon displacements. This model computes a probability distribution that is more representative of the true distribution of geometries at a given configuration than a model that outputs a single geometry, while also reducing the computation time. We demonstrate one use of this model through a trajectory optimization method that explicitly reasons about the workspace uncertainty to minimize the probability of collision.")],-1),pi=e("h4",{id:"multi-modal-perception-for-soft-robotic-interactions-using-generative-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#multi-modal-perception-for-soft-robotic-interactions-using-generative-models"},[e("span",null,"Multi-modal perception for soft robotic interactions using generative models")])],-1),gi=e("p",null,[e("strong",null,"Authors"),t(": Enrico Donato, Egidio Falotico, Thomas George Thuruthel")],-1),mi=e("strong",null,"Link",-1),bi={href:"http://arxiv.org/abs/2404.04220v1",target:"_blank",rel:"noopener noreferrer"},fi=e("p",null,[e("strong",null,"Abstract"),t(": Perception is essential for the active interaction of physical agents with the external environment. The integration of multiple sensory modalities, such as touch and vision, enhances this perceptual process, creating a more comprehensive and robust understanding of the world. Such fusion is particularly useful for highly deformable bodies such as soft robots. Developing a compact, yet comprehensive state representation from multi-sensory inputs can pave the way for the development of complex control strategies. This paper introduces a perception model that harmonizes data from diverse modalities to build a holistic state representation and assimilate essential information. The model relies on the causality between sensory input and robotic actions, employing a generative model to efficiently compress fused information and predict the next observation. We present, for the first time, a study on how touch can be predicted from vision and proprioception on soft robots, the importance of the cross-modal generation and why this is essential for soft robotic interactions in unstructured environments.")],-1),vi=e("h4",{id:"continual-policy-distillation-of-reinforcement-learning-based-controllers-for-soft-robotic-in-hand-manipulation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#continual-policy-distillation-of-reinforcement-learning-based-controllers-for-soft-robotic-in-hand-manipulation"},[e("span",null,"Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation")])],-1),yi=e("p",null,[e("strong",null,"Authors"),t(": Lanpei Li, Enrico Donato, Vincenzo Lomonaco, Egidio Falotico")],-1),wi=e("strong",null,"Link",-1),_i={href:"http://arxiv.org/abs/2404.04219v1",target:"_blank",rel:"noopener noreferrer"},ki=e("p",null,[e("strong",null,"Abstract"),t(": Dexterous manipulation, often facilitated by multi-fingered robotic hands, holds solid impact for real-world applications. Soft robotic hands, due to their compliant nature, offer flexibility and adaptability during object grasping and manipulation. Yet, benefits come with challenges, particularly in the control development for finger coordination. Reinforcement Learning (RL) can be employed to train object-specific in-hand manipulation policies, but limiting adaptability and generalizability. We introduce a Continual Policy Distillation (CPD) framework to acquire a versatile controller for in-hand manipulation, to rotate different objects in shape and size within a four-fingered soft gripper. The framework leverages Policy Distillation (PD) to transfer knowledge from expert policies to a continually evolving student policy network. Exemplar-based rehearsal methods are then integrated to mitigate catastrophic forgetting and enhance generalization. The performance of the CPD framework over various replay strategies demonstrates its effectiveness in consolidating knowledge from multiple experts and achieving versatile and adaptive behaviours for in-hand manipulation tasks.")],-1),xi=e("h4",{id:"probabilistically-informed-robot-object-search-with-multiple-regions",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#probabilistically-informed-robot-object-search-with-multiple-regions"},[e("span",null,"Probabilistically Informed Robot Object Search with Multiple Regions")])],-1),Ai=e("p",null,[e("strong",null,"Authors"),t(": Matthew Collins, Jared J. Beard, Nicholas Ohi, Yu Gu")],-1),Li=e("strong",null,"Link",-1),Ti={href:"http://arxiv.org/abs/2404.04186v1",target:"_blank",rel:"noopener noreferrer"},Ri=e("p",null,[e("strong",null,"Abstract"),t(': The increasing use of autonomous robot systems in hazardous environments underscores the need for efficient search and rescue operations. Despite significant advancements, existing literature on object search often falls short in overcoming the difficulty of long planning horizons and dealing with sensor limitations, such as noise. This study introduces a novel approach that formulates the search problem as a belief Markov decision processes with options (BMDP-O) to make Monte Carlo tree search (MCTS) a viable tool for overcoming these challenges in large scale environments. The proposed formulation incorporates sequences of actions (options) to move between regions of interest, enabling the algorithm to efficiently scale to large environments. This approach also enables the use of customizable fields of view, for use with multiple types of sensors. Experimental results demonstrate the superiority of this approach in large environments when compared to the problem without options and alternative tools such as receding horizon planners. Given compute time for the proposed formulation is relatively high, a further approximated "lite" formulation is proposed. The lite formulation finds objects in a comparable number of steps with faster computation.')],-1),Si=e("h4",{id:"designing-robots-to-help-women",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#designing-robots-to-help-women"},[e("span",null,"Designing Robots to Help Women")])],-1),Mi=e("p",null,[e("strong",null,"Authors"),t(": Martin Cooney, Lena Klasén, Fernando Alonso-Fernandez")],-1),zi=e("strong",null,"Link",-1),Pi={href:"http://arxiv.org/abs/2404.04123v1",target:"_blank",rel:"noopener noreferrer"},ji=e("p",null,[e("strong",null,"Abstract"),t(": Robots are being designed to help people in an increasing variety of settings--but seemingly little attention has been given so far to the specific needs of women, who represent roughly half of the world's population but are highly underrepresented in robotics. Here we used a speculative prototyping approach to explore this expansive design space: First, we identified some potential challenges of interest, including crimes and illnesses that disproportionately affect women, as well as potential opportunities for designers, which were visualized in five sketches. Then, one of the sketched scenarios was further explored by developing a prototype, of a robotic helper drone equipped with computer vision to detect hidden cameras that could be used to spy on women. While object detection introduced some errors, hidden cameras were identified with a reasonable accuracy of 80% (Intersection over Union (IoU) score: 0.40). Our aim is that the identified challenges and opportunities could help spark discussion and inspire designers, toward realizing a safer, more inclusive future through responsible use of technology.")],-1),Ci=e("h4",{id:"self-sensing-feedback-control-of-an-electrohydraulic-robotic-shoulder",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#self-sensing-feedback-control-of-an-electrohydraulic-robotic-shoulder"},[e("span",null,"Self-Sensing Feedback Control of an Electrohydraulic Robotic Shoulder")])],-1),Di=e("p",null,[e("strong",null,"Authors"),t(": Clemens C. Christoph, Amirhossein Kazemipour, Michel R. Vogt, Yu Zhang, Robert K. Katzschmann")],-1),Ii=e("strong",null,"Link",-1),Wi={href:"http://arxiv.org/abs/2404.04079v1",target:"_blank",rel:"noopener noreferrer"},Bi=e("p",null,[e("strong",null,"Abstract"),t(": The human shoulder, with its glenohumeral joint, tendons, ligaments, and muscles, allows for the execution of complex tasks with precision and efficiency. However, current robotic shoulder designs lack the compliance and compactness inherent in their biological counterparts. A major limitation of these designs is their reliance on external sensors like rotary encoders, which restrict mechanical joint design and introduce bulk to the system. To address this constraint, we present a bio-inspired antagonistic robotic shoulder with two degrees of freedom powered by self-sensing hydraulically amplified self-healing electrostatic actuators. Our artificial muscle design decouples the high-voltage electrostatic actuation from the pair of low-voltage self-sensing electrodes. This approach allows for proprioceptive feedback control of trajectories in the task space while eliminating the necessity for any additional sensors. We assess the platform's efficacy by comparing it to a feedback control based on position data provided by a motion capture system. The study demonstrates closed-loop controllable robotic manipulators based on an inherent self-sensing capability of electrohydraulic actuators. The proposed architecture can serve as a basis for complex musculoskeletal joint arrangements.")],-1),Oi=e("h4",{id:"bidirectional-human-interactive-ai-framework-for-social-robot-navigation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#bidirectional-human-interactive-ai-framework-for-social-robot-navigation"},[e("span",null,"Bidirectional Human Interactive AI Framework for Social Robot Navigation")])],-1),qi=e("p",null,[e("strong",null,"Authors"),t(": Tuba Girgin, Emre Girgin, Yigit Yildirim, Emre Ugur, Mehmet Haklidir")],-1),Fi=e("strong",null,"Link",-1),Hi={href:"http://arxiv.org/abs/2404.04069v1",target:"_blank",rel:"noopener noreferrer"},Ei=e("p",null,[e("strong",null,"Abstract"),t(": Trustworthiness is a crucial concept in the context of human-robot interaction. Cooperative robots must be transparent regarding their decision-making process, especially when operating in a human-oriented environment. This paper presents a comprehensive end-to-end framework aimed at fostering trustworthy bidirectional human-robot interaction in collaborative environments for the social navigation of mobile robots. Our method enables a mobile robot to predict the trajectory of people and adjust its route in a socially-aware manner. In case of conflict between human and robot decisions, detected through visual examination, the route is dynamically modified based on human preference while verbal communication is maintained. We present our pipeline, framework design, and preliminary experiments that form the foundation of our proposition.")],-1),Gi=e("h4",{id:"voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots"},[e("span",null,"VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots")])],-1),Ni=e("p",null,[e("strong",null,"Authors"),t(": Akhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla, Carmel Majidi, Henny Admoni, Zackory Erickson")],-1),Ki=e("strong",null,"Link",-1),Ji={href:"http://arxiv.org/abs/2404.04066v1",target:"_blank",rel:"noopener noreferrer"},Yi=e("p",null,[e("strong",null,"Abstract"),t(": Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating LLMs as interfaces to robots for high level task planning and code generation have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating LLMs as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using LLMs as speech interfaces for assistive robots. Videos and supporting files are located on our project website: https://sites.google.com/andrew.cmu.edu/voicepilot/")],-1),Vi=e("h4",{id:"towards-safe-robot-use-with-edged-or-pointed-objects-a-surrogate-study-assembling-a-human-hand-injury-protection-database",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-safe-robot-use-with-edged-or-pointed-objects-a-surrogate-study-assembling-a-human-hand-injury-protection-database"},[e("span",null,"Towards Safe Robot Use with Edged or Pointed Objects: A Surrogate Study Assembling a Human Hand Injury Protection Database")])],-1),Ui=e("p",null,[e("strong",null,"Authors"),t(": Robin Jeanne Kirschner, Carina M. Micheler, Yangcan Zhou, Sebastian Siegner, Mazin Hamad, Claudio Glowalla, Jan Neumann, Nader Rajaei, Rainer Burgkart, Sami Haddadin")],-1),Zi=e("strong",null,"Link",-1),$i={href:"http://arxiv.org/abs/2404.04004v1",target:"_blank",rel:"noopener noreferrer"},Xi=e("p",null,[e("strong",null,"Abstract"),t(": The use of pointed or edged tools or objects is one of the most challenging aspects of today's application of physical human-robot interaction (pHRI). One reason for this is that the severity of harm caused by such edged or pointed impactors is less well studied than for blunt impactors. Consequently, the standards specify well-reasoned force and pressure thresholds for blunt impactors and advise avoiding any edges and corners in contacts. Nevertheless, pointed or edged impactor geometries cannot be completely ruled out in real pHRI applications. For example, to allow edged or pointed tools such as screwdrivers near human operators, the knowledge of injury severity needs to be extended so that robot integrators can perform well-reasoned, time-efficient risk assessments. In this paper, we provide the initial datasets on injury prevention for the human hand based on drop tests with surrogates for the human hand, namely pig claws and chicken drumsticks. We then demonstrate the ease and efficiency of robot use using the dataset for contact on two examples. Finally, our experiments provide a set of injuries that may also be expected for human subjects under certain robot mass-velocity constellations in collisions. To extend this work, testing on human samples and a collaborative effort from research institutes worldwide is needed to create a comprehensive human injury avoidance database for any pHRI scenario and thus for safe pHRI applications including edged and pointed geometries.")],-1),Qi=e("h4",{id:"pomdp-guided-active-force-based-search-for-robotic-insertion",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#pomdp-guided-active-force-based-search-for-robotic-insertion"},[e("span",null,"POMDP-Guided Active Force-Based Search for Robotic Insertion")])],-1),er=e("p",null,[e("strong",null,"Authors"),t(": Chen Wang, Haoxiang Luo, Kun Zhang, Hua Chen, Jia Pan, Wei Zhang")],-1),tr=e("strong",null,"Link",-1),or={href:"http://arxiv.org/abs/2404.03943v1",target:"_blank",rel:"noopener noreferrer"},nr=e("p",null,[e("strong",null,"Abstract"),t(": In robotic insertion tasks where the uncertainty exceeds the allowable tolerance, a good search strategy is essential for successful insertion and significantly influences efficiency. The commonly used blind search method is time-consuming and does not exploit the rich contact information. In this paper, we propose a novel search strategy that actively utilizes the information contained in the contact configuration and shows high efficiency. In particular, we formulate this problem as a Partially Observable Markov Decision Process (POMDP) with carefully designed primitives based on an in-depth analysis of the contact configuration's static stability. From the formulated POMDP, we can derive a novel search strategy. Thanks to its simplicity, this search strategy can be incorporated into a Finite-State-Machine (FSM) controller. The behaviors of the FSM controller are realized through a low-level Cartesian Impedance Controller. Our method is based purely on the robot's proprioceptive sensing and does not need visual or tactile sensors. To evaluate the effectiveness of our proposed strategy and control framework, we conduct extensive comparison experiments in simulation, where we compare our method with the baseline approach. The results demonstrate that our proposed method achieves a higher success rate with a shorter search time and search trajectory length compared to the baseline method. Additionally, we show that our method is robust to various initial displacement errors.")],-1),ar=e("h2",{id:"_2024-04-04",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-04"},[e("span",null,"2024-04-04")])],-1),ir=e("h4",{id:"fast-k-connectivity-restoration-in-multi-robot-systems-for-robust-communication-maintenance",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#fast-k-connectivity-restoration-in-multi-robot-systems-for-robust-communication-maintenance"},[e("span",null,"Fast k-connectivity Restoration in Multi-Robot Systems for Robust Communication Maintenance")])],-1),rr=e("p",null,[e("strong",null,"Authors"),t(": Md Ishat-E-Rabban, Guangyao Shi, Griffin Bonner, Pratap Tokekar")],-1),sr=e("strong",null,"Link",-1),lr={href:"http://arxiv.org/abs/2404.03834v1",target:"_blank",rel:"noopener noreferrer"},cr=e("p",null,[e("strong",null,"Abstract"),t(": Maintaining a robust communication network plays an important role in the success of a multi-robot team jointly performing an optimization task. A key characteristic of a robust cooperative multi-robot system is the ability to repair the communication topology in the case of robot failure. In this paper, we focus on the Fast k-connectivity Restoration (FCR) problem, which aims to repair a network to make it k-connected with minimum robot movement. We develop a Quadratically Constrained Program (QCP) formulation of the FCR problem, which provides a way to optimally solve the problem, but cannot handle large instances due to high computational overhead. We therefore present a scalable algorithm, called EA-SCR, for the FCR problem using graph theoretic concepts. By conducting empirical studies, we demonstrate that the EA-SCR algorithm performs within 10 percent of the optimal while being orders of magnitude faster. We also show that EA-SCR outperforms existing solutions by 30 percent in terms of the FCR distance metric.")],-1),hr=e("h4",{id:"accounting-for-hysteresis-in-the-forward-kinematics-of-nonlinearly-routed-tendon-driven-continuum-robots-via-a-learned-deep-decoder-network",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#accounting-for-hysteresis-in-the-forward-kinematics-of-nonlinearly-routed-tendon-driven-continuum-robots-via-a-learned-deep-decoder-network"},[e("span",null,"Accounting for Hysteresis in the Forward Kinematics of Nonlinearly-Routed Tendon-Driven Continuum Robots via a Learned Deep Decoder Network")])],-1),dr=e("p",null,[e("strong",null,"Authors"),t(": Brian Y. Cho, Daniel S. Esser, Jordan Thompson, Bao Thach, Robert J. Webster III, Alan Kuntz")],-1),ur=e("strong",null,"Link",-1),pr={href:"http://arxiv.org/abs/2404.03816v1",target:"_blank",rel:"noopener noreferrer"},gr=e("p",null,[e("strong",null,"Abstract"),t(": Tendon-driven continuum robots have been gaining popularity in medical applications due to their ability to curve around complex anatomical structures, potentially reducing the invasiveness of surgery. However, accurate modeling is required to plan and control the movements of these flexible robots. Physics-based models have limitations due to unmodeled effects, leading to mismatches between model prediction and actual robot shape. Recently proposed learning-based methods have been shown to overcome some of these limitations but do not account for hysteresis, a significant source of error for these robots. To overcome these challenges, we propose a novel deep decoder neural network that predicts the complete shape of tendon-driven robots using point clouds as the shape representation, conditioned on prior configurations to account for hysteresis. We evaluate our method on a physical tendon-driven robot and show that our network model accurately predicts the robot's shape, significantly outperforming a state-of-the-art physics-based model and a learning-based model that does not account for hysteresis.")],-1),mr=e("h4",{id:"legible-and-proactive-robot-planning-for-prosocial-human-robot-interactions",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#legible-and-proactive-robot-planning-for-prosocial-human-robot-interactions"},[e("span",null,"Legible and Proactive Robot Planning for Prosocial Human-Robot Interactions")])],-1),br=e("p",null,[e("strong",null,"Authors"),t(": Jasper Geldenbott, Karen Leung")],-1),fr=e("strong",null,"Link",-1),vr={href:"http://arxiv.org/abs/2404.03734v1",target:"_blank",rel:"noopener noreferrer"},yr=e("p",null,[e("strong",null,"Abstract"),t(": Humans have a remarkable ability to fluently engage in joint collision avoidance in crowded navigation tasks despite the complexities and uncertainties inherent in human behavior. Underlying these interactions is a mutual understanding that (i) individuals are prosocial, that is, there is equitable responsibility in avoiding collisions, and (ii) individuals should behave legibly, that is, move in a way that clearly conveys their intent to reduce ambiguity in how they intend to avoid others. Toward building robots that can safely and seamlessly interact with humans, we propose a general robot trajectory planning framework for synthesizing legible and proactive behaviors and demonstrate that our robot planner naturally leads to prosocial interactions. Specifically, we introduce the notion of a markup factor to incentivize legible and proactive behaviors and an inconvenience budget constraint to ensure equitable collision avoidance responsibility. We evaluate our approach against well-established multi-agent planning algorithms and show that using our approach produces safe, fluent, and prosocial interactions. We demonstrate the real-time feasibility of our approach with human-in-the-loop simulations. Project page can be found at https://uw-ctrl.github.io/phri/.")],-1),wr=e("h4",{id:"juicer-data-efficient-imitation-learning-for-robotic-assembly",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#juicer-data-efficient-imitation-learning-for-robotic-assembly"},[e("span",null,"JUICER: Data-Efficient Imitation Learning for Robotic Assembly")])],-1),_r=e("p",null,[e("strong",null,"Authors"),t(": Lars Ankile, Anthony Simeonov, Idan Shenfeld, Pulkit Agrawal")],-1),kr=e("strong",null,"Link",-1),xr={href:"http://arxiv.org/abs/2404.03729v2",target:"_blank",rel:"noopener noreferrer"},Ar=e("p",null,[e("strong",null,"Abstract"),t(": While learning from demonstrations is powerful for acquiring visuomotor policies, high-performance imitation without large demonstration datasets remains challenging for tasks requiring precise, long-horizon manipulation. This paper proposes a pipeline for improving imitation learning performance with a small human demonstration budget. We apply our approach to assembly tasks that require precisely grasping, reorienting, and inserting multiple parts over long horizons and multiple task phases. Our pipeline combines expressive policy architectures and various techniques for dataset expansion and simulation-based data augmentation. These help expand dataset support and supervise the model with locally corrective actions near bottleneck regions requiring high precision. We demonstrate our pipeline on four furniture assembly tasks in simulation, enabling a manipulator to assemble up to five parts over nearly 2500 time steps directly from RGB images, outperforming imitation and data augmentation baselines. Project website: https://imitation-juicer.github.io/.")],-1),Lr=e("h4",{id:"robust-221-bugs-in-the-robot-operating-system",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robust-221-bugs-in-the-robot-operating-system"},[e("span",null,"ROBUST: 221 Bugs in the Robot Operating System")])],-1),Tr=e("p",null,[e("strong",null,"Authors"),t(": Christopher S. Timperley, Gijs van der Hoorn, André Santos, Harshavardhan Deshpande, Andrzej Wąsowski")],-1),Rr=e("strong",null,"Link",-1),Sr={href:"http://arxiv.org/abs/2404.03629v1",target:"_blank",rel:"noopener noreferrer"},Mr=e("p",null,[e("strong",null,"Abstract"),t(": As robotic systems such as autonomous cars and delivery drones assume greater roles and responsibilities within society, the likelihood and impact of catastrophic software failure within those systems is increased.To aid researchers in the development of new methods to measure and assure the safety and quality of robotics software, we systematically curated a dataset of 221 bugs across 7 popular and diverse software systems implemented via the Robot Operating System (ROS). We produce historically accurate recreations of each of the 221 defective software versions in the form of Docker images, and use a grounded theory approach to examine and categorize their corresponding faults, failures, and fixes. Finally, we reflect on the implications of our findings and outline future research directions for the community.")],-1),zr=e("h4",{id:"anticipate-collab-data-driven-task-anticipation-and-knowledge-driven-planning-for-human-robot-collaboration",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#anticipate-collab-data-driven-task-anticipation-and-knowledge-driven-planning-for-human-robot-collaboration"},[e("span",null,"Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration")])],-1),Pr=e("p",null,[e("strong",null,"Authors"),t(": Shivam Singh, Karthik Swaminathan, Raghav Arora, Ramandeep Singh, Ahana Datta, Dipanjan Das, Snehasis Banerjee, Mohan Sridharan, Madhava Krishna")],-1),jr=e("strong",null,"Link",-1),Cr={href:"http://arxiv.org/abs/2404.03587v1",target:"_blank",rel:"noopener noreferrer"},Dr=e("p",null,[e("strong",null,"Abstract"),t(": An agent assisting humans in daily living activities can collaborate more effectively by anticipating upcoming tasks. Data-driven methods represent the state of the art in task anticipation, planning, and related problems, but these methods are resource-hungry and opaque. Our prior work introduced a proof of concept framework that used an LLM to anticipate 3 high-level tasks that served as goals for a classical planning system that computed a sequence of low-level actions for the agent to achieve these goals. This paper describes DaTAPlan, our framework that significantly extends our prior work toward human-robot collaboration. Specifically, DaTAPlan planner computes actions for an agent and a human to collaboratively and jointly achieve the tasks anticipated by the LLM, and the agent automatically adapts to unexpected changes in human action outcomes and preferences. We evaluate DaTAPlan capabilities in a realistic simulation environment, demonstrating accurate task anticipation, effective human-robot collaboration, and the ability to adapt to unexpected changes. Project website: https://dataplan-hrc.github.io")],-1),Ir=e("h4",{id:"robot-safety-monitoring-using-programmable-light-curtains",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robot-safety-monitoring-using-programmable-light-curtains"},[e("span",null,"Robot Safety Monitoring using Programmable Light Curtains")])],-1),Wr=e("p",null,[e("strong",null,"Authors"),t(": Karnik Ram, Shobhit Aggarwal, Robert Tamburo, Siddharth Ancha, Srinivasa Narasimhan")],-1),Br=e("strong",null,"Link",-1),Or={href:"http://arxiv.org/abs/2404.03556v1",target:"_blank",rel:"noopener noreferrer"},qr=e("p",null,[e("strong",null,"Abstract"),t(": As factories continue to evolve into collaborative spaces with multiple robots working together with human supervisors in the loop, ensuring safety for all actors involved becomes critical. Currently, laser-based light curtain sensors are widely used in factories for safety monitoring. While these conventional safety sensors meet high accuracy standards, they are difficult to reconfigure and can only monitor a fixed user-defined region of space. Furthermore, they are typically expensive. Instead, we leverage a controllable depth sensor, programmable light curtains (PLC), to develop an inexpensive and flexible real-time safety monitoring system for collaborative robot workspaces. Our system projects virtual dynamic safety envelopes that tightly envelop the moving robot at all times and detect any objects that intrude the envelope. Furthermore, we develop an instrumentation algorithm that optimally places (multiple) PLCs in a workspace to maximize the visibility coverage of robots. Our work enables fence-less human-robot collaboration, while scaling to monitor multiple robots with few sensors. We analyze our system in a real manufacturing testbed with four robot arms and demonstrate its capabilities as a fast, accurate, and inexpensive safety monitoring solution.")],-1),Fr=e("h4",{id:"integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#integrating-large-language-models-with-multimodal-virtual-reality-interfaces-to-support-collaborative-human-robot-construction-work"},[e("span",null,"Integrating Large Language Models with Multimodal Virtual Reality Interfaces to Support Collaborative Human-Robot Construction Work")])],-1),Hr=e("p",null,[e("strong",null,"Authors"),t(": Somin Park, Carol C. Menassa, Vineet R. Kamat")],-1),Er=e("strong",null,"Link",-1),Gr={href:"http://arxiv.org/abs/2404.03498v1",target:"_blank",rel:"noopener noreferrer"},Nr=e("p",null,[e("strong",null,"Abstract"),t(": In the construction industry, where work environments are complex, unstructured and often dangerous, the implementation of Human-Robot Collaboration (HRC) is emerging as a promising advancement. This underlines the critical need for intuitive communication interfaces that enable construction workers to collaborate seamlessly with robotic assistants. This study introduces a conversational Virtual Reality (VR) interface integrating multimodal interaction to enhance intuitive communication between construction workers and robots. By integrating voice and controller inputs with the Robot Operating System (ROS), Building Information Modeling (BIM), and a game engine featuring a chat interface powered by a Large Language Model (LLM), the proposed system enables intuitive and precise interaction within a VR setting. Evaluated by twelve construction workers through a drywall installation case study, the proposed system demonstrated its low workload and high usability with succinct command inputs. The proposed multimodal interaction system suggests that such technological integration can substantially advance the integration of robotic assistants in the construction industry.")],-1),Kr=e("h4",{id:"design-of-stickbug-a-six-armed-precision-pollination-robot",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#design-of-stickbug-a-six-armed-precision-pollination-robot"},[e("span",null,"Design of Stickbug: a Six-Armed Precision Pollination Robot")])],-1),Jr=e("p",null,[e("strong",null,"Authors"),t(": Trevor Smith, Madhav Rijal, Christopher Tatsch, R. Michael Butts, Jared Beard, R. Tyler Cook, Andy Chu, Jason Gross, Yu Gu")],-1),Yr=e("strong",null,"Link",-1),Vr={href:"http://arxiv.org/abs/2404.03489v1",target:"_blank",rel:"noopener noreferrer"},Ur=e("p",null,[e("strong",null,"Abstract"),t(": This work presents the design of Stickbug, a six-armed, multi-agent, precision pollination robot that combines the accuracy of single-agent systems with swarm parallelization in greenhouses. Precision pollination robots have often been proposed to offset the effects of a decreasing population of natural pollinators, but they frequently lack the required parallelization and scalability. Stickbug achieves this by allowing each arm and drive base to act as an individual agent, significantly reducing planning complexity. Stickbug uses a compact holonomic Kiwi drive to navigate narrow greenhouse rows, a tall mast to support multiple manipulators and reach plant heights, a detection model and classifier to identify Bramble flowers, and a felt-tipped end-effector for contact-based pollination. Initial experimental validation demonstrates that Stickbug can attempt over 1.5 pollinations per minute with a 50% success rate. Additionally, a Bramble flower perception dataset was created and is publicly available alongside Stickbug's software and design files.")],-1),Zr=e("h4",{id:"you-only-scan-once-a-dynamic-scene-reconstruction-pipeline-for-6-dof-robotic-grasping-of-novel-objects",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#you-only-scan-once-a-dynamic-scene-reconstruction-pipeline-for-6-dof-robotic-grasping-of-novel-objects"},[e("span",null,"You Only Scan Once: A Dynamic Scene Reconstruction Pipeline for 6-DoF Robotic Grasping of Novel Objects")])],-1),$r=e("p",null,[e("strong",null,"Authors"),t(": Lei Zhou, Haozhe Wang, Zhengshen Zhang, Zhiyang Liu, Francis EH Tay, adn Marcelo H. Ang. Jr")],-1),Xr=e("strong",null,"Link",-1),Qr={href:"http://arxiv.org/abs/2404.03462v1",target:"_blank",rel:"noopener noreferrer"},es=e("p",null,[e("strong",null,"Abstract"),t(": In the realm of robotic grasping, achieving accurate and reliable interactions with the environment is a pivotal challenge. Traditional methods of grasp planning methods utilizing partial point clouds derived from depth image often suffer from reduced scene understanding due to occlusion, ultimately impeding their grasping accuracy. Furthermore, scene reconstruction methods have primarily relied upon static techniques, which are susceptible to environment change during manipulation process limits their efficacy in real-time grasping tasks. To address these limitations, this paper introduces a novel two-stage pipeline for dynamic scene reconstruction. In the first stage, our approach takes scene scanning as input to register each target object with mesh reconstruction and novel object pose tracking. In the second stage, pose tracking is still performed to provide object poses in real-time, enabling our approach to transform the reconstructed object point clouds back into the scene. Unlike conventional methodologies, which rely on static scene snapshots, our method continuously captures the evolving scene geometry, resulting in a comprehensive and up-to-date point cloud representation. By circumventing the constraints posed by occlusion, our method enhances the overall grasp planning process and empowers state-of-the-art 6-DoF robotic grasping algorithms to exhibit markedly improved accuracy.")],-1),ts=e("h4",{id:"simultaneous-state-estimation-and-contact-detection-for-legged-robots-by-multiple-model-kalman-filtering",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#simultaneous-state-estimation-and-contact-detection-for-legged-robots-by-multiple-model-kalman-filtering"},[e("span",null,"Simultaneous State Estimation and Contact Detection for Legged Robots by Multiple-Model Kalman Filtering")])],-1),os=e("p",null,[e("strong",null,"Authors"),t(": Marcel Menner, Karl Berntorp")],-1),ns=e("strong",null,"Link",-1),as={href:"http://arxiv.org/abs/2404.03444v1",target:"_blank",rel:"noopener noreferrer"},is=e("p",null,[e("strong",null,"Abstract"),t(": This paper proposes an algorithm for combined contact detection and state estimation for legged robots. The proposed algorithm models the robot's movement as a switched system, in which different modes relate to different feet being in contact with the ground. The key element in the proposed algorithm is an interacting multiple-model Kalman filter, which identifies the currently-active mode defining contacts, while estimating the state. The rationale for the proposed estimation framework is that contacts (and contact forces) impact the robot's state and vice versa. This paper presents validation studies with a quadruped using (i) the high-fidelity simulator Gazebo for a comparison with ground truth values and a baseline estimator, and (ii) hardware experiments with the Unitree A1 robot. The simulation study shows that the proposed algorithm outperforms the baseline estimator, which does not simultaneous detect contacts. The hardware experiments showcase the applicability of the proposed algorithm and highlights the ability to detect contacts.")],-1),rs=e("h4",{id:"future-predictive-success-or-failure-classification-for-long-horizon-robotic-tasks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#future-predictive-success-or-failure-classification-for-long-horizon-robotic-tasks"},[e("span",null,"Future Predictive Success-or-Failure Classification for Long-Horizon Robotic Tasks")])],-1),ss=e("p",null,[e("strong",null,"Authors"),t(": Naoya Sogi, Hiroyuki Oyama, Takashi Shibata, Makoto Terao")],-1),ls=e("strong",null,"Link",-1),cs={href:"http://arxiv.org/abs/2404.03415v1",target:"_blank",rel:"noopener noreferrer"},hs=e("p",null,[e("strong",null,"Abstract"),t(": Automating long-horizon tasks with a robotic arm has been a central research topic in robotics. Optimization-based action planning is an efficient approach for creating an action plan to complete a given task. Construction of a reliable planning method requires a design process of conditions, e.g., to avoid collision between objects. The design process, however, has two critical issues: 1) iterative trials--the design process is time-consuming due to the trial-and-error process of modifying conditions, and 2) manual redesign--it is difficult to cover all the necessary conditions manually. To tackle these issues, this paper proposes a future-predictive success-or-failure-classification method to obtain conditions automatically. The key idea behind the proposed method is an end-to-end approach for determining whether the action plan can complete a given task instead of manually redesigning the conditions. The proposed method uses a long-horizon future-prediction method to enable success-or-failure classification without the execution of an action plan. This paper also proposes a regularization term called transition consistency regularization to provide easy-to-predict feature distribution. The regularization term improves future prediction and classification performance. The effectiveness of our method is demonstrated through classification and robotic-manipulation experiments.")],-1),ds=e("h4",{id:"radium-predicting-and-repairing-end-to-end-robot-failures-using-gradient-accelerated-sampling",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#radium-predicting-and-repairing-end-to-end-robot-failures-using-gradient-accelerated-sampling"},[e("span",null,"RADIUM: Predicting and Repairing End-to-End Robot Failures using Gradient-Accelerated Sampling")])],-1),us=e("p",null,[e("strong",null,"Authors"),t(": Charles Dawson, Anjali Parashar, Chuchu Fan")],-1),ps=e("strong",null,"Link",-1),gs={href:"http://arxiv.org/abs/2404.03412v1",target:"_blank",rel:"noopener noreferrer"},ms=e("p",null,[e("strong",null,"Abstract"),t(": Before autonomous systems can be deployed in safety-critical applications, we must be able to understand and verify the safety of these systems. For cases where the risk or cost of real-world testing is prohibitive, we propose a simulation-based framework for a) predicting ways in which an autonomous system is likely to fail and b) automatically adjusting the system's design and control policy to preemptively mitigate those failures. Existing tools for failure prediction struggle to search over high-dimensional environmental parameters, cannot efficiently handle end-to-end testing for systems with vision in the loop, and provide little guidance on how to mitigate failures once they are discovered. We approach this problem through the lens of approximate Bayesian inference and use differentiable simulation and rendering for efficient failure case prediction and repair. For cases where a differentiable simulator is not available, we provide a gradient-free version of our algorithm, and we include a theoretical and empirical evaluation of the trade-offs between gradient-based and gradient-free methods. We apply our approach on a range of robotics and control problems, including optimizing search patterns for robot swarms, UAV formation control, and robust network control. Compared to optimization-based falsification methods, our method predicts a more diverse, representative set of failure modes, and we find that our use of differentiable simulation yields solutions that have up to 10x lower cost and requires up to 2x fewer iterations to converge relative to gradient-free techniques. In hardware experiments, we find that repairing control policies using our method leads to a 5x robustness improvement. Accompanying code and video can be found at https://mit-realm.github.io/radium/")],-1),bs=e("h4",{id:"space-physiology-and-technology-musculoskeletal-adaptations-countermeasures-and-the-opportunity-for-wearable-robotics",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#space-physiology-and-technology-musculoskeletal-adaptations-countermeasures-and-the-opportunity-for-wearable-robotics"},[e("span",null,"Space Physiology and Technology: Musculoskeletal Adaptations, Countermeasures, and the Opportunity for Wearable Robotics")])],-1),fs=e("p",null,[e("strong",null,"Authors"),t(": Shamas Ul Ebad Khan, Rejin John Varghese, Panagiotis Kassanos, Dario Farina, Etienne Burdet")],-1),vs=e("strong",null,"Link",-1),ys={href:"http://arxiv.org/abs/2404.03363v1",target:"_blank",rel:"noopener noreferrer"},ws=e("p",null,[e("strong",null,"Abstract"),t(": Space poses significant challenges for human physiology, leading to physiological adaptations in response to an environment vastly different from Earth. While these adaptations can be beneficial, they may not fully counteract the adverse impact of space-related stressors. A comprehensive understanding of these physiological adaptations is needed to devise effective countermeasures to support human life in space. This review focuses on the impact of the environment in space on the musculoskeletal system. It highlights the complex interplay between bone and muscle adaptation, the underlying physiological mechanisms, and their implications on astronaut health. Furthermore, the review delves into the deployed and current advances in countermeasures and proposes, as a perspective for future developments, wearable sensing and robotic technologies, such as exoskeletons, as a fitting alternative.")],-1),_s=e("h4",{id:"embodied-neuromorphic-artificial-intelligence-for-robotics-perspectives-challenges-and-research-development-stack",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#embodied-neuromorphic-artificial-intelligence-for-robotics-perspectives-challenges-and-research-development-stack"},[e("span",null,"Embodied Neuromorphic Artificial Intelligence for Robotics: Perspectives, Challenges, and Research Development Stack")])],-1),ks=e("p",null,[e("strong",null,"Authors"),t(": Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Fakhreddine Zayer, Jorge Dias, Muhammad Shafique")],-1),xs=e("strong",null,"Link",-1),As={href:"http://arxiv.org/abs/2404.03325v1",target:"_blank",rel:"noopener noreferrer"},Ls=e("p",null,[e("strong",null,"Abstract"),t(': Robotic technologies have been an indispensable part for improving human productivity since they have been helping humans in completing diverse, complex, and intensive tasks in a fast yet accurate and efficient way. Therefore, robotic technologies have been deployed in a wide range of applications, ranging from personal to industrial use-cases. However, current robotic technologies and their computing paradigm still lack embodied intelligence to efficiently interact with operational environments, respond with correct/expected actions, and adapt to changes in the environments. Toward this, recent advances in neuromorphic computing with Spiking Neural Networks (SNN) have demonstrated the potential to enable the embodied intelligence for robotics through bio-plausible computing paradigm that mimics how the biological brain works, known as "neuromorphic artificial intelligence (AI)". However, the field of neuromorphic AI-based robotics is still at an early stage, therefore its development and deployment for solving real-world problems expose new challenges in different design aspects, such as accuracy, adaptability, efficiency, reliability, and security. To address these challenges, this paper will discuss how we can enable embodied neuromorphic AI for robotic systems through our perspectives: (P1) Embodied intelligence based on effective learning rule, training mechanism, and adaptability; (P2) Cross-layer optimizations for energy-efficient neuromorphic computing; (P3) Representative and fair benchmarks; (P4) Low-cost reliability and safety enhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A synergistic development for energy-efficient and robust neuromorphic-based robotics. Furthermore, this paper identifies research challenges and opportunities, as well as elaborates our vision for future research development toward embodied neuromorphic AI for robotics.')],-1),Ts=e("h4",{id:"delta-decomposed-efficient-long-term-robot-task-planning-using-large-language-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#delta-decomposed-efficient-long-term-robot-task-planning-using-large-language-models"},[e("span",null,"DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models")])],-1),Rs=e("p",null,[e("strong",null,"Authors"),t(": Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello")],-1),Ss=e("strong",null,"Link",-1),Ms={href:"http://arxiv.org/abs/2404.03275v1",target:"_blank",rel:"noopener noreferrer"},zs=e("p",null,[e("strong",null,"Abstract"),t(": Recent advancements in Large Language Models (LLMs) have sparked a revolution across various research fields. In particular, the integration of common-sense knowledge from LLMs into robot task and motion planning has been proven to be a game-changer, elevating performance in terms of explainability and downstream task efficiency to unprecedented heights. However, managing the vast knowledge encapsulated within these large models has posed challenges, often resulting in infeasible plans generated by LLM-based planning systems due to hallucinations or missing domain information. To overcome these challenges and obtain even greater planning feasibility and computational efficiency, we propose a novel LLM-driven task planning approach called DELTA. For achieving better grounding from environmental topology into actionable knowledge, DELTA leverages the power of scene graphs as environment representations within LLMs, enabling the fast generation of precise planning problem descriptions. For obtaining higher planning performance, we use LLMs to decompose the long-term task goals into an autoregressive sequence of sub-goals for an automated task planner to solve. Our contribution enables a more efficient and fully automatic task planning pipeline, achieving higher planning success rates and significantly shorter planning times compared to the state of the art.")],-1),Ps=e("h4",{id:"design-and-evaluation-of-a-compact-3d-end-effector-assistive-robot-for-adaptive-arm-support",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#design-and-evaluation-of-a-compact-3d-end-effector-assistive-robot-for-adaptive-arm-support"},[e("span",null,"Design and Evaluation of a Compact 3D End-effector Assistive Robot for Adaptive Arm Support")])],-1),js=e("p",null,[e("strong",null,"Authors"),t(": Sibo Yang, Lincong Luo, Wei Chuan Law, Youlong Wang, Lei Li, Wei Tech Ang")],-1),Cs=e("strong",null,"Link",-1),Ds={href:"http://arxiv.org/abs/2404.03149v1",target:"_blank",rel:"noopener noreferrer"},Is=e("p",null,[e("strong",null,"Abstract"),t(": We developed a 3D end-effector type of upper limb assistive robot, named as Assistive Robotic Arm Extender (ARAE), that provides transparency movement and adaptive arm support control to achieve home-based therapy and training in the real environment. The proposed system composes five degrees of freedom, including three active motors and two passive joints at the end-effector module. The core structure of the system is based on a parallel mechanism. The kinematic and dynamic modeling are illustrated in detail. The proposed adaptive arm support control framework calculates the compensated force based on the estimated human arm posture in 3D space. It firstly estimates human arm joint angles using two proposed methods: fixed torso and sagittal plane models without using external sensors such as IMUs, magnetic sensors, or depth cameras. The experiments were carried out to evaluate the performance of the two proposed angle estimation methods. Then, the estimated human joint angles were input into the human upper limb dynamics model to derive the required support force generated by the robot. The muscular activities were measured to evaluate the effects of the proposed framework. The obvious reduction of muscular activities was exhibited when participants were tested with the ARAE under an adaptive arm gravity compensation control framework. The overall results suggest that the ARAE system, when combined with the proposed control framework, has the potential to offer adaptive arm support. This integration could enable effective training with Activities of Daily Living (ADLs) and interaction with real environments.")],-1),Ws=e("h2",{id:"_2024-04-03",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-03"},[e("span",null,"2024-04-03")])],-1),Bs=e("h4",{id:"multi-robot-planning-for-filming-groups-of-moving-actors-leveraging-submodularity-and-pixel-density",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#multi-robot-planning-for-filming-groups-of-moving-actors-leveraging-submodularity-and-pixel-density"},[e("span",null,"Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density")])],-1),Os=e("p",null,[e("strong",null,"Authors"),t(": Skyler Hughes, Rebecca Martin, Micah Corah, Sebastian Scherer")],-1),qs=e("strong",null,"Link",-1),Fs={href:"http://arxiv.org/abs/2404.03103v1",target:"_blank",rel:"noopener noreferrer"},Hs=e("p",null,[e("strong",null,"Abstract"),t(": Observing and filming a group of moving actors with a team of aerial robots is a challenging problem that combines elements of multi-robot coordination, coverage, and view planning. A single camera may observe multiple actors at once, and the robot team may observe individual actors from multiple views. As actors move about, groups may split, merge, and reform, and robots filming these actors should be able to adapt smoothly to such changes in actor formations. Rather than adopt an approach based on explicit formations or assignments, we propose an approach based on optimizing views directly. We model actors as moving polyhedra and compute approximate pixel densities for each face and camera view. Then, we propose an objective that exhibits diminishing returns as pixel densities increase from repeated observation. This gives rise to a multi-robot perception planning problem which we solve via a combination of value iteration and greedy submodular maximization. %using a combination of value iteration to optimize views for individual robots and sequential submodular maximization methods to coordinate the team. We evaluate our approach on challenging scenarios modeled after various kinds of social behaviors and featuring different numbers of robots and actors and observe that robot assignments and formations arise implicitly based on the movements of groups of actors. Simulation results demonstrate that our approach consistently outperforms baselines, and in addition to performing well with the planner's approximation of pixel densities our approach also performs comparably for evaluation based on rendered views. Overall, the multi-round variant of the sequential planner we propose meets (within 1%) or exceeds the formation and assignment baselines in all scenarios we consider.")],-1),Es=e("h4",{id:"unsupervised-bottom-up-category-discovery-for-symbol-grounding-with-a-curious-robot",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unsupervised-bottom-up-category-discovery-for-symbol-grounding-with-a-curious-robot"},[e("span",null,"Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a Curious Robot")])],-1),Gs=e("p",null,[e("strong",null,"Authors"),t(": Catherine Henry, Casey Kennington")],-1),Ns=e("strong",null,"Link",-1),Ks={href:"http://arxiv.org/abs/2404.03092v1",target:"_blank",rel:"noopener noreferrer"},Js=e("p",null,[e("strong",null,"Abstract"),t(": Towards addressing the Symbol Grounding Problem and motivated by early childhood language development, we leverage a robot which has been equipped with an approximate model of curiosity with particular focus on bottom-up building of unsupervised categories grounded in the physical world. That is, rather than starting with a top-down symbol (e.g., a word referring to an object) and providing meaning through the application of predetermined samples, the robot autonomously and gradually breaks up its exploration space into a series of increasingly specific unlabeled categories at which point an external expert may optionally provide a symbol association. We extend prior work by using a robot that can observe the visual world, introducing a higher dimensional sensory space, and using a more generalizable method of category building. Our experiments show that the robot learns categories based on actions and what it visually observes, and that those categories can be symbolically grounded into.https://info.arxiv.org/help/prep#comments")],-1),Ys=e("h4",{id:"self-supervised-6-dof-robot-grasping-by-demonstration-via-augmented-reality-teleoperation-system",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#self-supervised-6-dof-robot-grasping-by-demonstration-via-augmented-reality-teleoperation-system"},[e("span",null,"Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System")])],-1),Vs=e("p",null,[e("strong",null,"Authors"),t(": Xiwen Dengxiong, Xueting Wang, Shi Bai, Yunbo Zhang")],-1),Us=e("strong",null,"Link",-1),Zs={href:"http://arxiv.org/abs/2404.03067v1",target:"_blank",rel:"noopener noreferrer"},$s=e("p",null,[e("strong",null,"Abstract"),t(": Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area. To this end, we propose a self-supervised 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations. Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration. For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations.")],-1),Xs=e("h4",{id:"language-environment-and-robotic-navigation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#language-environment-and-robotic-navigation"},[e("span",null,"Language, Environment, and Robotic Navigation")])],-1),Qs=e("p",null,[e("strong",null,"Authors"),t(": Johnathan E. Avery")],-1),el=e("strong",null,"Link",-1),tl={href:"http://arxiv.org/abs/2404.03049v1",target:"_blank",rel:"noopener noreferrer"},ol=e("p",null,[e("strong",null,"Abstract"),t(": This paper explores the integration of linguistic inputs within robotic navigation systems, drawing upon the symbol interdependency hypothesis to bridge the divide between symbolic and embodied cognition. It examines previous work incorporating language and semantics into Neural Network (NN) and Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these integrations have advanced the field. By contrasting abstract symbol manipulation with sensory-motor grounding, we propose a unified framework where language functions both as an abstract communicative system and as a grounded representation of perceptual experiences. Our review of cognitive models of distributional semantics and their application to autonomous agents underscores the transformative potential of language-integrated systems.")],-1),nl=e("h4",{id:"forming-large-patterns-with-local-robots-in-the-oblot-model",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#forming-large-patterns-with-local-robots-in-the-oblot-model"},[e("span",null,"Forming Large Patterns with Local Robots in the OBLOT Model")])],-1),al=e("p",null,[e("strong",null,"Authors"),t(": Christopher Hahn, Jonas Harbig, Peter Kling")],-1),il=e("strong",null,"Link",-1),rl={href:"http://arxiv.org/abs/2404.02771v2",target:"_blank",rel:"noopener noreferrer"},sl=e("p",null,[e("strong",null,"Abstract"),t(": In the arbitrary pattern formation problem, $n$ autonomous, mobile robots must form an arbitrary pattern $P \\subseteq \\mathbb{R}^2$. The (deterministic) robots are typically assumed to be indistinguishable, disoriented, and unable to communicate. An important distinction is whether robots have memory and/or a limited viewing range. Previous work managed to form $P$ under a natural symmetry condition if robots have no memory but an unlimited viewing range [22] or if robots have a limited viewing range but memory [25]. In the latter case, $P$ is only formed in a shrunk version that has constant diameter. Without memory and with limited viewing range, forming arbitrary patterns remains an open problem. We provide a partial solution by showing that $P$ can be formed under the same symmetry condition if the robots' initial diameter is $\\leq 1$. Our protocol partitions $P$ into rotation-symmetric components and exploits the initial mutual visibility to form one cluster per component. Using a careful placement of the clusters and their robots, we show that a cluster can move in a coordinated way through its component while drawing $P$ by dropping one robot per pattern coordinate.")],-1),ll=e("h4",{id:"unsupervised-learning-of-effective-actions-in-robotics",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unsupervised-learning-of-effective-actions-in-robotics"},[e("span",null,"Unsupervised Learning of Effective Actions in Robotics")])],-1),cl=e("p",null,[e("strong",null,"Authors"),t(": Marko Zaric, Jakob Hollenstein, Justus Piater, Erwan Renaudo")],-1),hl=e("strong",null,"Link",-1),dl={href:"http://arxiv.org/abs/2404.02728v1",target:"_blank",rel:"noopener noreferrer"},ul=e("p",null,[e("strong",null,"Abstract"),t(`: Learning actions that are relevant to decision-making and can be executed effectively is a key problem in autonomous robotics. Current state-of-the-art action representations in robotics lack proper effect-driven learning of the robot's actions. Although successful in solving manipulation tasks, deep learning methods also lack this ability, in addition to their high cost in terms of memory or training data. In this paper, we propose an unsupervised algorithm to discretize a continuous motion space and generate "action prototypes", each producing different effects in the environment. After an exploration phase, the algorithm automatically builds a representation of the effects and groups motions into action prototypes, where motions more likely to produce an effect are represented more than those that lead to negligible changes. We evaluate our method on a simulated stair-climbing reinforcement learning task, and the preliminary results show that our effect driven discretization outperforms uniformly and randomly sampled discretizations in convergence speed and maximum reward.`)],-1),pl=e("h4",{id:"sliceit-a-dual-simulator-framework-for-learning-robot-food-slicing",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sliceit-a-dual-simulator-framework-for-learning-robot-food-slicing"},[e("span",null,"SliceIt! -- A Dual Simulator Framework for Learning Robot Food Slicing")])],-1),gl=e("p",null,[e("strong",null,"Authors"),t(": Cristian C. Beltran-Hernandez, Nicolas Erbetti, Masashi Hamaya")],-1),ml=e("strong",null,"Link",-1),bl={href:"http://arxiv.org/abs/2404.02569v1",target:"_blank",rel:"noopener noreferrer"},fl=e("p",null,[e("strong",null,"Abstract"),t(": Cooking robots can enhance the home experience by reducing the burden of daily chores. However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives. This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks. More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control. Our approach involves using Reinforcement Learning (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board. However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste. Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in simulation. Following a real2sim2real approach, our framework consists of collecting a few real food slicing data, calibrating our dual simulation environment (a high-fidelity cutting simulator and a robotic simulator), learning compliant control policies on the calibrated simulation environment, and finally, deploying the policies on the real robot.")],-1),vl=e("h4",{id:"on-the-go-tree-detection-and-geometric-traits-estimation-with-ground-mobile-robots-in-fruit-tree-groves",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#on-the-go-tree-detection-and-geometric-traits-estimation-with-ground-mobile-robots-in-fruit-tree-groves"},[e("span",null,"On-the-Go Tree Detection and Geometric Traits Estimation with Ground Mobile Robots in Fruit Tree Groves")])],-1),yl=e("p",null,[e("strong",null,"Authors"),t(": Dimitrios Chatziparaschis, Hanzhe Teng, Yipeng Wang, Pamodya Peiris, Elia Scudiero, Konstantinos Karydis")],-1),wl=e("strong",null,"Link",-1),_l={href:"http://arxiv.org/abs/2404.02516v1",target:"_blank",rel:"noopener noreferrer"},kl=e("p",null,[e("strong",null,"Abstract"),t(": By-tree information gathering is an essential task in precision agriculture achieved by ground mobile sensors, but it can be time- and labor-intensive. In this paper we present an algorithmic framework to perform real-time and on-the-go detection of trees and key geometric characteristics (namely, width and height) with wheeled mobile robots in the field. Our method is based on the fusion of 2D domain-specific data (normalized difference vegetation index [NDVI] acquired via a red-green-near-infrared [RGN] camera) and 3D LiDAR point clouds, via a customized tree landmark association and parameter estimation algorithm. The proposed system features a multi-modal and entropy-based landmark correspondences approach, integrated into an underlying Kalman filter system to recognize the surrounding trees and jointly estimate their spatial and vegetation-based characteristics. Realistic simulated tests are used to evaluate our proposed algorithm's behavior in a variety of settings. Physical experiments in agricultural fields help validate our method's efficacy in acquiring accurate by-tree information on-the-go and in real-time by employing only onboard computational and sensing resources.")],-1),xl=e("h4",{id:"tightly-coupled-lidar-imu-wheel-odometry-with-online-calibration-of-a-kinematic-model-for-skid-steering-robots",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tightly-coupled-lidar-imu-wheel-odometry-with-online-calibration-of-a-kinematic-model-for-skid-steering-robots"},[e("span",null,"Tightly-Coupled LiDAR-IMU-Wheel Odometry with Online Calibration of a Kinematic Model for Skid-Steering Robots")])],-1),Al=e("p",null,[e("strong",null,"Authors"),t(": Taku Okawara, Kenji Koide, Shuji Oishi, Masashi Yokozuka, Atsuhiko Banno, Kentaro Uno, Kazuya Yoshida")],-1),Ll=e("strong",null,"Link",-1),Tl={href:"http://arxiv.org/abs/2404.02515v1",target:"_blank",rel:"noopener noreferrer"},Rl=e("p",null,[e("strong",null,"Abstract"),t(": Tunnels and long corridors are challenging environments for mobile robots because a LiDAR point cloud should degenerate in these environments. To tackle point cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel odometry algorithm with an online calibration for skid-steering robots. We propose a full linear wheel odometry factor, which not only serves as a motion constraint but also performs the online calibration of kinematic models for skid-steering robots. Despite the dynamically changing kinematic model (e.g., wheel radii changes caused by tire pressures) and terrain conditions, our method can address the model error via online calibration. Moreover, our method enables an accurate localization in cases of degenerated environments, such as long and straight corridors, by calibration while the LiDAR-IMU fusion sufficiently operates. Furthermore, we estimate the uncertainty (i.e., covariance matrix) of the wheel odometry online for creating a reasonable constraint. The proposed method is validated through three experiments. The first indoor experiment shows that the proposed method is robust in severe degeneracy cases (long corridors) and changes in the wheel radii. The second outdoor experiment demonstrates that our method accurately estimates the sensor trajectory despite being in rough outdoor terrain owing to online uncertainty estimation of wheel odometry. The third experiment shows the proposed online calibration enables robust odometry estimation in changing terrains.")],-1),Sl=e("h4",{id:"promptrpa-generating-robotic-process-automation-on-smartphones-from-textual-prompts",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#promptrpa-generating-robotic-process-automation-on-smartphones-from-textual-prompts"},[e("span",null,"PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts")])],-1),Ml=e("p",null,[e("strong",null,"Authors"),t(": Tian Huang, Chun Yu, Weinan Shi, Zijian Peng, David Yang, Weiqi Sun, Yuanchun Shi")],-1),zl=e("strong",null,"Link",-1),Pl={href:"http://arxiv.org/abs/2404.02475v1",target:"_blank",rel:"noopener noreferrer"},jl=e("p",null,[e("strong",null,"Abstract"),t(": Robotic Process Automation (RPA) offers a valuable solution for efficiently automating tasks on the graphical user interface (GUI), by emulating human interactions, without modifying existing code. However, its broader adoption is constrained by the need for expertise in both scripting languages and workflow design. To address this challenge, we present PromptRPA, a system designed to comprehend various task-related textual prompts (e.g., goals, procedures), thereby generating and performing corresponding RPA tasks. PromptRPA incorporates a suite of intelligent agents that mimic human cognitive functions, specializing in interpreting user intent, managing external information for RPA generation, and executing operations on smartphones. The agents can learn from user feedback and continuously improve their performance based on the accumulated knowledge. Experimental results indicated a performance jump from a 22.28% success rate in the baseline to 95.21% with PromptRPA, requiring an average of 1.66 user interventions for each new task. PromptRPA presents promising applications in fields such as tutorial creation, smart assistance, and customer service.")],-1);function Cl(Dl,Il){const o=i("ExternalLinkIcon");return r(),s("div",null,[h,e("p",null,[d,t(": "),e("a",u,[t("http://arxiv.org/abs/2404.07880v1"),n(o)])]),p,g,m,e("p",null,[b,t(": "),e("a",f,[t("http://arxiv.org/abs/2404.07795v1"),n(o)])]),v,y,w,e("p",null,[_,t(": "),e("a",k,[t("http://arxiv.org/abs/2404.07774v1"),n(o)])]),x,A,L,e("p",null,[T,t(": "),e("a",R,[t("http://arxiv.org/abs/2404.07735v1"),n(o)])]),S,M,z,e("p",null,[P,t(": "),e("a",j,[t("http://arxiv.org/abs/2404.07717v1"),n(o)])]),C,D,I,e("p",null,[W,t(": "),e("a",B,[t("http://arxiv.org/abs/2404.07672v1"),n(o)])]),O,q,F,e("p",null,[H,t(": "),e("a",E,[t("http://arxiv.org/abs/2404.07594v1"),n(o)])]),G,N,K,e("p",null,[J,t(": "),e("a",Y,[t("http://arxiv.org/abs/2404.07590v1"),n(o)])]),V,U,Z,e("p",null,[$,t(": "),e("a",X,[t("http://arxiv.org/abs/2404.07560v1"),n(o)])]),Q,ee,te,e("p",null,[oe,t(": "),e("a",ne,[t("http://arxiv.org/abs/2404.07505v1"),n(o)])]),ae,ie,re,e("p",null,[se,t(": "),e("a",le,[t("http://arxiv.org/abs/2404.07428v1"),n(o)])]),ce,he,de,e("p",null,[ue,t(": "),e("a",pe,[t("http://arxiv.org/abs/2404.07409v1"),n(o)])]),ge,me,be,fe,e("p",null,[ve,t(": "),e("a",ye,[t("http://arxiv.org/abs/2404.07360v1"),n(o)])]),we,_e,ke,e("p",null,[xe,t(": "),e("a",Ae,[t("http://arxiv.org/abs/2404.07344v1"),n(o)])]),Le,Te,Re,e("p",null,[Se,t(": "),e("a",Me,[t("http://arxiv.org/abs/2404.07168v1"),n(o)])]),ze,Pe,je,e("p",null,[Ce,t(": "),e("a",De,[t("http://arxiv.org/abs/2404.07158v1"),n(o)])]),Ie,We,Be,e("p",null,[Oe,t(": "),e("a",qe,[t("http://arxiv.org/abs/2404.06974v1"),n(o)])]),Fe,He,Ee,e("p",null,[Ge,t(": "),e("a",Ne,[t("http://arxiv.org/abs/2404.06940v1"),n(o)])]),Ke,Je,Ye,e("p",null,[Ve,t(": "),e("a",Ue,[t("http://arxiv.org/abs/2404.06904v1"),n(o)])]),Ze,$e,Xe,e("p",null,[Qe,t(": "),e("a",et,[t("http://arxiv.org/abs/2404.06807v1"),n(o)])]),tt,ot,nt,e("p",null,[at,t(": "),e("a",it,[t("http://arxiv.org/abs/2404.06740v1"),n(o)])]),rt,st,lt,e("p",null,[ct,t(": "),e("a",ht,[t("http://arxiv.org/abs/2404.06687v1"),n(o)])]),dt,ut,pt,gt,e("p",null,[mt,t(": "),e("a",bt,[t("http://arxiv.org/abs/2404.06645v1"),n(o)])]),ft,vt,yt,e("p",null,[wt,t(": "),e("a",_t,[t("http://arxiv.org/abs/2404.06631v1"),n(o)])]),kt,xt,At,e("p",null,[Lt,t(": "),e("a",Tt,[t("http://arxiv.org/abs/2404.06570v1"),n(o)])]),Rt,St,Mt,e("p",null,[zt,t(": "),e("a",Pt,[t("http://arxiv.org/abs/2404.06413v1"),n(o)])]),jt,Ct,Dt,e("p",null,[It,t(": "),e("a",Wt,[t("http://arxiv.org/abs/2404.06277v1"),n(o)])]),Bt,Ot,qt,e("p",null,[Ft,t(": "),e("a",Ht,[t("http://arxiv.org/abs/2404.06178v1"),n(o)])]),Et,Gt,Nt,e("p",null,[Kt,t(": "),e("a",Jt,[t("http://arxiv.org/abs/2404.06171v1"),n(o)])]),Yt,Vt,Ut,e("p",null,[Zt,t(": "),e("a",$t,[t("http://arxiv.org/abs/2404.06129v1"),n(o)])]),Xt,Qt,eo,e("p",null,[to,t(": "),e("a",oo,[t("http://arxiv.org/abs/2404.06089v1"),n(o)])]),no,ao,io,e("p",null,[ro,t(": "),e("a",so,[t("http://arxiv.org/abs/2404.05953v1"),n(o)])]),lo,co,ho,e("p",null,[uo,t(": "),e("a",po,[t("http://arxiv.org/abs/2404.05952v1"),n(o)])]),go,mo,bo,e("p",null,[fo,t(": "),e("a",vo,[t("http://arxiv.org/abs/2404.05932v1"),n(o)])]),yo,wo,_o,ko,e("p",null,[xo,t(": "),e("a",Ao,[t("http://arxiv.org/abs/2404.05887v1"),n(o)])]),Lo,To,Ro,e("p",null,[So,t(": "),e("a",Mo,[t("http://arxiv.org/abs/2404.05870v2"),n(o)])]),zo,Po,jo,e("p",null,[Co,t(": "),e("a",Do,[t("http://arxiv.org/abs/2404.05858v1"),n(o)])]),Io,Wo,Bo,e("p",null,[Oo,t(": "),e("a",qo,[t("http://arxiv.org/abs/2404.05836v1"),n(o)])]),Fo,Ho,Eo,e("p",null,[Go,t(": "),e("a",No,[t("http://arxiv.org/abs/2404.05695v1"),n(o)])]),Ko,Jo,Yo,e("p",null,[Vo,t(": "),e("a",Uo,[t("http://arxiv.org/abs/2404.05627v1"),n(o)])]),Zo,$o,Xo,e("p",null,[Qo,t(": "),e("a",en,[t("http://arxiv.org/abs/2404.05318v1"),n(o)])]),tn,on,nn,e("p",null,[an,t(": "),e("a",rn,[t("http://arxiv.org/abs/2404.05291v1"),n(o)])]),sn,ln,cn,e("p",null,[hn,t(": "),e("a",dn,[t("http://arxiv.org/abs/2404.05262v1"),n(o)])]),un,pn,gn,e("p",null,[mn,t(": "),e("a",bn,[t("http://arxiv.org/abs/2404.05203v1"),n(o)])]),fn,vn,yn,e("p",null,[wn,t(": "),e("a",_n,[t("http://arxiv.org/abs/2404.05134v1"),n(o)])]),kn,xn,An,e("p",null,[Ln,t(": "),e("a",Tn,[t("http://arxiv.org/abs/2404.05120v1"),n(o)])]),Rn,Sn,Mn,zn,e("p",null,[Pn,t(": "),e("a",jn,[t("http://arxiv.org/abs/2404.05100v1"),n(o)])]),Cn,Dn,In,e("p",null,[Wn,t(": "),e("a",Bn,[t("http://arxiv.org/abs/2404.05087v1"),n(o)])]),On,qn,Fn,e("p",null,[Hn,t(": "),e("a",En,[t("http://arxiv.org/abs/2404.05067v1"),n(o)])]),Gn,Nn,Kn,e("p",null,[Jn,t(": "),e("a",Yn,[t("http://arxiv.org/abs/2404.05050v1"),n(o)])]),Vn,Un,Zn,e("p",null,[$n,t(": "),e("a",Xn,[t("http://arxiv.org/abs/2404.05039v1"),n(o)])]),Qn,ea,ta,e("p",null,[oa,t(": "),e("a",na,[t("http://arxiv.org/abs/2404.05024v1"),n(o)])]),aa,ia,ra,e("p",null,[sa,t(": "),e("a",la,[t("http://arxiv.org/abs/2404.04929v1"),n(o)])]),ca,ha,da,e("p",null,[ua,t(": "),e("a",pa,[t("http://arxiv.org/abs/2404.04857v1"),n(o)])]),ga,ma,ba,e("p",null,[fa,t(": "),e("a",va,[t("http://arxiv.org/abs/2404.04852v1"),n(o)])]),ya,wa,_a,e("p",null,[ka,t(": "),e("a",xa,[t("http://arxiv.org/abs/2404.04832v1"),n(o)])]),Aa,La,Ta,e("p",null,[Ra,t(": "),e("a",Sa,[t("http://arxiv.org/abs/2404.04772v1"),n(o)])]),Ma,za,Pa,ja,e("p",null,[Ca,t(": "),e("a",Da,[t("http://arxiv.org/abs/2404.04698v1"),n(o)])]),Ia,Wa,Ba,e("p",null,[Oa,t(": "),e("a",qa,[t("http://arxiv.org/abs/2404.04579v1"),n(o)])]),Fa,Ha,Ea,e("p",null,[Ga,t(": "),e("a",Na,[t("http://arxiv.org/abs/2404.04458v1"),n(o)])]),Ka,Ja,Ya,Va,e("p",null,[Ua,t(": "),e("a",Za,[t("http://arxiv.org/abs/2404.04416v1"),n(o)])]),$a,Xa,Qa,e("p",null,[ei,t(": "),e("a",ti,[t("http://arxiv.org/abs/2404.04404v1"),n(o)])]),oi,ni,ai,e("p",null,[ii,t(": "),e("a",ri,[t("http://arxiv.org/abs/2404.04249v1"),n(o)])]),si,li,ci,e("p",null,[hi,t(": "),e("a",di,[t("http://arxiv.org/abs/2404.04241v1"),n(o)])]),ui,pi,gi,e("p",null,[mi,t(": "),e("a",bi,[t("http://arxiv.org/abs/2404.04220v1"),n(o)])]),fi,vi,yi,e("p",null,[wi,t(": "),e("a",_i,[t("http://arxiv.org/abs/2404.04219v1"),n(o)])]),ki,xi,Ai,e("p",null,[Li,t(": "),e("a",Ti,[t("http://arxiv.org/abs/2404.04186v1"),n(o)])]),Ri,Si,Mi,e("p",null,[zi,t(": "),e("a",Pi,[t("http://arxiv.org/abs/2404.04123v1"),n(o)])]),ji,Ci,Di,e("p",null,[Ii,t(": "),e("a",Wi,[t("http://arxiv.org/abs/2404.04079v1"),n(o)])]),Bi,Oi,qi,e("p",null,[Fi,t(": "),e("a",Hi,[t("http://arxiv.org/abs/2404.04069v1"),n(o)])]),Ei,Gi,Ni,e("p",null,[Ki,t(": "),e("a",Ji,[t("http://arxiv.org/abs/2404.04066v1"),n(o)])]),Yi,Vi,Ui,e("p",null,[Zi,t(": "),e("a",$i,[t("http://arxiv.org/abs/2404.04004v1"),n(o)])]),Xi,Qi,er,e("p",null,[tr,t(": "),e("a",or,[t("http://arxiv.org/abs/2404.03943v1"),n(o)])]),nr,ar,ir,rr,e("p",null,[sr,t(": "),e("a",lr,[t("http://arxiv.org/abs/2404.03834v1"),n(o)])]),cr,hr,dr,e("p",null,[ur,t(": "),e("a",pr,[t("http://arxiv.org/abs/2404.03816v1"),n(o)])]),gr,mr,br,e("p",null,[fr,t(": "),e("a",vr,[t("http://arxiv.org/abs/2404.03734v1"),n(o)])]),yr,wr,_r,e("p",null,[kr,t(": "),e("a",xr,[t("http://arxiv.org/abs/2404.03729v2"),n(o)])]),Ar,Lr,Tr,e("p",null,[Rr,t(": "),e("a",Sr,[t("http://arxiv.org/abs/2404.03629v1"),n(o)])]),Mr,zr,Pr,e("p",null,[jr,t(": "),e("a",Cr,[t("http://arxiv.org/abs/2404.03587v1"),n(o)])]),Dr,Ir,Wr,e("p",null,[Br,t(": "),e("a",Or,[t("http://arxiv.org/abs/2404.03556v1"),n(o)])]),qr,Fr,Hr,e("p",null,[Er,t(": "),e("a",Gr,[t("http://arxiv.org/abs/2404.03498v1"),n(o)])]),Nr,Kr,Jr,e("p",null,[Yr,t(": "),e("a",Vr,[t("http://arxiv.org/abs/2404.03489v1"),n(o)])]),Ur,Zr,$r,e("p",null,[Xr,t(": "),e("a",Qr,[t("http://arxiv.org/abs/2404.03462v1"),n(o)])]),es,ts,os,e("p",null,[ns,t(": "),e("a",as,[t("http://arxiv.org/abs/2404.03444v1"),n(o)])]),is,rs,ss,e("p",null,[ls,t(": "),e("a",cs,[t("http://arxiv.org/abs/2404.03415v1"),n(o)])]),hs,ds,us,e("p",null,[ps,t(": "),e("a",gs,[t("http://arxiv.org/abs/2404.03412v1"),n(o)])]),ms,bs,fs,e("p",null,[vs,t(": "),e("a",ys,[t("http://arxiv.org/abs/2404.03363v1"),n(o)])]),ws,_s,ks,e("p",null,[xs,t(": "),e("a",As,[t("http://arxiv.org/abs/2404.03325v1"),n(o)])]),Ls,Ts,Rs,e("p",null,[Ss,t(": "),e("a",Ms,[t("http://arxiv.org/abs/2404.03275v1"),n(o)])]),zs,Ps,js,e("p",null,[Cs,t(": "),e("a",Ds,[t("http://arxiv.org/abs/2404.03149v1"),n(o)])]),Is,Ws,Bs,Os,e("p",null,[qs,t(": "),e("a",Fs,[t("http://arxiv.org/abs/2404.03103v1"),n(o)])]),Hs,Es,Gs,e("p",null,[Ns,t(": "),e("a",Ks,[t("http://arxiv.org/abs/2404.03092v1"),n(o)])]),Js,Ys,Vs,e("p",null,[Us,t(": "),e("a",Zs,[t("http://arxiv.org/abs/2404.03067v1"),n(o)])]),$s,Xs,Qs,e("p",null,[el,t(": "),e("a",tl,[t("http://arxiv.org/abs/2404.03049v1"),n(o)])]),ol,nl,al,e("p",null,[il,t(": "),e("a",rl,[t("http://arxiv.org/abs/2404.02771v2"),n(o)])]),sl,ll,cl,e("p",null,[hl,t(": "),e("a",dl,[t("http://arxiv.org/abs/2404.02728v1"),n(o)])]),ul,pl,gl,e("p",null,[ml,t(": "),e("a",bl,[t("http://arxiv.org/abs/2404.02569v1"),n(o)])]),fl,vl,yl,e("p",null,[wl,t(": "),e("a",_l,[t("http://arxiv.org/abs/2404.02516v1"),n(o)])]),kl,xl,Al,e("p",null,[Ll,t(": "),e("a",Tl,[t("http://arxiv.org/abs/2404.02515v1"),n(o)])]),Rl,Sl,Ml,e("p",null,[zl,t(": "),e("a",Pl,[t("http://arxiv.org/abs/2404.02475v1"),n(o)])]),jl])}const Bl=a(c,[["render",Cl],["__file","Robot.html.vue"]]),Ol=JSON.parse('{"path":"/posts/Robot/paper/Robot.html","title":"Robot","lang":"en-US","frontmatter":{"description":"Robot 2024-04-11 Multi-Robot Target Tracking with Sensing and Communication Danger Zones Authors: Jiazhen Li, Peihan Li, Yuwei Wu, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou L...","head":[["meta",{"property":"og:url","content":"https://opendesign.world/posts/Robot/paper/Robot.html"}],["meta",{"property":"og:site_name","content":"OpenDesign"}],["meta",{"property":"og:title","content":"Robot"}],["meta",{"property":"og:description","content":"Robot 2024-04-11 Multi-Robot Target Tracking with Sensing and Communication Danger Zones Authors: Jiazhen Li, Peihan Li, Yuwei Wu, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou L..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Robot\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[]}"]]},"headers":[{"level":2,"title":"2024-04-11","slug":"_2024-04-11","link":"#_2024-04-11","children":[]},{"level":2,"title":"2024-04-10","slug":"_2024-04-10","link":"#_2024-04-10","children":[]},{"level":2,"title":"2024-04-09","slug":"_2024-04-09","link":"#_2024-04-09","children":[]},{"level":2,"title":"2024-04-08","slug":"_2024-04-08","link":"#_2024-04-08","children":[]},{"level":2,"title":"2024-04-07","slug":"_2024-04-07","link":"#_2024-04-07","children":[]},{"level":2,"title":"2024-04-06","slug":"_2024-04-06","link":"#_2024-04-06","children":[]},{"level":2,"title":"2024-04-05","slug":"_2024-04-05","link":"#_2024-04-05","children":[]},{"level":2,"title":"2024-04-04","slug":"_2024-04-04","link":"#_2024-04-04","children":[]},{"level":2,"title":"2024-04-03","slug":"_2024-04-03","link":"#_2024-04-03","children":[]}],"git":{"updatedTime":null,"contributors":[]},"filePathRelative":"posts/Robot/paper/Robot.md","autoDesc":true,"excerpt":"\\n<h2>2024-04-11</h2>\\n<h4>Multi-Robot Target Tracking with Sensing and Communication Danger Zones</h4>\\n<p><strong>Authors</strong>: Jiazhen Li, Peihan Li, Yuwei Wu, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou</p>\\n<p><strong>Link</strong>: <a href=\\"http://arxiv.org/abs/2404.07880v1\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">http://arxiv.org/abs/2404.07880v1</a></p>"}');export{Bl as comp,Ol as data};
